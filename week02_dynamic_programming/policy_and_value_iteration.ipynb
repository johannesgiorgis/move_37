{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy and Value Iteration\n",
    "source: [Policy and Value Iteration PDF](https://www.theschool.ai/wp-content/uploads/2018/09/policy_and_value_iteration.pdf)\n",
    "\n",
    "### Lecture Notes\n",
    "Now that we are familiar with the basics, its time to move forward. Let us revisit the same problem we\n",
    "discussed last week. Robotic vacuum cleaner famously known as Roomba is a machine that cleans\n",
    "the floor. Roomba needs to start, clean, avoid obstacles and find the charging station. These 4 states\n",
    "describe the possible positions of the robot and the action describes the direction of motion. The\n",
    "robot can move left/right/up/down. The first (Battery Full) and the final (Charging) states are the\n",
    "terminal states. The goal is to find an optimal policy that maximizes the return from any initial states.\n",
    "\n",
    "Let us use OpenAI gym and try to solve this problem. FrozenLake-v0 environment will be a best fit for\n",
    "this problem. The reward and next state models are stochastic $p(r_t+1 | s_t , a_t ), p(s_t+1 | s_t , a_t\n",
    ")$ for this environment. To solve this problem, we should find an optimal policy that can reach the goal\n",
    "more than **70%** of the times. (Please try to come up with a solution before looking at the solutions) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score = 0.54. Time taken = 7.7903 seconds\n"
     ]
    }
   ],
   "source": [
    "# Brute Force Method\n",
    "'''\n",
    "We got 16 states and 4 possible moves that gives us 4^16 = 4294967296 possible policies to choose\n",
    "from. It is a computationally intensive task to evaluate all of them so we are going to choose few\n",
    "cases randomly and select the best among them\n",
    "'''\n",
    "import numpy as np\n",
    "import time\n",
    "import gym\n",
    "\"\"\"\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI gym env.\n",
    "        render: boolean to turn rendering on/off.\n",
    "\"\"\"\n",
    "# Execution\n",
    "def execute(env, policy, episode_length=100, render=False):\n",
    "    total_reward = 0\n",
    "    start = env.reset()\n",
    "    for t in range(episode_length):\n",
    "        if render:\n",
    "            env.render()\n",
    "        action = policy[start]\n",
    "        start, reward, done, _ = env.step(action)\n",
    "        total_reward += reward\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Evaluation\n",
    "def evaluatePolicy(env, policy, n_episodes=100):\n",
    "    total_reward = 0.0\n",
    "    for _ in range(n_episodes):\n",
    "        total_reward += execute(env, policy)\n",
    "    return total_reward / n_episodes\n",
    "\n",
    "\n",
    "# Funtion for a random policy\n",
    "def gen_random_policy():\n",
    "    return np.random.choice(4, size=((16)))\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "\n",
    "    ## Policy Search\n",
    "    n_policies = 1000\n",
    "    start_time = time.time()\n",
    "    policy_set = [gen_random_policy() for _ in range(n_policies)]\n",
    "    policy_score = [evaluatePolicy(env, p) for p in policy_set]\n",
    "    end_time = time.time()\n",
    "    print(\"Best score = {:0.2f}. Time taken = {:4.4f} seconds\".format(np.max(policy_score), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above script searches the environment for best policy in a random set of 1000 solutions and evaluates them. The best policy score we get is **0.54** in **7.7903** seconds. It means that the chance of agent reaching the goal is **54%**. It is not even close to our goal. Random search does not work well for complex problems where the search space is huge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**_How can we improve this?_**\n",
    "\n",
    "The goal of the Roomba is to pick the best policy that will maximize the total rewards received in the environment:\n",
    "- t_0: Roomba observes the environment state s_0 and picks an action a_0, then upon performing its action, environment state becomes s_1 and the Roomba receives a reward r_1.\n",
    "- t_1: Roomba observes the environment state s_1 and picks an action a_1, then upon performing its action, environment state becomes s_2 and the Roomba receives a reward r_2.\n",
    "- t_2: Roomba observes the environment state s_2 and picks an action a_2, then upon performing its action, environment state becomes s_3 and the Roomba receives a reward r_3.\n",
    "\n",
    "So the total reward received by the Roomba in response to the actions selected by its policy is going to be:\n",
    "\n",
    "\n",
    "$$ Total Reward = r_1 + r_2 + r_3 + r_4 + r_5 + ...$$\n",
    "\n",
    "However, it is common to use a discount factor to give higher weight if the rewards are nearby and lower weight if the rewards are far in the future:\n",
    "\n",
    "$$Total Discounted Reward = r_1 + \\gamma{r_2} + \\gamma^2{r_3} + \\gamma^3{r_4} + \\gamma^4{r_5} + ...$$\n",
    "\n",
    "$$\t\\sum_{i=1}^{T} \\gamma^{i-1}{r_i} $$\n",
    "\n",
    "where T is the length of the episode. It can be infinity if there is huge length for the episode.\n",
    "\n",
    "**_Why discount factor?_**\n",
    "\n",
    "Using a discount favor prevents the total reward from going to infinity (because $0 <= \\gamma <= 1$). It also models the agent behavior when the agent prefers intermediate rewards than rewards that are potentially received later in the future.\n",
    "\n",
    "In our example, Roomba can take two paths to reach its goal state:\n",
    "- One if longer but gives higher reward\n",
    "- There is a shorter path with a smaller reward\n",
    "We need the Roomba to cover and clean maximum distance before it reaches the goal. By adjusting the $\\gamma$ value, we can control which path the Roomba should prefer to take.\n",
    "\n",
    "**Value Function**  \n",
    "A popular term in Reinforcement Learning and denoted as **V(s)**, it represents how good a state is for an agent to be in.\n",
    "\n",
    "Amongst all the possible value functions, there will be an optimal value function that has higher value than other functions for all states. This is denoted as **V*(s)**.\n",
    "\n",
    "The optimal policy $\\pi^*$ is the policy that corresponds to the optimal value function.\n",
    "\n",
    "**Q Function**  \n",
    "Q is a function of a state-action pair and returns real value, \"**Q: S x A --> R**\". An optimal Q-function **Q*(s,a)** means the expected total reward received by an agent starting in a state **s** and picks an action **a** will then move optimally forward. In other words, this indicates how good it is for an agent to pick action **a** while in state **s**.\n",
    "\n",
    "Since **V*(s)** is the maximum expected total reward when starting from state **s**, it will be the maximum of **Q*(s,a)** over all possible actions.\n",
    "\n",
    "When a known optimal Q-function **Q*(s,a)** exists, the optimal policy can be attained by choosing the action **a** that declares the maximum **Q*(s,a)** for a particular state **s**.\n",
    "\n",
    "Bellman equation using Dynamic Programming gives a recursive definition for the optimal Q-function. The **Q*(s,a)** equals the summation of immediate reward after performing action **a** while in state **s** and the discounted expected future reward after transition to a next state **s'**.\n",
    "\n",
    "**Value Iteration**  \n",
    "Value Iteration computes the optimal state value function by iteratively improving the estimate of **V(s)**. The algorithm initializes **V(s)** to arbitrary random values when it repeatedly updates the **Q(s,a)** and **V(s)** values until they converge. Value iteration is guaranteed to converge to the optimal values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value-iteration converged at # 1373.\n",
      "Best score = 0.73. Time taken = 0.3576 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's take the above problem and try to solve it using the Value Iteration method.\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\"\"\"\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI Gym env.\n",
    "            env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        gamma: Gamma discount factor.\n",
    "        render: boolean to turn rendering on/off.\n",
    "\"\"\"\n",
    "# Executes an episode\n",
    "def execute(env, policy, gamma=0.1, render=False):\n",
    "    start = env.reset()\n",
    "    total_reward = 0\n",
    "    step_index = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        start, reward, done, _ = env.step(int(policy[start]))\n",
    "        total_reward += (gamma ** step_index * reward)\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Evaluates a policy by running it n times. returns: average total reward\n",
    "def evaluatePolicy(env, policy, gamma=1.0, n=100):\n",
    "    scores = [\n",
    "        execute(env, policy, gamma=gamma, render=False)\n",
    "        for _ in range(n)\n",
    "    ]\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Choosing the policy given a value-function\n",
    "def calculatePolicy(v, gamma=1.0):\n",
    "    policy = np.zeros(env.env.nS)\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = np.zeros(env.action_space.n)\n",
    "        for a in range(env.action_space.n):\n",
    "            for next_sr in env.env.P[s][a]:\n",
    "                # next_sr is a tuple of (probability, next state, reward, done)\n",
    "                p, s_, r, _ = next_sr\n",
    "                q_sa[a] += (p * (r + gamma * v[s_]))\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "\n",
    "# Value Iteration Algorithm\n",
    "def valueIteration(env, gamma=1.0):\n",
    "    value = np.zeros(env.env.nS) # initialize value-function\n",
    "    max_iterations = 10000\n",
    "    eps = 1e-20\n",
    "    for i in range(max_iterations):\n",
    "        previous_value = np.copy(value)\n",
    "        for s in range(env.env.nS):\n",
    "            q_sa = [\n",
    "                sum([p * (r + previous_value[s_])\n",
    "                     for p, s_, r, _ in env.env.P[s][a]])\n",
    "                for a in range(env.env.nA)\n",
    "            ]\n",
    "            value[s] = max(q_sa)\n",
    "        if (np.sum(np.fabs(previous_value - value)) <= eps):\n",
    "            print('Value-iteration converged at # {}.'.format(i + 1)) \n",
    "            break\n",
    "    return value\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    gamma = 1.0\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    optimal_value = valueIteration(env, gamma)\n",
    "    start_time = time.time()\n",
    "    policy = calculatePolicy(optimal_value, gamma)\n",
    "    policy_score = evaluatePolicy(env, policy, gamma, n=1000)\n",
    "    end_time = time.time()\n",
    "    # why np.mean? typo? Should it not be np.max?\n",
    "    print(\"Best score = {:0.2f}. Time taken = {:4.4f} seconds\".format(np.mean(policy_score), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Policy Iteration:**\n",
    "The Value-Iteration algorithm keeps improving the value function at each iteration until the Value function converges. Since the agent only cares about finding the optimal policy, sometimes the optimal policy will converge before the Value-Function. Therefore, we have another algorithm called Policy-Iteration. Instead of repeatedly improving the Value-Function estimate, it will re-define the policy at each step and compute the value according to this new policy until the policy converges. Policy Iteration is also guaranteed to converge to the optimal policy and it often takes less iterations to converge than the Value-Iteration algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Policy Iteration converged at 4\n",
      "Best score = 0.77. Time taken = 0.1275 seconds\n"
     ]
    }
   ],
   "source": [
    "# Let's take the above problem and try to solve it using the Policy Iteration method.\n",
    "import numpy as np\n",
    "import gym\n",
    "import time\n",
    "\"\"\"\n",
    "    Args:\n",
    "        policy: [S, A] shaped matrix representing the policy.\n",
    "        env: OpenAI Gym env.\n",
    "            env.P represents the transition probabilities of the environment.\n",
    "            env.P[s][a] is a list of transition tuples (prob, next_state, reward, done).\n",
    "            env.nS is a number of states in the environment.\n",
    "            env.nA is a number of actions in the environment.\n",
    "        gamma: Gamma discount factor.\n",
    "        render: boolean to turn rendering on/off.\n",
    "\"\"\"\n",
    "# Executes an episode\n",
    "def execute(env, policy, gamma=0.1, render=False):\n",
    "    start = env.reset()\n",
    "    total_reward = 0\n",
    "    step_index = 0\n",
    "    while True:\n",
    "        if render:\n",
    "            env.render()\n",
    "        start, reward, done, _ = env.step(int(policy[start]))\n",
    "        total_reward += (gamma ** step_index * reward)\n",
    "        step_index += 1\n",
    "        if done:\n",
    "            break\n",
    "    return total_reward\n",
    "\n",
    "\n",
    "# Evaluates a policy by running it n times. returns: average total reward\n",
    "def evaluatePolicy(env, policy, gamma=1.0, n=100):\n",
    "    scores = [\n",
    "        execute(env, policy, gamma=gamma, render=False)\n",
    "        for _ in range(n)\n",
    "    ]\n",
    "    return np.mean(scores)\n",
    "\n",
    "\n",
    "# Extract the policy given a value-function\n",
    "def extractPolicy(v, gamma=1.0):\n",
    "    policy = np.zeros(env.env.nS)\n",
    "    for s in range(env.env.nS):\n",
    "        q_sa = np.zeros(env.env.nA)\n",
    "        for a in range(env.env.nA):\n",
    "            q_sa[a] = sum([p * (r + gamma * v[s_]) \n",
    "                           for p, s_, r, _ in env.env.P[s][a]]\n",
    "                         )\n",
    "        policy[s] = np.argmax(q_sa)\n",
    "    return policy\n",
    "\n",
    "# Iteratively calculates the Value-Function under Policy.\n",
    "def calculatePolicyValue(env, policy, gamma=1.0):\n",
    "    value = np.zeros(env.env.nS)\n",
    "    eps = 1e-10\n",
    "    while True:\n",
    "        previous_value = np.copy(value)\n",
    "        for states in range(env.env.nS):\n",
    "            policy_a = policy[states]\n",
    "            value[states] = sum([p * (r + gamma * previous_value[s_]) \n",
    "                                 for p, s_, r, _ in env.env.P[states][policy_a]]\n",
    "                               )\n",
    "        if (np.sum((np.fabs(previous_value - value))) <= eps):\n",
    "            # value converged\n",
    "            break\n",
    "    return value\n",
    "\n",
    "\n",
    "# Policy Iteration Algorithm\n",
    "def policyIteration(env, gamma=1.0):\n",
    "    policy = np.random.choice(env.env.nA, size=(env.env.nS)) # initialize a random policy\n",
    "    max_iterations = 1000\n",
    "    gamma = 1.0\n",
    "    for i in range(max_iterations):\n",
    "        old_policy_value = calculatePolicyValue(env, policy, gamma)\n",
    "        new_policy = extractPolicy(old_policy_value, gamma)\n",
    "        if (np.all(policy == new_policy)):\n",
    "            print('Policy Iteration converged at {}'.format(i + 1))\n",
    "            break\n",
    "        policy = new_policy\n",
    "    return policy\n",
    "\n",
    "\n",
    "# Main\n",
    "if __name__ == '__main__':\n",
    "    env = gym.make('FrozenLake-v0')\n",
    "    start_time = time.time()\n",
    "    optimal_policy = policyIteration(env, gamma=1.0)\n",
    "    scores = evaluatePolicy(env, optimal_policy, gamma=1.0)\n",
    "    end_time = time.time()\n",
    "    print(\"Best score = {:0.2f}. Time taken = {:4.4f} seconds\".format(np.max(scores), end_time - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Value-Iteration vs. Policy-Iteration:**\n",
    "Both Value-Iteration and Policy Iteration Algorithms can be used for _offline planning_ where the agent is assumed to have prior knowledge about the effects of its actions on the environment (they assume the MDP model is known). Comparing with each other, Policy-Iteration is computationally efficient as it often takes considerably fewer number of iterations to converge although each iteration is more computationally expensive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Credits\n",
    "- [Deep Reinforcement Learning Slide](http://learning.mpi-sws.org/mlss2016/slides/2016-MLSS-RL.pdf)\n",
    "- [Reinforcement Learning: An Introduction 2nd Ed In Progress](http://incompleteideas.net/book/bookdraft2018jan1.pdf)\n",
    "- [Deep RL Course - Value Function Methods](http://rail.eecs.berkeley.edu/deeprlcourse/static/slides/lec-7.pdf)\n",
    "- [OpenAI Gym - FrozenLake-v0 Algorithm](https://gym.openai.com/evaluations/eval_4VyQBhXMRLmG9y9MQA5ePA/)\n",
    "- [Moustafa F. Alzantot](http://web.cs.ucla.edu/~malzantot/)\n",
    "- [planetB Syntax Highlight Code in Word Documents](http://www.planetb.ca/syntax-highlight-word)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
