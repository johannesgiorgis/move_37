{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment Week 3 - Monte Carlo\n",
    "\n",
    "This weeks homework is to apply a Monte Carlo technique in a unique OpenAI Gym environment (not blackjack). The different Monte Carlo techniques are detailed [here](https://oneraynyday.github.io/ml/2018/05/24/Reinforcement-Learning-Monte-Carlo/#example-cliff-walking).\n",
    "\n",
    "Write a Medium blog post about your experience implementing the algorithm and what you learned. And (optionally) share your post on Twitter with the #move37 hashtag!  Siraj Raval."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Monte Carlo GLIE: 2\n",
    "Applied to Taxi-v2 environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# import necessary libraries\n",
    "import gym\n",
    "from gym import wrappers\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "\n",
    "# Helper Functions\n",
    "def update_report(report):\n",
    "    '''\n",
    "    Calculates training metrics and stores them in a pandas dataframe\n",
    "    '''\n",
    "    r = report\n",
    "    r['last_avg_total_score'] - r['avg_total_score']\n",
    "    r['avg_total_score'] = sum(r['rewards_list']) / r['episode_interval']\n",
    "    \n",
    "    # calculate delta\n",
    "    delta = r['avg_total_score'] - r['last_avg_total_score']\n",
    "    \n",
    "    print(\n",
    "        f\"K episodes: {r['k_episodes']:>8.3f} \"\n",
    "        f\"Average Total Score: {r['avg_total_score']:>10.3f} \"\n",
    "        f\"Delta: {delta:>10.3f} \"\n",
    "        f\"Epsilon: {r['epsilon']:>10.3f} \"\n",
    "    )\n",
    "    \n",
    "    # add to dataframe\n",
    "    r['df'].loc[r['df_index']] = [\n",
    "        r['k_episodes'],\n",
    "        r['avg_total_score'],\n",
    "        delta,\n",
    "        r['epsilon']\n",
    "    ]\n",
    "    \n",
    "    r['df_index'] += 1\n",
    "    \n",
    "    # re-initialize rewards_list - allows us to only capture last\n",
    "    # episode_interval runs\n",
    "    r['reward_list'] = []\n",
    "    \n",
    "\n",
    "    \n",
    "def get_epsilon(target_epsilon, i, episode_interval, num_episodes):\n",
    "    '''\n",
    "    Creates a function that changes the value of epsilon over time\n",
    "    '''\n",
    "    # allow us to test last 5% to see how much training it has done\n",
    "    if i > num_episodes * 0.94:\n",
    "        return 0.0\n",
    "    else:\n",
    "        return 0.3 * (1 - (i / num_episodes)) + target_epsilon\n",
    "\n",
    "\n",
    "\n",
    "# Main\n",
    "def main(num_episodes=10000, target_epsilon=0.2, episode_interval=1000):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # create our environment\n",
    "    env = gym.make(\"Taxi-v2\")\n",
    "    #env = wrappers.Monitor(env, \"./results\", force=True)\n",
    "\n",
    "    # initialize the action value function Q(s, a),\n",
    "    # and a counter function N(s, a)\n",
    "    Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    n_s_a = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "\n",
    "    # initialize report\n",
    "    labels = ['k episodes', 'Average Total Score', 'Delta', 'Epsilon']\n",
    "    report = {\n",
    "        'rewards_list': [],\n",
    "        'avg_total_score': 0,\n",
    "        'last_avg_total_score': 0,\n",
    "        'df': pd.DataFrame(columns=labels),\n",
    "        'df_index': 0,\n",
    "        'episode_interval': episode_interval,\n",
    "    }\n",
    "    \n",
    "    # set df labels type\n",
    "    report['df'][labels] = report['df'][labels].astype(float)\n",
    "    \n",
    "    \n",
    "    for i in range(num_episodes + 1):\n",
    "        \n",
    "        state = env.reset()\n",
    "        r_all = 0 # sum of all rewards\n",
    "        done = False\n",
    "        results_list = []\n",
    "        results_sum = 0.0\n",
    "        \n",
    "        while not done:\n",
    "            if np.random() < target_epsilon:\n",
    "                action = env.action_space.sample()\n",
    "            else:\n",
    "                action = np.argmax(Q[state, :])\n",
    "            new_state, reward, done, _ = env.step(action)\n",
    "            results_list.append((state, action))\n",
    "            results_sum += reward\n",
    "            state = new_state\n",
    "            r_all += reward\n",
    "        \n",
    "        report['rewards_list'].append(r_all)\n",
    "        \n",
    "        for (state, action) in results_list:\n",
    "            n_s_a[state, action] += 1.0\n",
    "            alpha = 1.0 / n_s_a[state, action]\n",
    "            Q[state, action] += alpha * (results_sum - Q[state, action])\n",
    "        \n",
    "        if i % report['episode_interval'] == 0 and i is not 0:\n",
    "            report['k_episodes'] = i / 1000.0\n",
    "            report['epsilon'] - get_epsilon(target_epsilon, \n",
    "                                            i,\n",
    "                                            report['episode_interval'],\n",
    "                                            num_episodes)\n",
    "            \n",
    "            # update report\n",
    "            update_report(report)\n",
    "    \n",
    "    # close environment\n",
    "    env.close()\n",
    "    \n",
    "    # calculate run time\n",
    "    end_time = time.time()\n",
    "    run_time = end_time - start_time\n",
    "    print(f\"Time taken: {run_time:10.2f} seconds\")\n",
    "    return report['df'], run_time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Introduction\n",
    "For assignment 3 of Move 37, we had to apply a Monte Carlo technique to a unique OpenAI Gym environment other than Blackjack.\n",
    "\n",
    "For this assignment, I collaborated with [Andrew Key][ak github].\n",
    "\n",
    "We started off with this [code][original code], which applies Monte Carlo GLIE to the FrozenLake-v0 environment.\n",
    "\n",
    "### Monte Carlo GLIE\n",
    "\n",
    "\n",
    "### Reward Evaluation\n",
    "\n",
    "### Epsilon\n",
    "\n",
    "[original code]: https://gist.github.com/Sathishruw/d609e358b61268cdf891cc93e15e5f63\n",
    "[ak github]: https://github.com/redpanda-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Extension Ideas\n",
    "\n",
    "- Test with different parameters\n",
    "- Have an agent interact in an environment with a trained Q-Function."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
