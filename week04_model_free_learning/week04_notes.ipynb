{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04 Notes - Model Free Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dopamine-in-Neuroscience\" data-toc-modified-id=\"Dopamine-in-Neuroscience-1\">Dopamine in Neuroscience</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.2\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.3\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Reading-Assignment:-Model-Based-vs.-Model-Free-Learning-\" data-toc-modified-id=\"Reading-Assignment:-Model-Based-vs.-Model-Free-Learning--2\">Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a></a></span></li><li><span><a href=\"#Homework-Assignment:-Q-Learning\" data-toc-modified-id=\"Homework-Assignment:-Q-Learning-3\">Homework Assignment: Q Learning</a></span></li><li><span><a href=\"#Temporal-Difference-Learning\" data-toc-modified-id=\"Temporal-Difference-Learning-4\">Temporal Difference Learning</a></span></li><li><span><a href=\"#Quiz:-Model-Free-Learning\" data-toc-modified-id=\"Quiz:-Model-Free-Learning-5\">Quiz: Model Free Learning</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-5.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-5.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-5.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-5.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-5.5\">Question 5</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-5.6\">Resources</a></span></li></ul></li><li><span><a href=\"#Q-Learning-Tutorial-for-Ride-Sharing\" data-toc-modified-id=\"Q-Learning-Tutorial-for-Ride-Sharing-6\">Q Learning Tutorial for Ride Sharing</a></span></li><li><span><a href=\"#Quantum-Interview\" data-toc-modified-id=\"Quantum-Interview-7\">Quantum Interview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-7.1\">Learning Resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine in Neuroscience\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "The human brain is wondrous in its capabilities. The rules that govern its prowess at so many tasks are becoming slightly clearer everyday. In this video, I'll highlight how 4 key reinforcement learning algorithms help explain how the human brain works, specifically through the lens of the neurotransmitter known as 'dopamine'. These algorithms have been used to help train everything from autopilot systems for airplanes, to video game bots. TD-Learning, Rescorla-Wagner, Kalman Filters, and Bayesian Learning, all in one go!\n",
    "\n",
    "### Notes\n",
    "- **Associative Learning Theory**\n",
    "    - Describes the process by which a person or animal learns an association between 2 stimuli\n",
    "    - Basic claims are that Reinforcement Learning is the acquisition of associations between states, actions and rewards\n",
    "\n",
    "|    Type    | Punishment<br/>(decreasing behavior) | Reinforcement<br/>(increasing behavior)  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Positive<br/>_(adding)_ | adding something<br/>to<br/>decrease behavior | adding something<br/>to<br/> increase behavior |\n",
    "| Negative<br/>_(subtracting)_ | subtracting something<br/>to<br/>decrease behavior | subtracting something<br/>to<br/>increase behavior |\n",
    "\n",
    "\n",
    "- **Rescorla-Wagner Model**\n",
    "    - Most influential idea of associative learning theory\n",
    "    - Prediction error based learning model where stimuli acquire value when there is a mismatch between the prediction and the outcome\n",
    "    - In the equation, the value of stimulus s at trial T is set equal to the value of stimulus s at the previous time step plus the reward and the expectation. The learning rate defines how much this prediction error is weighted\n",
    "    - This prediction error states that when the animal gets more reward than it expected, it leads to strengthening of the associative weights and a negative prediction error leads to a weakening of the associative weights\n",
    "    \n",
    "    $$ \\Delta{V_n} = c(\\lambda - V_{n-1})$$\n",
    "    \n",
    "    - where:\n",
    "        - $\\Delta{V_n}$: change in associative strength for CS on one trial\n",
    "        - $c$: represent salience of CS and US; a constant (0.0 - 1.0)\n",
    "        - $\\lambda$: maximum associative strength (magnitude of UR)\n",
    "        - $V_{n-1})$: associative strength _already_ accured by CS\n",
    "        - CS: conditional stimuli\n",
    "        - US: unconditioned stimuli\n",
    "        \n",
    "    - Groundbreaking because:\n",
    "        1. Able to explain the conditioning phenomena\n",
    "        2. Useful in early Natural Language Processing Systems\n",
    "    - While it provided a basis for associative learning theory, it only estimated a single value. We know that biological brains are able to represent uncertainty about the world somewhere since the world is full of uncertainty\n",
    "    - Probability theory suggests that to properly represent a biological brains uncertainty of the world, it should utilize a probability distribution over the possible weights instead of a single value\n",
    "    - Use Bayes Rule:\n",
    "     \n",
    "    $$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "    \n",
    "    - The Bayesian generalization of the Rescorla-Wagner Model is embodied in Kalman Filter.\n",
    "    \n",
    "    \n",
    "- **Kalman Filter**\n",
    "    - States that uncertainty grows over time due to the random diffusion of the weights\n",
    "    - This uncertainty can be reduced by observing the data\n",
    "    - Uses a series of measurements observed over time and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone by estimating a joint probability distribution over the variables for each time frame\n",
    "    - Has numerous applications:\n",
    "        - Navigation\n",
    "        - Control of Vehicles (aircrafts)\n",
    "        - Time Series Analysis\n",
    "        - Robotics\n",
    "    - Works by modeling the central nervous systems control of movement\n",
    "    - Because of the time delay between issuing motor commands and receiving sensory feedback, the Kalman Filter is a realistic model for making estimates of the current state of a motor system and using commands.\n",
    "    - It's a two step process:\n",
    "        - In the prediction step, it produces estimates of the current state variables along with their uncertainties \n",
    "        - Once the outcome of the next measurement is observed, these estimates are updated using a weighted average with the most weights given to estimates with higher certainty\n",
    "     - It's a recursive real-time algorithm using just the present input measurements and the previously calculated state as well as its uncertainty matrix \n",
    "\n",
    "So far, we have broken down tasks into trials but real life operates in continuous time and our algorithms are short-sighted. They have been only able to predict the immediate reward, the one that will be received in the very next state. To extend the capabilities of what we are able to model mathematically, we need to switch to modern reinforcement learning theory from classical learning theory.\n",
    "\n",
    "Let's focus on a specific sequential decision problem - a mouse navigating to the exit of a maze. There are two popular classes of Reinforcement Learning algorithms we can use to help solve this problem:\n",
    "\n",
    "|    Type    | Model Free<br/>Less online deliberation | Model Based<br/>More online deliberation  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Learn: | Policy $\\pi$ | Model of<br/> R and T |\n",
    "| Online: | $\\pi(s)$ | Solve MDP |\n",
    "| Method: | Learning from<br/>demonstration | Adaptive dynamic<br/>programming |\n",
    "| Pros: | Simpler execution | Fewer examples needed to learn? |\n",
    "\n",
    "\n",
    "- **Temporal Difference Learning**\n",
    "    - Extended the Rescorla-Wagner Model by introducing a discount factor into the prediction error, which helps define how much a reward matters to an agent depending on when in time it's received\n",
    "    - We can make rewards that happen in the near term worth more\n",
    "    - Invented by 2 researchers (Sutton & Bartow) in the 1980's\n",
    "    - It has been used for:\n",
    "        - Game bots that can beat Humans (most popular is DeepMind and it's DeepQ Learning Algorithm that's able to beat many Atari games)\n",
    "    - TD Learning helps capture some important properties of temporal dynamics as well as dopamine responses but it lacks the uncertainty tracking mechanism of the Kalman Filter.\n",
    "    - So we need a Bayesian version of TD Learning, which is called Kalman TD\n",
    "    - Instead of estimating a single value of the weights, we estimating a mean and a covariance matrix for the weights of our model. \n",
    "    \n",
    "We can think of all four of these models along two dimensions based on what kind of estimator they are (Bayesian or Point based) and what the target they're trying to estimate is (immediate reward or value):\n",
    "\n",
    "|    Estimator\\Target    | Reward  | Value  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Point    | Rescorla-Wagner | TD Learning |\n",
    "| Bayesian | Kalman Filter | Kalman TD |\n",
    "\n",
    "\n",
    "### Take Aways\n",
    "- Associative learning is a learning process in which a new response becomes associated with a particular stimulus\n",
    "- When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainty about the world\n",
    "- Temporal Difference Learning is a Model Free Learning technique that predicts the expected value of a variable occurring at the end of a sequence of states\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=-vhYoS3751g)\n",
    "- [Code Link](https://github.com/llSourcell/Mathematics_of_Dopamine)\n",
    "- [Youtube: The Rescorla-Wagner Model](https://www.youtube.com/watch?v=pYyUSh1veoo)\n",
    "- [Youtube: TD Learning - Richard S. Sutton](https://www.youtube.com/watch?v=LyCpuLikLyQ)\n",
    "- [Youtube: Special Topics - The Kalman Filter](https://www.youtube.com/watch?v=CaCcOwJPytQ)\n",
    "- [Youtube: Bayesian Learning](https://www.youtube.com/watch?v=C2OUfJW5UNM)\n",
    "- [PDF: A Unifying Probabilistic View of Associative Learning](https://dash.harvard.edu/bitstream/handle/1/23845336/4633133.pdf?sequence=1&isAllowed=y)\n",
    "- [Book: Chapter 9 Temporal-Difference Learning](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a>\n",
    "- [Model Based vs. Model Free Reading Assignment](https://www.theschool.ai/wp-content/uploads/2018/09/Move37-Reading-Assignment-Model-Based-vs-Model-Free-1.pdf)\n",
    "\n",
    "**Model**: a plan for our agent. When we have a set of defined state transition probabilities, we call that working with a model. Reinforcement learning can be applied with or without a model, or even used to define a model.\n",
    "\n",
    "A complete model of the environment is required to do Dynamic Programming. If our agent doesn't have a complete map of what to expect, we can instead employ what is called **model-free learning**, where the model learns via trial an error.\n",
    "\n",
    "For some board games such as Chess and Go, although we can accurately model the environment's dynamics, computational power constrains us from calculating the Bellman Optimality equation. This is where Model-free Learning methods shine. We handle this situation by optimizing for a smaller subset of states that are frequently encountered, at the cost of knowing less about the infrequently visited states.\n",
    "\n",
    "Further Reading:\n",
    "- [Medium: Model Free Reinforcement Learning Algorithms](https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4)\n",
    "- [Book: Temporal-Difference Learning (RL: An Introducion Chapeter 6)](http://incompleteideas.net/book/the-book.html)\n",
    "- [PDF: Reward-Based Learning, Model-Based and Model-Free (2014)](https://www.quentinhuys.com/pub/HuysEa14-ModelBasedModelFree.pdf)\n",
    "- [BAIR: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://bair.berkeley.edu/blog/2018/04/26/tdm/)\n",
    "- [ArXiv: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://arxiv.org/abs/1802.09081)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment: Q Learning\n",
    "\n",
    "This weeks homework assignment is to implement Q learning from scratch for the gridworld environment. Use this [repository](https://github.com/rlcode/reinforcement-learning/tree/master/1-grid-world) as a guide, but try not to peak at the Q learning code, recreate it, then check your code with it. Good luck!\n",
    "\n",
    "See [homework04 notebook](homework04/homework04.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=f4zTDRavVq0)\n",
    "\n",
    "**TD Learning**:\n",
    "- This general rule can be summarized as:\n",
    "\n",
    "$$ New\\ Estimate \\leftarrow Old\\ Estimate + Step\\ Size\\ [\\ Target - Old\\ Estimate\\ ] \\\\ $$\n",
    "$$ Where\\ [\\ Target - Old\\ Estimate\\ ]\\ \\rightarrow estimation\\ error\\ (\\delta) \\\\ $$\n",
    "\n",
    "- The Target is the expected return of the state:\n",
    "\n",
    "$$ Target\\ = E_{\\pi}\\ [\\ \\sum_{k=0}^{\\infty} \\gamma^{k}r_t + k + 1 ] $$\n",
    "\n",
    "- The learning rate is a parameter which determines to what extent the error has to be integrated in the new estimation. If the step size is 0, the agent does not learn anything at all. If the step size is 1, the agent considers only the most recent information.\n",
    "\n",
    "\n",
    "$TD(\\lambda) $:\n",
    "\n",
    "- TD(0) algorithm does not take past states into account\n",
    "- What matters in TD(0) is the current state and the state at $t + 1$\n",
    "- However it would be useful to extend what is learned at $t + 1$ to previous states. To achieve this objective, it is necessary to have a **short-term memory mechanism** to store the states which have been visited in the last steps.\n",
    "\n",
    "For each state $s$ at time $t$ we can defined $e_t(s)$ as the **eligibility trace**:\n",
    "\n",
    "$$\n",
    "e_t(s) = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      \\gamma \\lambda e_{t - 1}(s)   \\quad\\quad    if s \\neq s_t;\\\\\n",
    "      \\gamma \\lambda e_{t - 1}(s) + 1 \\quad      if s = s_t;\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$$\n",
    "\n",
    "Here $\\gamma$ is the discount rate and $\\lambda \\in [0, 1]$ is a decay parameter called **trace-decay** or **accumulating trace** which defines the update weight for each state visited. When $0 < \\lambda < 1$, the traces decrease in time.\n",
    "\n",
    "This allows giving a small weight to infrequent states. For $\\lambda = 0$, we have the TD(0) case, and only the immediately preceding prediction is updated. For $\\lambda = 1$, we have TD(1) where all the preceding predictions are equally updated.\n",
    "\n",
    "Now it's time to define the **update rule for TD($\\lambda$)**:\n",
    "\n",
    "- Estimation error $\\delta$ is defined as:\n",
    "\n",
    "$$ \\delta_t = r_{t+1} + \\gamma U(s_{t+1}) - U(s_t) $$\n",
    "\n",
    "- We can update the utility function as:\n",
    "\n",
    "$$ U_t(s) = U_t(s) + \\alpha \\delta_t e_t(s) \\quad for\\ all\\ s \\in S $$\n",
    "\n",
    "![temporal_difference_learning_lambda](imgs/temporal_difference_learning_at_lambda.jpg)\n",
    "\n",
    "**Python Implementation**:\n",
    "- The Python Implementation of $TD(\\lambda)$ is straightforward. We only need to add an eligibility matrix and its updated rule. \n",
    "- The **main loop** is much simpler than MC methods. In this case, we do not have any first-visit constraints and all we need to do is to apply the update rule.\n",
    "\n",
    "See the next cell.\n",
    "\n",
    "**Credits**:\n",
    "\n",
    "- [Wikipedia: Temporal Difference Learning](https:/en.wikipedia.org/wiki/Temporal_difference_learning)\n",
    "- [Blog: Dissecting Reinforcement Learning 3](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html)\n",
    "- [Blog: Suttons Temporal Difference Learning](http://kldavenport.com/suttons-temporal-difference-learning/)\n",
    "- [Youtube: IIT (Madras) -  Machine Learning #84 RL Framework, Temporal Difference (TD) Learning](https://www.youtube.com/watch?v=7eYzvQci9x0)\n",
    "- [Youtube: DeepMind's Richard Sutton - The Long-Term of AI & Temporal-Difference Learning](https://www.youtube.com/watch?v=EeMCEQa85tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Implementation of TD at lambda\n",
    "def update_utility(utility_matrix, trace_matrix, alpha, delta):\n",
    "    '''Return the updated utility matrix\n",
    "    \n",
    "    @param utility_matrix the matrix before the update\n",
    "    @param alpha the step size (learning rate)\n",
    "    @param delta the error (Target - Old_Estimate)\n",
    "    @return the updated utility matrix\n",
    "    '''\n",
    "    utility_matrix += alpha * delta * trace_matrix\n",
    "    return utility_matrix\n",
    "\n",
    "def update_eligibility(trace_matrix, grace, lambda_):\n",
    "    '''Return the updated trace_matrix\n",
    "    \n",
    "    @param trace_matrix the eligibility traces matrix\n",
    "    @param gamma discount factor\n",
    "    @param lambda_ the decaying value\n",
    "    @return the updated trace_matrix\n",
    "    '''\n",
    "    trace_matrix = trace_matrix * gamma * lambda_\n",
    "    return trace_matrix\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    # Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    \n",
    "    for step in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        \n",
    "        # Move one step in the environment adn gets obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        \n",
    "        # Estimate the error delta (Target - Old_Estimate)\n",
    "        delta = reward + gamma * \\\n",
    "            utility_matrix[new_observation[0], new_observation[1]] - \\\n",
    "            utility_matrix[observation[0], observation[1]]\n",
    "        \n",
    "        # Adding +1 in the trace matrix (only the state visited)\n",
    "        trace_matrix[observation[0], observation[1]] += 1\n",
    "        \n",
    "        # Update the utility matrix (all the states)\n",
    "        utility_matrix = update_utility(utility_matrix, trace_matrix, alpha, delta)\n",
    "        \n",
    "        # Update the trace_matrix (delaying) (all the states)\n",
    "        trace_matrix = update_eligibility(trace_matrix, gamma, lambda_)\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done:\n",
    "            break # return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Model Free Learning\n",
    "\n",
    "Topics covered:\n",
    "- SARSA vs Q-Learning\n",
    "- On-policy vs Off-policy\n",
    "- Model-based vs Model-Free\n",
    "- Temporal Difference vs Monte Carlo\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Check all that apply for SARSA:\n",
    "- [x] SARSA is on-policy\n",
    "- [x] SARSA uses action-value function.\n",
    "- [x] SARSA can be expressed as $ Q(S,A) \\leftarrow Q(S,A) + \\alpha (R + \\gamma Q(S',A') - Q(S,A)) $\n",
    "- [o] SARSA uses MDP (Markov Decision Process)\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "4. SARSA is model-free. MDP model is not used.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Check all that apply for SARSA & Q-Learning:\n",
    "- [o] SARSA can be expressed as ![quiz_2_image](imgs/quiz_2_q_learning_representation.png)\n",
    "- [x] Q-Learning is off-policy\n",
    "- [x] Both SARSA and Q-Learning are Model-Free Learning\n",
    "- [x] Q-Learning can be expressed as $ Q(S,A) \\leftarrow Q(S,A) + \\alpha (R + \\gamma max(Q(S',A')) - Q(S,A)) $\n",
    "    \n",
    "**Explanation**:\n",
    "\n",
    "1. SARSA is expressed as ![quiz 2 sarsa representation](imgs/quiz_2_sarsa_representation.png)\n",
    "2. Q-Learning is expressed as ![quiz 2 q-learning representation](imgs/quiz_2_q_learning_representation.png)\n",
    "3. Q-Learning is off-policy. SARSA is on-policy.\n",
    "4. Both SARSA and Q-Learning are representative Model-Free\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Check all that apply for SARSA, Q-Learning, on-policy and off-policy:\n",
    "- [x] SARSA is on-policy, Q-Policy is off-policy\n",
    "- [x] Q-Learning: We update Q-Function by assuming we are taking action $a$ that maximizes our post-state Q-Function $ Q(S_{t+a}, a) $.\n",
    "- [x] SARSA: We use the same policy to generate the current action at $t$ and the next action $A_{t+1}$.\n",
    "- [o] Q-Learning uses MDP (Markov Decision Process)\n",
    "    \n",
    "**Explanation**:\n",
    "\n",
    "4. Q-Learning is Model-Free. It does not need MDP Model.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Check all that apply for Temporal-Difference Learning:\n",
    "- [x] TD is Model-Free\n",
    "- [x] TD can learn before knowing the final outcome.\n",
    "- [x] TD can learn without the final outcome.\n",
    "- [x] TD has low variance, some bias over Monte Carlo.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Check all that apply for SARSA, Q-Learning, TD, Monte Carlo:\n",
    "- [o] Q-Learning uses the same policy to choose the next action $A'$\n",
    "- [o] SARSA uses the target policy (greedy) to choose the best next action $A'$ while following the behavior policy ($\\epislon$ greedy)\n",
    "- [o] TD has to wait until the end of the episode to get the reward.\n",
    "- [x] TD works for both episodic and continuous tasks.\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "1. Q-Learning is off policy so it uses the target policy (greedy) to choose the next best action $A'$ while following the behavior policy ($\\epsilon$ greedy)\n",
    "2. SARSA is on-policy so it uses the same policy to choose the next action $A'$\n",
    "3. TD does not have to wait until the end of the episode to get the reward unlike Monte Carlo which does.\n",
    "4. Monte Carlo only works for episodic tasks.\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Resources\n",
    "\n",
    "- [Medium: Ch 12.1: Model Free Reinforcement Learning Algorithms (Monte Carlo, SARSA, Q-Learning)](https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4)\n",
    "- [Towards Data Science: Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287)\n",
    "- [Quora: What is the difference between Q-Learning and SARSA Learning](https://www.quora.com/What-is-the-difference-between-Q-learning-and-SARSA-learning)\n",
    "- [David Silver: MC-TD Lecture](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/MC-TD.pdf)\n",
    "- [David Silver: Control Lecture](http://www0.cs.ucl.ac.uk/staff/d.silver/web/Teaching_files/control.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Tutorial for Ride Sharing\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Learn how to use Q Learning to optimize dispatch and routing for a ride sharing app.\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=tU6_Fc6bKyQ)\n",
    "- [Code Link](https://github.com/colinskow/move37/tree/master/q_learning)\n",
    "\n",
    "\n",
    "**Which Algorithm?**\n",
    "\n",
    "- Reinforcement learning is the ideal framework for solving complex problems like ride-sharing dispatch and routing\n",
    "- In a very simple environment with just a few passengers and no traffic considerations, we can use dynamic programming and brute force computing to get a perfect solution. This becomes unworkable as the complexity of the system increases\n",
    "- Q-Learning a is very good candidate for solving this problem while we learn from experience\n",
    "\n",
    "\n",
    "**Master the Fundamantals First**\n",
    "\n",
    "1. Basic Bellman Equation\n",
    "2. Dynamic Programming\n",
    "3. Monte Carlo\n",
    "4. Temporal Difference\n",
    "5. Q Learning\n",
    "6. DEEP Q Learning\n",
    "\n",
    "\n",
    "**OpenAI Gym Taxi v2**\n",
    "\n",
    "- We will use the OpenAI Gym Taxi v2 environment, where we will dispatch a taxi to pick up a passenger who must be dropped off in the designated location as quickly as possible.\n",
    "- Environment:\n",
    "    - 5x5 grid\n",
    "    - 5 possible states for passenger (4 pickups, plus taxi)\n",
    "    - 25 squares x 4 possible pick up locations x 5 passenger states = 500 discrete states\n",
    "    - Actions: north, south, east, west, pick-up, drop-off\n",
    "    - So Our Q-table size is 500 x 6 = 3000\n",
    "    - taxi and passenger always start off in a random square\n",
    "    - the taxi must pick up the passenger and drop off at the designated spot\n",
    "    - 20 points for correct drop-off\n",
    "    - every move costs 1 point\n",
    "    - Penalty -10 for illegal pick-up/drop-off\n",
    "    \n",
    "    \n",
    "**On Policy vs. Off Policy**\n",
    "\n",
    "- A Reinforcement Algorithm can either be On Policy or Off Policy\n",
    "- On-Policy: we must act based on our current policy\n",
    "    - Values only converge if we continue to greedily take the current best action according to our Q-Table\n",
    "    - e.g. we must be playing the game live as we update our model\n",
    "- Off Policy: any action is okay\n",
    "    - We can even train on recorded data from a human or another algorithm playing the game\n",
    "    - No matter what we do, the values will eventually converge\n",
    "\n",
    "\n",
    "**Q Learning**\n",
    "\n",
    "- Q = Quality\n",
    "- Q = long-term discounted reward we expect from taking action a in state s\n",
    "\n",
    "$$ V(s) = max_a(R(s,a) + \\gamma V(s')) \\quad\\quad where\\ Q(s,a) = R(s,a) + \\gamma V(s') \\\\ $$\n",
    "$$ \\pi(s) = max_a(\\ Q(s,a)\\ ) $$\n",
    "\n",
    "- Primary advantage: an off policy method. it doesn't matter which policy we follow as long as we play the episodes. But if we were to take only random actions that would create much longer episodes and our values would take a lot longer to converge, so Epsilon greedy is still the ideal way to play since it converges the fastest\n",
    "- No need to collect and store transitions for an entire episode\n",
    "- Update the Q-Table using the Q Learning algorithm using one transition which includes the state, the action, the reward received, state prime and action prime (SARSA)\n",
    "- Use temporal Difference algorithm to update our Q Table at each step\n",
    "\n",
    "\n",
    "**Temporal Difference**\n",
    "\n",
    "- How can we calculate returns without storing an entire episode and iterating backwards as we did in Monte Carlo?\n",
    "- Calculate average returns using a blending method, similar to a moving average (updates gradually)\n",
    "- Relies on (state, action reward, state', action')\n",
    "- $ a' = argmax(\\ Q(s')\\ ) $\n",
    "- $ V(s) = V(s) + \\alpha\\ [\\ r + \\gamma V(s') - V(s)\\ ] $\n",
    "- $ Q(s,a) = Q(s,a) + \\alpha\\ [\\ r + \\gamma argmax(\\ Q(s')\\ ) - V(s)\\ ] $\n",
    "- Policy is the action with the maximum Q, same as before\n",
    "\n",
    "Let's look at additional tricks to enhance the performance or our algorithm:\n",
    "\n",
    "\n",
    "**Adaptive Epsilon**\n",
    "\n",
    "- Epsilon is a probability of taking a random action in any given step\n",
    "- Values can converge faster is we taper or lower epsilon over time\n",
    "- Start out with an almost completely random policy\n",
    "- Taper $\\epsilon$ over time\n",
    "- $ \\epsilon = \\frac{\\epsilon_{0}}{1 + epsilon\\_taper * t} $\n",
    "- It will approach but never reach 0\n",
    "- Epsilon taper is a hyper-parameter (0.01 works well)\n",
    "\n",
    "\n",
    "**Adaptive Learning Rate**\n",
    "\n",
    "- Learning rate expressed as alpha is the blending factor we use to update our Q-Table with each new unit of experience\n",
    "- If $ \\alpha $ is too high we may miss the ideal policy sweet spot\n",
    "- If $ \\alpha $ is too low training will take forever\n",
    "- To get both speed and accuracy we use an adaptive learning rate\n",
    "- One approach is to start high and taper it down at each step. But some states are visited more often than others so a more intelligent way to do this is to count how many times we've experienced each state action pair and taper based on that experience count\n",
    "- $ \\alpha(s,a) = \\frac{\\alpha_{0}}{1 + count(s,a)* alpha\\_taper} $\n",
    "- Alpha taper is a hyper-parameter (0.01 works well)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Interview\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "I ask 67 questions to Dr Alan Baratz - EVP, R&D and Chief Product Officer of D-Wave Systems. D-Wave has built an incredible quantum computer, and invited me to Vancouver to attend a special launch event of their new Leap system, which allows any developer to use quantum computing very easily in the cloud. In this interview, Alan walks me through the D-Wave facility in Vancouver, and we even get to step inside the quantum computer room. Enjoy!\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=Ewf_gBWBH2A)\n",
    "- [D-Wave Website](https://www.dwavesys.com/home)\n",
    "- [Youtube: Quantum Computers Explained](https://www.youtube.com/watch?v=JhHMJCUmq28)\n",
    "- [Hackernoon: Quantum Computing Explained!](https://hackernoon.com/quantum-computing-explained-a114999299ca)\n",
    "- [Youtube: Quantum Algorithm](https://www.youtube.com/watch?v=LhtnECml-KI)\n",
    "- [Youtube: Quantum Machine Learning](https://www.youtube.com/watch?v=DmzWsvb-Un4)\n",
    "- [Clerro: Quantum Computing Explained](https://www.clerro.com/guide/580/quantum-computing-explained)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
