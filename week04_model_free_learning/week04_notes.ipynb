{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04 Notes - Model Free Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dopamine-in-Neuroscience\" data-toc-modified-id=\"Dopamine-in-Neuroscience-1\">Dopamine in Neuroscience</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.2\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.3\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Reading-Assignment:-Model-Based-vs.-Model-Free-Learning-\" data-toc-modified-id=\"Reading-Assignment:-Model-Based-vs.-Model-Free-Learning--2\">Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a></a></span></li><li><span><a href=\"#Homework-Assignment:-Q-Learning\" data-toc-modified-id=\"Homework-Assignment:-Q-Learning-3\">Homework Assignment: Q Learning</a></span></li><li><span><a href=\"#Temporal-Difference-Learning\" data-toc-modified-id=\"Temporal-Difference-Learning-4\">Temporal Difference Learning</a></span></li><li><span><a href=\"#Quiz:-Model-Free-Learning\" data-toc-modified-id=\"Quiz:-Model-Free-Learning-5\">Quiz: Model Free Learning</a></span></li><li><span><a href=\"#Q-Learning-Tutorial-for-Ride-Sharing\" data-toc-modified-id=\"Q-Learning-Tutorial-for-Ride-Sharing-6\">Q Learning Tutorial for Ride Sharing</a></span></li><li><span><a href=\"#Quantum-Interview\" data-toc-modified-id=\"Quantum-Interview-7\">Quantum Interview</a></span><ul class=\"toc-item\"><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-7.1\">Learning Resources</a></span></li></ul></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine in Neuroscience\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "The human brain is wondrous in its capabilities. The rules that govern its prowess at so many tasks are becoming slightly clearer everyday. In this video, I'll highlight how 4 key reinforcement learning algorithms help explain how the human brain works, specifically through the lens of the neurotransmitter known as 'dopamine'. These algorithms have been used to help train everything from autopilot systems for airplanes, to video game bots. TD-Learning, Rescorla-Wagner, Kalman Filters, and Bayesian Learning, all in one go!\n",
    "\n",
    "### Notes\n",
    "- **Associative Learning Theory**\n",
    "    - Describes the process by which a person or animal learns an association between 2 stimuli\n",
    "    - Basic claims are that Reinforcement Learning is the acquisition of associations between states, actions and rewards\n",
    "\n",
    "|    Type    | Punishment<br/>(decreasing behavior) | Reinforcement<br/>(increasing behavior)  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Positive<br/>_(adding)_ | adding something<br/>to<br/>decrease behavior | adding something<br/>to<br/> increase behavior |\n",
    "| Negative<br/>_(subtracting)_ | subtracting something<br/>to<br/>decrease behavior | subtracting something<br/>to<br/>increase behavior |\n",
    "\n",
    "\n",
    "- **Rescorla-Wagner Model**\n",
    "    - Most influential idea of associative learning theory\n",
    "    - Prediction error based learning model where stimuli acquire value when there is a mismatch between the prediction and the outcome\n",
    "    - In the equation, the value of stimulus s at trial T is set equal to the value of stimulus s at the previous time step plus the reward and the expectation. The learning rate defines how much this prediction error is weighted\n",
    "    - This prediction error states that when the animal gets more reward than it expected, it leads to strengthening of the associative weights and a negative prediction error leads to a weakening of the associative weights\n",
    "    \n",
    "    $$ \\Delta{V_n} = c(\\lambda - V_{n-1})$$\n",
    "    \n",
    "    - where:\n",
    "        - $\\Delta{V_n}$: change in associative strength for CS on one trial\n",
    "        - $c$: represent salience of CS and US; a constant (0.0 - 1.0)\n",
    "        - $\\lambda$: maximum associative strength (magnitude of UR)\n",
    "        - $V_{n-1})$: associative strength _already_ accured by CS\n",
    "        - CS: conditional stimuli\n",
    "        - US: unconditioned stimuli\n",
    "        \n",
    "    - Groundbreaking because:\n",
    "        1. Able to explain the conditioning phenomena\n",
    "        2. Useful in early Natural Language Processing Systems\n",
    "    - While it provided a basis for associative learning theory, it only estimated a single value. We know that biological brains are able to represent uncertainty about the world somewhere since the world is full of uncertainty\n",
    "    - Probability theory suggests that to properly represent a biological brains uncertainty of the world, it should utilize a probability distribution over the possible weights instead of a single value\n",
    "    - Use Bayes Rule:\n",
    "     \n",
    "    $$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "    \n",
    "    - The Bayesian generalization of the Rescorla-Wagner Model is embodied in Kalman Filter.\n",
    "    \n",
    "    \n",
    "- **Kalman Filter**\n",
    "    - States that uncertainty grows over time due to the random diffusion of the weights\n",
    "    - This uncertainty can be reduced by observing the data\n",
    "    - Uses a series of measurements observed over time and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone by estimating a joint probability distribution over the variables for each time frame\n",
    "    - Has numerous applications:\n",
    "        - Navigation\n",
    "        - Control of Vehicles (aircrafts)\n",
    "        - Time Series Analysis\n",
    "        - Robotics\n",
    "    - Works by modeling the central nervous systems control of movement\n",
    "    - Because of the time delay between issuing motor commands and receiving sensory feedback, the Kalman Filter is a realistic model for making estimates of the current state of a motor system and using commands.\n",
    "    - It's a two step process:\n",
    "        - In the prediction step, it produces estimates of the current state variables along with their uncertainties \n",
    "        - Once the outcome of the next measurement is observed, these estimates are updated using a weighted average with the most weights given to estimates with higher certainty\n",
    "     - It's a recursive real-time algorithm using just the present input measurements and the previously calculated state as well as its uncertainty matrix \n",
    "\n",
    "So far, we have broken down tasks into trials but real life operates in continuous time and our algorithms are short-sighted. They have been only able to predict the immediate reward, the one that will be received in the very next state. To extend the capabilities of what we are able to model mathematically, we need to switch to modern reinforcement learning theory from classical learning theory.\n",
    "\n",
    "Let's focus on a specific sequential decision problem - a mouse navigating to the exit of a maze. There are two popular classes of Reinforcement Learning algorithms we can use to help solve this problem:\n",
    "\n",
    "|    Type    | Model Free<br/>Less online deliberation | Model Based<br/>More online deliberation  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Learn: | Policy $\\pi$ | Model of<br/> R and T |\n",
    "| Online: | $\\pi(s)$ | Solve MDP |\n",
    "| Method: | Learning from<br/>demonstration | Adaptive dynamic<br/>programming |\n",
    "| Pros: | Simpler execution | Fewer examples needed to learn? |\n",
    "\n",
    "\n",
    "- **Temporal Difference Learning**\n",
    "    - Extended the Rescorla-Wagner Model by introducing a discount factor into the prediction error, which helps define how much a reward matters to an agent depending on when in time it's received\n",
    "    - We can make rewards that happen in the near term worth more\n",
    "    - Invented by 2 researchers (Sutton & Bartow) in the 1980's\n",
    "    - It has been used for:\n",
    "        - Game bots that can beat Humans (most popular is DeepMind and it's DeepQ Learning Algorithm that's able to beat many Atari games)\n",
    "    - TD Learning helps capture some important properties of temporal dynamics as well as dopamine responses but it lacks the uncertainty tracking mechanism of the Kalman Filter.\n",
    "    - So we need a Bayesian version of TD Learning, which is called Kalman TD\n",
    "    - Instead of estimating a single value of the weights, we estimating a mean and a covariance matrix for the weights of our model. \n",
    "    \n",
    "We can think of all four of these models along two dimensions based on what kind of estimator they are (Bayesian or Point based) and what the target they're trying to estimate is (immediate reward or value):\n",
    "\n",
    "|    Estimator\\Target    | Reward  | Value  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Point    | Rescorla-Wagner | TD Learning |\n",
    "| Bayesian | Kalman Filter | Kalman TD |\n",
    "\n",
    "\n",
    "### Take Aways\n",
    "- Associative learning is a learning process in which a new response becomes associated with a particular stimulus\n",
    "- When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainty about the world\n",
    "- Temporal Difference Learning is a Model Free Learning technique that predicts the expected value of a variable occurring at the end of a sequence of states\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=-vhYoS3751g)\n",
    "- [Code Link](https://github.com/llSourcell/Mathematics_of_Dopamine)\n",
    "- [Youtube: The Rescorla-Wagner Model](https://www.youtube.com/watch?v=pYyUSh1veoo)\n",
    "- [Youtube: TD Learning - Richard S. Sutton](https://www.youtube.com/watch?v=LyCpuLikLyQ)\n",
    "- [Youtube: Special Topics - The Kalman Filter](https://www.youtube.com/watch?v=CaCcOwJPytQ)\n",
    "- [Youtube: Bayesian Learning](https://www.youtube.com/watch?v=C2OUfJW5UNM)\n",
    "- [PDF: A Unifying Probabilistic View of Associative Learning](https://dash.harvard.edu/bitstream/handle/1/23845336/4633133.pdf?sequence=1&isAllowed=y)\n",
    "- [Book: Chapter 9 Temporal-Difference Learning](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a>\n",
    "- [Model Based vs. Model Free Reading Assignment](https://www.theschool.ai/wp-content/uploads/2018/09/Move37-Reading-Assignment-Model-Based-vs-Model-Free-1.pdf)\n",
    "\n",
    "**Model**: a plan for our agent. When we have a set of defined state transition probabilities, we call that working with a model. Reinforcement learning can be applied with or without a model, or even used to define a model.\n",
    "\n",
    "A complete model of the environment is required to do Dynamic Programming. If our agent doesn't have a complete map of what to expect, we can instead employ what is called **model-free learning**, where the model learns via trial an error.\n",
    "\n",
    "For some board games such as Chess and Go, although we can accurately model the environment's dynamics, computational power constrains us from calculating the Bellman Optimality equation. This is where Model-free Learning methods shine. We handle this situation by optimizing for a smaller subset of states that are frequently encountered, at the cost of knowing less about the infrequently visited states.\n",
    "\n",
    "Further Reading:\n",
    "- [Medium: Model Free Reinforcement Learning Algorithms](https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4)\n",
    "- [Book: Temporal-Difference Learning (RL: An Introducion Chapeter 6)](http://incompleteideas.net/book/the-book.html)\n",
    "- [PDF: Reward-Based Learning, Model-Based and Model-Free (2014)](https://www.quentinhuys.com/pub/HuysEa14-ModelBasedModelFree.pdf)\n",
    "- [BAIR: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://bair.berkeley.edu/blog/2018/04/26/tdm/)\n",
    "- [ArXiv: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://arxiv.org/abs/1802.09081)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment: Q Learning\n",
    "\n",
    "This weeks homework assignment is to implement Q learning from scratch for the gridworld environment. Use this [repository](https://github.com/rlcode/reinforcement-learning/tree/master/1-grid-world) as a guide, but try not to peak at the Q learning code, recreate it, then check your code with it. Good luck!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=f4zTDRavVq0)\n",
    "\n",
    "**TD Learning**:\n",
    "- This general rule can be summarized as:\n",
    "\n",
    "$$ New\\ Estimate \\leftarrow Old\\ Estimate + Step\\ Size\\ [\\ Target - Old\\ Estimate\\ ] \\\\ $$\n",
    "$$ Where\\ [\\ Target - Old\\ Estimate\\ ]\\ \\rightarrow estimation\\ error\\ (\\delta) \\\\ $$\n",
    "\n",
    "- The Target is the expected return of the state:\n",
    "\n",
    "$$ Target\\ = E_{\\pi}\\ [\\ \\sum_{k=0}^{\\infty} \\gamma^{k}r_t + k + 1 ] $$\n",
    "\n",
    "- The learning rate is a parameter which determines to what extent the error has to be integrated in the new estimation. If the step size is 0, the agent does not learn anything at all. If the step size is 1, the agent considers only the most recent information.\n",
    "\n",
    "\n",
    "$TD(\\lambda) $:\n",
    "\n",
    "- TD(0) algorithm does not take past states into account\n",
    "- What matters in TD(0) is the current state and the state at $t + 1$\n",
    "- However it would be useful to extend what is learned at $t + 1$ to previous states. To achieve this objective, it is necessary to have a **short-term memory mechanism** to store the states which have been visited in the last steps.\n",
    "\n",
    "For each state $s$ at time $t$ we can defined $e_t(s)$ as the **eligibility trace**:\n",
    "\n",
    "$$\n",
    "e_t(s) = \\left\\{\n",
    "    \\begin{array}{l}\n",
    "      \\gamma \\lambda e_{t - 1}(s)   \\quad\\quad    if s \\neq s_t;\\\\\n",
    "      \\gamma \\lambda e_{t - 1}(s) + 1 \\quad      if s = s_t;\n",
    "    \\end{array}\n",
    "  \\right.\n",
    "$$\n",
    "\n",
    "Here $\\gamma$ is the discount rate and $\\lambda \\in [0, 1]$ is a decay parameter called **trace-decay** or **accumulating trace** which defines the update weight for each state visited. When $0 < \\lambda < 1$, the traces decrease in time.\n",
    "\n",
    "This allows giving a small weight to infrequent states. For $\\lambda = 0$, we have the TD(0) case, and only the immediately preceding prediction is updated. For $\\lambda = 1$, we have TD(1) where all the preceding predictions are equally updated.\n",
    "\n",
    "Now it's time to define the **update rule for TD($\\lambda$)**:\n",
    "\n",
    "- Estimation error \\$delta\\ is defined as:\n",
    "\n",
    "$$ \\delta_t = r_{t+1} + \\gamma U(s_{t+1}) - U(s_t) $$\n",
    "\n",
    "- We can update the utility function as:\n",
    "\n",
    "$$ U_t(s) = U_t(s) + \\alpha \\delta_t e_t(s) \\quad for\\ all\\ s \\in S $$\n",
    "\n",
    "![temporal_difference_learning_lambda](imgs/temporal_difference_learning_at_lambda.jpg)\n",
    "\n",
    "**Python Implementation**:\n",
    "- The Python Implementation of $TD(\\lambda)$ is straightforward. We only need to add an eligibility matrix and its updated rule. \n",
    "- The **main loop** is much simpler than MC methods. In this case, we do not have any first-visit constraints and all we need to do is to apply the update rule.\n",
    "\n",
    "See the next cell.\n",
    "\n",
    "**Credits**:\n",
    "\n",
    "- [Wikipedia: Temporal Difference Learning](https:/en.wikipedia.org/wiki/Temporal_difference_learning)\n",
    "- [Blog: Dissecting Reinforcement Learning 3](https://mpatacchiola.github.io/blog/2017/01/29/dissecting-reinforcement-learning-3.html)\n",
    "- [Blog: Suttons Temporal Difference Learning](http://kldavenport.com/suttons-temporal-difference-learning/)\n",
    "- [Youtube: IIT (Madras) -  Machine Learning #84 RL Framework, Temporal Difference (TD) Learning](https://www.youtube.com/watch?v=7eYzvQci9x0)\n",
    "- [Youtube: DeepMind's Richard Sutton - The Long-Term of AI & Temporal-Difference Learning](https://www.youtube.com/watch?v=EeMCEQa85tw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Implementation of TD at lambda\n",
    "def update_utility(utility_matrix, trace_matrix, alpha, delta):\n",
    "    '''Return the updated utility matrix\n",
    "    \n",
    "    @param utility_matrix the matrix before the update\n",
    "    @param alpha the step size (learning rate)\n",
    "    @param delta the error (Target - Old_Estimate)\n",
    "    @return the updated utility matrix\n",
    "    '''\n",
    "    utility_matrix += alpha * delta * trace_matrix\n",
    "    return utility_matrix\n",
    "\n",
    "def update_eligibility(trace_matrix, grace, lambda_):\n",
    "    '''Return the updated trace_matrix\n",
    "    \n",
    "    @param trace_matrix the eligibility traces matrix\n",
    "    @param gamma discount factor\n",
    "    @param lambda_ the decaying value\n",
    "    @return the updated trace_matrix\n",
    "    '''\n",
    "    trace_matrix = trace_matrix * gamma * lambda_\n",
    "    return trace_matrix\n",
    "\n",
    "for epoch in range(tot_epoch):\n",
    "    # Reset and return the first observation\n",
    "    observation = env.reset(exploring_starts=True)\n",
    "    \n",
    "    for step in range(1000):\n",
    "        # Take the action from the action matrix\n",
    "        action = policy_matrix[observation[0], observation[1]]\n",
    "        \n",
    "        # Move one step in the environment adn gets obs and reward\n",
    "        new_observation, reward, done = env.step(action)\n",
    "        \n",
    "        # Estimate the error delta (Target - Old_Estimate)\n",
    "        delta = reward + gamma * \\\n",
    "            utility_matrix[new_observation[0], new_observation[1]] - \\\n",
    "            utility_matrix[observation[0], observation[1]]\n",
    "        \n",
    "        # Adding +1 in the trace matrix (only the state visited)\n",
    "        trace_matrix[observation[0], observation[1]] += 1\n",
    "        \n",
    "        # Update the utility matrix (all the states)\n",
    "        utility_matrix = update_utility(utility_matrix, trace_matrix, alpha, delta)\n",
    "        \n",
    "        # Update the trace_matrix (delaying) (all the states)\n",
    "        trace_matrix = update_eligibility(trace_matrix, gamma, lambda_)\n",
    "        observation = new_observation\n",
    "        \n",
    "        if done:\n",
    "            break # return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Model Free Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Tutorial for Ride Sharing\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Learn how to use Q Learning to optimize dispatch and routing for a ride sharing app.\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=tU6_Fc6bKyQ)\n",
    "- [Code Link](https://github.com/colinskow/move37/tree/master/q_learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Interview\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "I ask 67 questions to Dr Alan Baratz - EVP, R&D and Chief Product Officer of D-Wave Systems. D-Wave has built an incredible quantum computer, and invited me to Vancouver to attend a special launch event of their new Leap system, which allows any developer to use quantum computing very easily in the cloud. In this interview, Alan walks me through the D-Wave facility in Vancouver, and we even get to step inside the quantum computer room. Enjoy!\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=Ewf_gBWBH2A)\n",
    "- [D-Wave Website](https://www.dwavesys.com/home)\n",
    "- [Youtube: Quantum Computers Explained](https://www.youtube.com/watch?v=JhHMJCUmq28)\n",
    "- [Hackernoon: Quantum Computing Explained!](https://hackernoon.com/quantum-computing-explained-a114999299ca)\n",
    "- [Youtube: Quantum Algorithm](https://www.youtube.com/watch?v=LhtnECml-KI)\n",
    "- [Youtube: Quantum Machine Learning](https://www.youtube.com/watch?v=DmzWsvb-Un4)\n",
    "- [Clerro: Quantum Computing Explained](https://www.clerro.com/guide/580/quantum-computing-explained)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
