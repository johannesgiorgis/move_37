{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 04 Notes - Model Free Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Dopamine-in-Neuroscience\" data-toc-modified-id=\"Dopamine-in-Neuroscience-1\">Dopamine in Neuroscience</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.2\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.3\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Reading-Assignment:-Model-Based-vs.-Model-Free-Learning-\" data-toc-modified-id=\"Reading-Assignment:-Model-Based-vs.-Model-Free-Learning--2\">Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a></a></span></li><li><span><a href=\"#Homework-Assignment:-Q-Learning\" data-toc-modified-id=\"Homework-Assignment:-Q-Learning-3\">Homework Assignment: Q Learning</a></span></li><li><span><a href=\"#Temporal-Difference-Learning\" data-toc-modified-id=\"Temporal-Difference-Learning-4\">Temporal Difference Learning</a></span></li><li><span><a href=\"#Quiz:-Model-Free-Learning\" data-toc-modified-id=\"Quiz:-Model-Free-Learning-5\">Quiz: Model Free Learning</a></span></li><li><span><a href=\"#Q-Learning-Tutorial-for-Ride-Sharing\" data-toc-modified-id=\"Q-Learning-Tutorial-for-Ride-Sharing-6\">Q Learning Tutorial for Ride Sharing</a></span></li><li><span><a href=\"#Quantum-Interview\" data-toc-modified-id=\"Quantum-Interview-7\">Quantum Interview</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dopamine in Neuroscience\n",
    "\n",
    "The human brain is wondrous in its capabilities. The rules that govern its prowess at so many tasks are becoming slightly clearer everyday. In this video, I'll highlight how 4 key reinforcement learning algorithms help explain how the human brain works, specifically through the lens of the neurotransmitter known as 'dopamine'. These algorithms have been used to help train everything from autopilot systems for airplanes, to video game bots. TD-Learning, Rescorla-Wagner, Kalman Filters, and Bayesian Learning, all in one go!\n",
    "\n",
    "### Notes\n",
    "- **Associative Learning Theory**\n",
    "    - Describes the process by which a person or animal learns an association between 2 stimuli\n",
    "    - Basic claims are that Reinforcement Learning is the acquisition of associations between states, actions and rewards\n",
    "\n",
    "|    Type    | Punishment<br/>(decreasing behavior) | Reinforcement<br/>(increasing behavior)  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Positive<br/>_(adding)_ | adding something<br/>to<br/>decrease behavior | adding something<br/>to<br/> increase behavior |\n",
    "| Negative<br/>_(subtracting)_ | subtracting something<br/>to<br/>decrease behavior | subtracting something<br/>to<br/>increase behavior |\n",
    "\n",
    "\n",
    "- **Rescorla-Wagner Model**\n",
    "    - Most influential idea of associative learning theory\n",
    "    - Prediction error based learning model where stimuli acquire value when there is a mismatch between the prediction and the outcome\n",
    "    - In the equation, the value of stimulus s at trial T is set equal to the value of stimulus s at the previous time step plus the reward and the expectation. The learning rate defines how much this prediction error is weighted\n",
    "    - This prediction error states that when the animal gets more reward than it expected, it leads to strengthening of the associative weights and a negative prediction error leads to a weakening of the associative weights\n",
    "    \n",
    "    $$ \\Delta{V_n} = c(\\lambda - V_{n-1})$$\n",
    "    \n",
    "    - where:\n",
    "        - $\\Delta{V_n}$: change in associative strength for CS on one trial\n",
    "        - $c$: represent salience of CS and US; a constant (0.0 - 1.0)\n",
    "        - $\\lambda$: maximum associative strength (magnitude of UR)\n",
    "        - $V_{n-1})$: associative strength _already_ accured by CS\n",
    "        - CS: conditional stimuli\n",
    "        - US: unconditioned stimuli\n",
    "        \n",
    "    - Groundbreaking because:\n",
    "        1. Able to explain the conditioning phenomena\n",
    "        2. Useful in early Natural Language Processing Systems\n",
    "    - While it provided a basis for associative learning theory, it only estimated a single value. We know that biological brains are able to represent uncertainty about the world somewhere since the world is full of uncertainty\n",
    "    - Probability theory suggests that to properly represent a biological brains uncertainty of the world, it should utilize a probability distribution over the possible weights instead of a single value\n",
    "    - Use Bayes Rule:\n",
    "     \n",
    "    $$ P(A|B) = \\frac{P(B|A)P(A)}{P(B)} $$\n",
    "    \n",
    "    - The Bayesian generalization of the Rescorla-Wagner Model is embodied in Kalman Filter.\n",
    "    \n",
    "    \n",
    "- **Kalman Filter**\n",
    "    - States that uncertainty grows over time due to the random diffusion of the weights\n",
    "    - This uncertainty can be reduced by observing the data\n",
    "    - Uses a series of measurements observed over time and produces estimates of unknown variables that tend to be more accurate than those based on a single measurement alone by estimating a joint probability distribution over the variables for each time frame\n",
    "    - Has numerous applications:\n",
    "        - Navigation\n",
    "        - Control of Vehicles (aircrafts)\n",
    "        - Time Series Analysis\n",
    "        - Robotics\n",
    "    - Works by modeling the central nervous systems control of movement\n",
    "    - Because of the time delay between issuing motor commands and receiving sensory feedback, the Kalman Filter is a realistic model for making estimates of the current state of a motor system and using commands.\n",
    "    - It's a two step process:\n",
    "        - In the prediction step, it produces estimates of the current state variables along with their uncertainties \n",
    "        - Once the outcome of the next measurement is observed, these estimates are updated using a weighted average with the most weights given to estimates with higher certainty\n",
    "     - It's a recursive real-time algorithm using just the present input measurements and the previously calculated state as well as its uncertainty matrix \n",
    "\n",
    "So far, we have broken down tasks into trials but real life operates in continuous time and our algorithms are short-sighted. They have been only able to predict the immediate reward, the one that will be received in the very next state. To extend the capabilities of what we are able to model mathematically, we need to switch to modern reinforcement learning theory from classical learning theory.\n",
    "\n",
    "Let's focus on a specific sequential decision problem - a mouse navigating to the exit of a maze. There are two popular classes of Reinforcement Learning algorithms we can use to help solve this problem:\n",
    "\n",
    "|    Type    | Model Free<br/>Less online deliberation | Model Based<br/>More online deliberation  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Learn: | Policy $\\pi$ | Model of<br/> R and T |\n",
    "| Online: | $\\pi(s)$ | Solve MDP |\n",
    "| Method: | Learning from<br/>demonstration | Adaptive dynamic<br/>programming |\n",
    "| Pros: | Simpler execution | Fewer examples needed to learn? |\n",
    "\n",
    "\n",
    "- **Temporal Difference Learning**\n",
    "    - Extended the Rescorla-Wagner Model by introducing a discount factor into the prediction error, which helps define how much a reward matters to an agent depending on when in time it's received\n",
    "    - We can make rewards that happen in the near term worth more\n",
    "    - Invented by 2 researchers (Sutton & Bartow) in the 1980's\n",
    "    - It has been used for:\n",
    "        - Game bots that can beat Humans (most popular is DeepMind and it's DeepQ Learning Algorithm that's able to beat many Atari games)\n",
    "    - TD Learning helps capture some important properties of temporal dynamics as well as dopamine responses but it lacks the uncertainty tracking mechanism of the Kalman Filter.\n",
    "    - So we need a Bayesian version of TD Learning, which is called Kalman TD\n",
    "    - Instead of estimating a single value of the weights, we estimating a mean and a covariance matrix for the weights of our model. \n",
    "    \n",
    "We can think of all four of these models along two dimensions based on what kind of estimator they are (Bayesian or Point based) and what the target they're trying to estimate is (immediate reward or value):\n",
    "\n",
    "|    Estimator\\Target    | Reward  | Value  |\n",
    "| :-------------: | :-------------: | :-----: |\n",
    "| Point    | Rescorla-Wagner | TD Learning |\n",
    "| Bayesian | Kalman Filter | Kalman TD |\n",
    "\n",
    "\n",
    "### Take Aways\n",
    "- Associative learning is a learning process in which a new response becomes associated with a particular stimulus\n",
    "- When we build mathematical models of learning, we can use distributions instead of single values to help represent uncertainty about the world\n",
    "- Temporal Difference Learning is a Model Free Learning technique that predicts the expected value of a variable occurring at the end of a sequence of states\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=-vhYoS3751g)\n",
    "- [Code Link](https://github.com/llSourcell/Mathematics_of_Dopamine)\n",
    "- [Youtube: The Rescorla-Wagner Model](https://www.youtube.com/watch?v=pYyUSh1veoo)\n",
    "- [Youtube: TD Learning - Richard S. Sutton](https://www.youtube.com/watch?v=LyCpuLikLyQ)\n",
    "- [Youtube: Special Topics - The Kalman Filter](https://www.youtube.com/watch?v=CaCcOwJPytQ)\n",
    "- [Youtube: Bayesian Learning](https://www.youtube.com/watch?v=C2OUfJW5UNM)\n",
    "- [PDF: A Unifying Probabilistic View of Associative Learning](https://dash.harvard.edu/bitstream/handle/1/23845336/4633133.pdf?sequence=1&isAllowed=y)\n",
    "- [Book: Chapter 9 Temporal-Difference Learning](https://web.stanford.edu/group/pdplab/pdphandbook/handbookch10.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignment: Model Based vs. Model Free Learning <a name=\"dopamine\"></a>\n",
    "- [Model Based vs. Model Free Reading Assignment](https://www.theschool.ai/wp-content/uploads/2018/09/Move37-Reading-Assignment-Model-Based-vs-Model-Free-1.pdf)\n",
    "\n",
    "**Model**: a plan for our agent. When we have a set of defined state transition probabilities, we call that working with a model. Reinforcement learning can be applied with or without a model, or even used to define a model.\n",
    "\n",
    "A complete model of the environment is required to do Dynamic Programming. If our agent doesn't have a complete map of what to expect, we can instead employ what is called **model-free learning**, where the model learns via trial an error.\n",
    "\n",
    "For some board games such as Chess and Go, although we can accurately model the environment's dynamics, computational power constrains us from calculating the Bellman Optimality equation. This is where Model-free Learning methods shine. We handle this situation by optimizing for a smaller subset of states that are frequently encountered, at the cost of knowing less about the infrequently visited states.\n",
    "\n",
    "Further Reading:\n",
    "- [Medium: Model Free Reinforcement Learning Algorithms](https://medium.com/deep-math-machine-learning-ai/ch-12-1-model-free-reinforcement-learning-algorithms-monte-carlo-sarsa-q-learning-65267cb8d1b4)\n",
    "- [Book: Temporal-Difference Learning (RL: An Introducion Chapeter 6)](http://incompleteideas.net/book/the-book.html)\n",
    "- [PDF: Reward-Based Learning, Model-Based and Model-Free (2014)](https://www.quentinhuys.com/pub/HuysEa14-ModelBasedModelFree.pdf)\n",
    "- [BAIR: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://bair.berkeley.edu/blog/2018/04/26/tdm/)\n",
    "- [ArXiv: Temporal Difference Methods: Model-Free Deep RL for Model-Based Control (2018)](https://arxiv.org/abs/1802.09081)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment: Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Model Free Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q Learning Tutorial for Ride Sharing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Interview"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
