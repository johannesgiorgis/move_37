{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Midterm Assignment (Make a Bipedal Robot Walk) <a class=\"tocSkip\">\n",
    "\n",
    "Its midterm season Wizards! The midterm is to make a bipedal humanoid robot walk in a simulation.\n",
    "You can use OpenAI Gym for the environment, this [link](https://github.com/search?q=bipedal+gym) shows some potential solutions that you can use to help you when you build your own. Submit your repository to schoolofaigrading@gmail.com . We will review your work and send back grades! Weâ€™re looking for good documentation, readable code, and bonus points for using reinforcement learning in a novel way for this challenge. Due date for all midterms is October 29, 2018 at 12 PM PST. Good luck!\n",
    "\n",
    "---\n",
    "\n",
    "### Explanation <a class=\"tocSkip\">\n",
    "\n",
    "For this homework assignment, I collaborated with [Andrew Key](https://github.com/redpanda-ai).\n",
    "\n",
    "We started by comparing ARS with Evolutionary Strategies. Specifically, we compared Colin's [ARS script](https://github.com/colinskow/move37/tree/master/ars) with Alirezamika's [Bi-pedal ES](https://github.com/alirezamika/bipedal-es).\n",
    "\n",
    "We trained them both side by side. Initially, Evolutionary Strategies approach looked more promising as it quickly learned but it plateaued around 150 reward. For context, the 'successful' reward threshold for this Bipedal Walker environment is over 300. ARS on the other hand took much longer to train, yet reached over 300 rewards consistently after playing through 500 episodes. This led us to use ARS instead of Evolutionary Strategies.\n",
    "\n",
    "We then set out to see how much faster we could train our agent using ARS via tuning the Hyperparameters - if ARS without tuning could do it after 500 episodes, could we half it? We found that noise played a big factor in this. An ideal amount of noise could dramatically speed up the learning and we were able to get close to 300 rewards consistently with only 200 episodes. This ideal amount was found to be around 0.2 - 0.3. We still need to do more testing to successfully conclude this but our initial results indicate so as some of our results were worse than expected while noise was set within this range.\n",
    "\n",
    "In our current implementation, we save all the rewards for each episode within each experiment to a numpy array and save it to a .npy file under rewards/ directory.\n",
    "\n",
    "**Other Experiments**:\n",
    "\n",
    "- Save theta values to a pickle file in order to allow continuous training as well as playback to see how our agent performed - were not able to get it to work quite right\n",
    "\n",
    "\n",
    "**Potential Next Steps**:\n",
    "\n",
    "- Create plots to compare how rewards trended over the course of an experiment\n",
    "- Allow experiments to run in parallel to speed up capturing experiment data\n",
    "- Compare ARS vs. Evolutionary Strategies and other Algorithms more in depth\n",
    "- Continue to tune Hyperparameters (Learning rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Code\" data-toc-modified-id=\"Code-1\">Code</a></span><ul class=\"toc-item\"><li><span><a href=\"#import-libraries-&amp;-global-variables\" data-toc-modified-id=\"import-libraries-&amp;-global-variables-1.1\">import libraries &amp; global variables</a></span></li><li><span><a href=\"#slider-widget-code\" data-toc-modified-id=\"slider-widget-code-1.2\">slider widget code</a></span></li><li><span><a href=\"#Hyperparameters-Class\" data-toc-modified-id=\"Hyperparameters-Class-1.3\">Hyperparameters Class</a></span></li><li><span><a href=\"#ARS-Agent\" data-toc-modified-id=\"ARS-Agent-1.4\">ARS Agent</a></span></li><li><span><a href=\"#Helper-Functions\" data-toc-modified-id=\"Helper-Functions-1.5\">Helper Functions</a></span></li><li><span><a href=\"#Hyperparameter-Sliders\" data-toc-modified-id=\"Hyperparameter-Sliders-1.6\">Hyperparameter Sliders</a></span></li><li><span><a href=\"#Train-the-agent\" data-toc-modified-id=\"Train-the-agent-1.7\">Train the agent</a></span></li></ul></li><li><span><a href=\"#Experiment-Results\" data-toc-modified-id=\"Experiment-Results-2\">Experiment Results</a></span></li><li><span><a href=\"#Sample-Video\" data-toc-modified-id=\"Sample-Video-3\">Sample Video</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### import libraries & global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import gym\n",
    "import ipywidgets as widgets\n",
    "\n",
    "from gym import wrappers\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "from ipywidgets import interact, interactive\n",
    "from IPython.display import display\n",
    "\n",
    "ENV_NAME = 'BipedalWalker-v2'\n",
    "hyperparameters = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### slider widget code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Code to create hyperparameter widgets (sliders)\"\"\"\n",
    "seed = widgets.IntSlider(\n",
    "    description='seed:', value=1946, min=0, max=3000, orientation='vertical')\n",
    "num_episodes = widgets.IntSlider(\n",
    "    description='num_episodes:', value=200, min=0, max=10000, step=10, orientation='vertical')\n",
    "learning_rate = widgets.FloatSlider(\n",
    "    description='learning_rate:', value=0.2, min=0.0, max=1.0, orientation='vertical')\n",
    "record_every = widgets.IntSlider(\n",
    "    description='record_every:', value=50, min=0, max=1000, step=10, orientation='vertical')\n",
    "episode_length = widgets.IntSlider(\n",
    "    description='episode_length:', value=2000, min=0, max=10000, step=100, orientation='vertical')\n",
    "num_deltas = widgets.IntSlider(\n",
    "    description='num_deltas:', value=16, min=0, max=100, orientation='vertical')\n",
    "num_best_deltas = widgets.IntSlider(\n",
    "    description='num_best_deltas:', value=16, min=0, max=100, orientation='vertical')\n",
    "noise = widgets.FloatSlider(\n",
    "    description='noise:', value=0.03, min=0.0, max=1.0, step=0.01, orientation='vertical')\n",
    "\n",
    "ui_0 = widgets.HBox([\n",
    "    seed, num_episodes, learning_rate, record_every, episode_length, num_deltas, num_best_deltas, noise])\n",
    "\n",
    "def f_0(\n",
    "    seed, num_episodes, learning_rate, record_every, episode_length, num_deltas,\n",
    "    num_best_deltas, noise):\n",
    "    hyperparameters[\"seed\"] = seed\n",
    "    hyperparameters[\"num_episodes\"] = num_episodes\n",
    "    hyperparameters[\"learning_rate\"] = learning_rate  \n",
    "    hyperparameters[\"record_every\"] = record_every         \n",
    "    hyperparameters[\"episode_length\"] = episode_length        \n",
    "    hyperparameters[\"num_deltas\"] = num_deltas           \n",
    "    hyperparameters[\"num_best_deltas\"] = num_best_deltas\n",
    "    hyperparameters[\"noise\"] = noise   \n",
    "    \n",
    "out_0 = widgets.interactive_output(f_0, {\n",
    "    'seed': seed, 'num_episodes': num_episodes, \n",
    "    'learning_rate': learning_rate, 'record_every': record_every,\n",
    "    'episode_length': episode_length, 'num_deltas': num_deltas,\n",
    "    'num_best_deltas': num_best_deltas, 'noise': noise\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HyperParams():\n",
    "    \"\"\"Hyperparameters\"\"\"\n",
    "    def __init__(self,\n",
    "                 num_episodes=200,\n",
    "                 episode_length=2000,\n",
    "                 learning_rate=0.02,\n",
    "                 num_deltas=16,\n",
    "                 num_best_deltas=16,\n",
    "                 noise=0.03,\n",
    "                 seed=1,\n",
    "                 env_name=ENV_NAME,\n",
    "                 record_every=50):\n",
    "\n",
    "        self.num_episodes = num_episodes\n",
    "        self.episode_length = episode_length\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_deltas = num_deltas\n",
    "        self.num_best_deltas = num_best_deltas\n",
    "        assert self.num_best_deltas <= self.num_deltas\n",
    "        self.noise = noise\n",
    "        self.seed = seed\n",
    "        self.env_name = env_name\n",
    "        self.record_every = record_every"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ARS Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Normalizer():\n",
    "    # Normalizes the inputs\n",
    "    def __init__(self, nb_inputs):\n",
    "        self.n = np.zeros(nb_inputs)\n",
    "        self.mean = np.zeros(nb_inputs)\n",
    "        self.mean_diff = np.zeros(nb_inputs)\n",
    "        self.var = np.zeros(nb_inputs)\n",
    "\n",
    "    def observe(self, x):\n",
    "        self.n += 1.0\n",
    "        last_mean = self.mean.copy()\n",
    "        self.mean += (x - self.mean) / self.n\n",
    "        self.mean_diff += (x - last_mean) * (x - self.mean)\n",
    "        self.var = (self.mean_diff / self.n).clip(min = 1e-2)\n",
    "\n",
    "    def normalize(self, inputs):\n",
    "        obs_mean = self.mean\n",
    "        obs_std = np.sqrt(self.var)\n",
    "        return (inputs - obs_mean) / obs_std\n",
    "\n",
    "\n",
    "class Policy():\n",
    "    def __init__(self, input_size, output_size, hp, theta=None):\n",
    "        self.theta = theta or np.zeros((output_size, input_size))\n",
    "        self.hp = hp\n",
    "\n",
    "    def evaluate(self, state, delta=None, direction=None):\n",
    "        if direction is None:\n",
    "            return self.theta.dot(state)\n",
    "        elif direction == \"+\":\n",
    "            return (self.theta + self.hp.noise * delta).dot(state)\n",
    "        elif direction == \"-\":\n",
    "            return (self.theta - self.hp.noise * delta).dot(state)\n",
    "\n",
    "    def sample_deltas(self):\n",
    "        return [np.random.randn(*self.theta.shape) for _ in range(self.hp.num_deltas)]\n",
    "\n",
    "    def update(self, rollouts, sigma_rewards):\n",
    "        # sigma_rewards is the standard deviation of the rewards\n",
    "        step = np.zeros(self.theta.shape)\n",
    "        for r_pos, r_neg, delta in rollouts:\n",
    "            step += (r_pos - r_neg) * delta\n",
    "        self.theta += self.hp.learning_rate / (self.hp.num_best_deltas * sigma_rewards) * step\n",
    "\n",
    "    def get_theta(self):\n",
    "        return self.theta\n",
    "    \n",
    "    def set_theta(self, new_theta):\n",
    "        self.theta = new_theta\n",
    "                \n",
    "\n",
    "class ArsTrainer():\n",
    "    def __init__(self,\n",
    "                 hp=None,\n",
    "                 input_size=None,\n",
    "                 output_size=None,\n",
    "                 normalizer=None,\n",
    "                 policy=None,\n",
    "                 monitor_dir=None):\n",
    "\n",
    "        self.hp = hp or HyperParams()\n",
    "        np.random.seed(self.hp.seed)\n",
    "        self.env = gym.make(self.hp.env_name)\n",
    "        if monitor_dir is not None:\n",
    "            should_record = lambda i: self.record_video\n",
    "            self.env = wrappers.Monitor(self.env, monitor_dir, video_callable=should_record, resume=True)\n",
    "        self.hp.episode_length = self.env.spec.timestep_limit or self.hp.episode_length\n",
    "        self.input_size = input_size or self.env.observation_space.shape[0]\n",
    "        self.output_size = output_size or self.env.action_space.shape[0]\n",
    "        self.normalizer = normalizer or Normalizer(self.input_size)\n",
    "        self.policy = policy or Policy(self.input_size, self.output_size, self.hp)\n",
    "        self.record_video = False\n",
    "        \n",
    "    def __del__(self):\n",
    "        self.env.close()\n",
    "        \n",
    "    def save(self, filename='theta.npy'):\n",
    "        print(f\"Saving {filename}\") \n",
    "        np.save(filename, self.policy.get_theta())\n",
    "            \n",
    "    def load(self, filename='theta.npy'):\n",
    "        print(f\"Loading {filename}\")\n",
    "        self.policy.set_theta(np.load(filename))\n",
    "        \n",
    "    def get_policy(self):\n",
    "        return self.policy\n",
    "\n",
    "    \n",
    "    # Explore the policy on one specific direction and over one episode\n",
    "    def explore(self, direction=None, delta=None):\n",
    "        state = self.env.reset()\n",
    "        done = False\n",
    "        num_plays = 0.0\n",
    "        sum_rewards = 0.0\n",
    "        while not done and num_plays < self.hp.episode_length:\n",
    "            self.normalizer.observe(state)\n",
    "            state = self.normalizer.normalize(state)\n",
    "            action = self.policy.evaluate(state, delta=delta, direction=direction)\n",
    "            state, reward, done, _ = self.env.step(action)\n",
    "            reward = max(min(reward, 1), -1)\n",
    "            sum_rewards += reward\n",
    "            num_plays += 1\n",
    "        return sum_rewards\n",
    "\n",
    "    def play(self):\n",
    "        \"\"\"play stuff\"\"\"\n",
    "        self.record_video = True\n",
    "        self.explore()\n",
    "        \n",
    "    \n",
    "    def train(self):\n",
    "        \"\"\"trains the \"\"\"\n",
    "        best_reward, reward_evaluation = float(\"-inf\"), float(\"-inf\")\n",
    "        \n",
    "        # used by tqdm to give us a formatted progress bar\n",
    "        desc = f\"Current Reward: {reward_evaluation:>3.5f}, Best Reward: {reward_evaluation:>3.5f}\"\n",
    "        t = tnrange(self.hp.num_episodes, desc=desc, leave=True)\n",
    "        \n",
    "        rewards = np.zeros((self.hp.num_episodes, 1))\n",
    "        \n",
    "        for step in t:\n",
    "            #print(f\"Step: {step}\")\n",
    "            # used by tqdm to give us a formatted progress bar            \n",
    "            t.set_description(f\"Current Reward: {reward_evaluation:>3.5f}, Best Reward: {best_reward:>3.5f}\")\n",
    "            t.refresh()\n",
    "            \n",
    "            # initialize the random noise deltas and the positive/negative rewards\n",
    "            deltas = self.policy.sample_deltas()\n",
    "            positive_rewards = [0] * self.hp.num_deltas\n",
    "            negative_rewards = [0] * self.hp.num_deltas\n",
    "\n",
    "            # play an episode each with positive deltas and negative deltas, collect rewards\n",
    "            for k in range(self.hp.num_deltas):\n",
    "                positive_rewards[k] = self.explore(direction=\"+\", delta=deltas[k])\n",
    "                negative_rewards[k] = self.explore(direction=\"-\", delta=deltas[k])\n",
    "                \n",
    "            # Compute the standard deviation of all rewards\n",
    "            sigma_rewards = np.array(positive_rewards + negative_rewards).std()\n",
    "\n",
    "            # Sort the rollouts by the max(r_pos, r_neg) and select the deltas with best rewards\n",
    "            scores = {k:max(r_pos, r_neg) for k,(r_pos,r_neg) in enumerate(zip(positive_rewards, negative_rewards))}\n",
    "            order = sorted(scores.keys(), key = lambda x:scores[x], reverse = True)[:self.hp.num_best_deltas]\n",
    "            rollouts = [(positive_rewards[k], negative_rewards[k], deltas[k]) for k in order]\n",
    "\n",
    "            # Update the policy\n",
    "            self.policy.update(rollouts, sigma_rewards)\n",
    "\n",
    "            # Only record video during evaluation, every n steps\n",
    "            if step % self.hp.record_every == 0 and step > 0 or step == self.hp.num_episodes - 1:\n",
    "                self.record_video = True\n",
    "                \n",
    "            # Play an episode with the new weights and print the score\n",
    "            reward_evaluation = self.explore()\n",
    "            rewards[step] = reward_evaluation\n",
    "            \n",
    "            best_reward = max(best_reward, reward_evaluation)\n",
    "            self.record_video = False\n",
    "            \n",
    "            if step >= 10:\n",
    "                last_ten_mean = np.mean(rewards[-10:])\n",
    "                if last_ten_mean >= 300.00:\n",
    "                    print(f\"Finished training early, the last_ten_mean is {last_ten_mean} \"\n",
    "                          f\"after {step} steps\")\n",
    "            \n",
    "        return rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mkdir(base, name):\n",
    "    path = os.path.join(base, name)\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    return path\n",
    "\n",
    "\n",
    "def train_agent(rewards_file='000.npy', continue_training=False):\n",
    "    \"\"\"Starts everything\"\"\"\n",
    "    print(hyperparameters)\n",
    "    videos_dir = mkdir('.', 'videos')\n",
    "    monitor_dir = mkdir(videos_dir, ENV_NAME)    \n",
    "    hp = HyperParams(**hyperparameters)\n",
    "    trainer = ArsTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "        \n",
    "    rewards = trainer.train()\n",
    "\n",
    "    rewards_dir = mkdir('.', 'rewards')\n",
    "    np.save(rewards_dir + \"/\" + rewards_file, rewards)\n",
    "\n",
    "    \n",
    "def play_agent(filename='theta.npy'):\n",
    "    videos_dir = mkdir('.', 'videos')\n",
    "    monitor_dir = mkdir(videos_dir, ENV_NAME)    \n",
    "    hp = HyperParams(**hyperparameters)\n",
    "    trainer = ArsTrainer(hp=hp, monitor_dir=monitor_dir)\n",
    "    trainer.load(filename=filename)\n",
    "    policy = trainer.get_policy()\n",
    "    print(f\"Policy.theta:\\n{policy.theta}\")\n",
    "    trainer.play()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Sliders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display(ui_0, out_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just verify our hyperparameters\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_agent(rewards_file='experiment_14.npy', continue_training=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After 12 experiments, we found that **experiment 8** showed a pretty reasonable result within 200 episodes:\n",
    "\n",
    "<br/>\n",
    "\n",
    "| Experiment | Seed   | num_episodes | learning_rate | episode_length | noise  | best_reward |\n",
    "|------------|--------|--------------|---------------|----------------|--------|-------------|\n",
    "|           1|1946    | 200          |            0.2|            2000|   0.03 |         4.08|\n",
    "|           2|1946    | 200          |            0.2|            2000|   0.00 |        -1.25|\n",
    "|           3|1946    | 200          |            0.2|            2000|   0.90 |         9.63|\n",
    "|           4|1946    | 200          |            0.2|            2000|   0.90 |       263.60|\n",
    "|           5|1946    | 200          |            0.2|            2000|   0.90 |       263.57|\n",
    "|           6|1946    | 200          |            0.2|            2000|   0.50 |         2.57|         \n",
    "|           7|1946    | 200          |            0.2|            2000|   0.40 |         3.17|         \n",
    "|       **8**|**1946**| **200**      |        **0.2**|        **2000**|**0.25**|   **281.37**|         \n",
    "|           9|1946    | 200          |            0.2|            2000|    0.22|       248.47|\n",
    "|          10|1946    | 200          |            0.3|            2000|    0.22|       190.21|\n",
    "|          11|1946    | 300          |            0.3|            2000|    0.25|       276.58|\n",
    "|          12|1946    | 300          |            0.3|            2000|    0.10|        38.40|\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Video\n",
    "\n",
    "Here is a video of one of the best results achieved by training\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "%%html\n",
    "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/lQl2zrrV1xM\" frameborder=\"0\" allow=\"autoplay; encrypted-media\" allowfullscreen></iframe>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
