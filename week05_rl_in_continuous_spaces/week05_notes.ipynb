{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 05 Notes - RL in Continuous Spaces <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Inverse-and-Forward-Kinematics\" data-toc-modified-id=\"Inverse-and-Forward-Kinematics-1\">Inverse and Forward Kinematics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.2\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Augmented-Random-Search-Tutorial-(Teach-a-Robot-to-Walk)\" data-toc-modified-id=\"Augmented-Random-Search-Tutorial-(Teach-a-Robot-to-Walk)-2\">Augmented Random Search Tutorial (Teach a Robot to Walk)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-2.1\">Notes</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-2.2\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Midterm-Assignment-(Make-a-Bipedal-Robot-Walk)\" data-toc-modified-id=\"Midterm-Assignment-(Make-a-Bipedal-Robot-Walk)-3\">Midterm Assignment (Make a Bipedal Robot Walk)</a></span></li><li><span><a href=\"#Inverse-Kinematics\" data-toc-modified-id=\"Inverse-Kinematics-4\">Inverse Kinematics</a></span><ul class=\"toc-item\"><li><span><a href=\"#Introduction\" data-toc-modified-id=\"Introduction-4.1\">Introduction</a></span></li><li><span><a href=\"#Forward-Kinematics\" data-toc-modified-id=\"Forward-Kinematics-4.2\">Forward Kinematics</a></span></li><li><span><a href=\"#Math\" data-toc-modified-id=\"Math-4.3\">Math</a></span></li><li><span><a href=\"#Inverse-Kinematics\" data-toc-modified-id=\"Inverse-Kinematics-4.4\">Inverse Kinematics</a></span></li><li><span><a href=\"#Gradient-Descent\" data-toc-modified-id=\"Gradient-Descent-4.5\">Gradient Descent</a></span></li><li><span><a href=\"#Real-world-Applications\" data-toc-modified-id=\"Real-world-Applications-4.6\">Real-world Applications</a></span></li><li><span><a href=\"#More-Readings\" data-toc-modified-id=\"More-Readings-4.7\">More Readings</a></span></li><li><span><a href=\"#Bonus\" data-toc-modified-id=\"Bonus-4.8\">Bonus</a></span></li><li><span><a href=\"#References\" data-toc-modified-id=\"References-4.9\">References</a></span></li></ul></li><li><span><a href=\"#Kalman-Filters\" data-toc-modified-id=\"Kalman-Filters-5\">Kalman Filters</a></span></li><li><span><a href=\"#Continuous-Action-Space---Study-Guide\" data-toc-modified-id=\"Continuous-Action-Space---Study-Guide-6\">Continuous Action Space - Study Guide</a></span><ul class=\"toc-item\"><li><span><a href=\"#Continuous-vs.-Discrete-Action-Spaces\" data-toc-modified-id=\"Continuous-vs.-Discrete-Action-Spaces-6.1\">Continuous vs. Discrete Action Spaces</a></span></li><li><span><a href=\"#Algorithms-for-Continuous-Action-Spaces\" data-toc-modified-id=\"Algorithms-for-Continuous-Action-Spaces-6.2\">Algorithms for Continuous Action Spaces</a></span></li></ul></li><li><span><a href=\"#Quiz:-Continuous-Action-Space\" data-toc-modified-id=\"Quiz:-Continuous-Action-Space-7\">Quiz: Continuous Action Space</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-7.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-7.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-7.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-7.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-7.5\">Question 5</a></span></li></ul></li><li><span><a href=\"#Quantum-Machine-Learning-(Live-Stream)\" data-toc-modified-id=\"Quantum-Machine-Learning-(Live-Stream)-8\">Quantum Machine Learning (Live Stream)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse and Forward Kinematics\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Robotics is a vast field of study, encompassing theories across multiple scientific disciplines. In this video, we'll program a robotic arm in a simulated environment to pick up an object. Along the way, we'll learn about both forward and inverse kinematics. We'll optimize our arms trajectory using calculus and observe how its angles change over time, measuring them with trigonometry. We'll code this in Python, this is an example of machine learning applied to robotic manipulation. Enjoy! \n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "- Kinematics is the branch of Classical Mechanics that describes the motion of points, objects and systems with groups of objects without reference to the causes of motion\n",
    "\n",
    "We can summarize the behavior of our system in two equations:\n",
    "\n",
    "- **Rotation**: The global rotation $r_i$ of a joint is the sum of all the rotations of all the previous joints:\n",
    "\n",
    "$$ r_i = \\sum_{k=0}^{i} \\alpha_{k} \\\\ $$\n",
    "\n",
    "- **Position**: The global position $P_i$ of a joing is given by:\n",
    "\n",
    "$$ P_i = P_{i-1} + rotate(\\ D_i, P_{i-1}, \\sum_{k=0}^{i-1} \\alpha_{k}\\ ) \\\\ $$\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video: Robotic Manipulation Explained](https://www.youtube.com/watch?v=mCI-f71MAvY)\n",
    "- [Code Link: Robotic Manipulation](https://github.com/llSourcell/Robotic_Manipulation)\n",
    "- [Robotiq Blog: How to Calculate a Robot's Forward Kinematics in 5 Easy Steps](https://blog.robotiq.com/how-to-calculate-a-robots-forward-kinematics-in-5-easy-steps)\n",
    "- [MIT Course 6.141 Lecture Notes: Forward and Inverse Kinematics](http://courses.csail.mit.edu/6.141/spring2011/pub/lectures/Lec14-Manipulation-II.pdf)\n",
    "- [Blog: The Mathematics of Forward Kinematics](https://www.alanzucconi.com/2017/04/06/forward-kinematics/)\n",
    "- [Applied Go: Inverse Kinematics](https://appliedgo.net/roboticarm/)\n",
    "- [Lecture Notes: Robot Manipulator Kinematics](http://www.ent.mrt.ac.lk/~rohan/teaching/ME5144/LectureNotes/Lec%205%20Kinematics.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Augmented Random Search Tutorial (Teach a Robot to Walk)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Learn one of the most advanced reinforcement learning algorithms to emerge in 2018, which has advanced the field of robotics by leaps and bounds, Augmented Random Search. Follow along with the coding tutorial and teach your own robot how to walk in less than an hour!\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "**Augmented Random Search**:\n",
    "\n",
    "- Shallow learning algorithm\n",
    "- Random noise\n",
    "- Genetic evolution\n",
    "- Cutting edge performance on locomotion tasks\n",
    "\n",
    "**How it works (simplified)**:\n",
    "\n",
    "1. Add random noise($\\delta$) to the weights ($\\theta$)\n",
    "2. Run a test\n",
    "3. If the reward improves, keep the new weights\n",
    "4. Otherwise discard\n",
    "\n",
    "Instead of gradient descent, we use a much simpler algorithm called the Method of Finite Differences to calculate updates to our weights.\n",
    "\n",
    "**Method of Finite Differences**:\n",
    "\n",
    "1. Generate random noise ($\\delta$) of the same shape as the weights ($\\theta$)\n",
    "2. Clone two versions of our current weights\n",
    "3. Add the noise to $\\theta{[+]}$, subtract from $\\theta{[-]}$\n",
    "4. Test out both versions one episode each, collect $r[+]$, $r[-]$\n",
    "5. Update the weights with: $ \\theta\\ += \\alpha\\ (\\ r[+] - r[-]\\ ) * \\delta $\n",
    "6. Test and repeat for maximum performance\n",
    "\n",
    "This algorithm works best if all inputs are squeezed between 0 and 1. This is called normalizing. We do this with a standard normalization algorithm\n",
    "\n",
    "**Normalize the Inputs**:\n",
    "\n",
    "- Normalized = (Inputs - Observation_mean)/ Observation_sigma\n",
    "- To track the mean, we keep a running average:\n",
    "    - mean = last_mean + (observation - last_mean) / num_observations\n",
    "\n",
    "**Training Loop**:\n",
    "\n",
    "1. Generate num_deltas deltas and evaluate positive and negative\n",
    "2. Run num_deltas episodes with positive and negative variations\n",
    "3. Collect rollouts as $(r[+], r[-], \\delta)$ tuples\n",
    "4. Calculate the standard deviation of all rewards (sigma_rewards)\n",
    "5. Sort the rollouts by maximum reward and select the best num_best_deltas rollouts\n",
    "6. $ step = sum(\\ (\\ r[+] - r[-]\\ )\\ * \\delta\\ ) $ for each best rollout\n",
    "7. theta += learning_rate / (num_best_deltas * sigma_rewards) * step\n",
    "8. Evaluate: play an episode with the new weights to measure improvement\n",
    "9. Continue until the desired performance is reached\n",
    "\n",
    "**Dependencies**:\n",
    "\n",
    "- OpenAI Gym (```pip install gym```)\n",
    "- Box2d (```pip install box2d```)\n",
    "- PyBullet environments (```pip install pybullet```) [optional]\n",
    "\n",
    "**Things to Try**:\n",
    "\n",
    "- Try to code this yourself!\n",
    "- Play around with the hyper-parameters\n",
    "- Try other environments (PyBullet Half Cheetah, Lunar Lander Continuous)\n",
    "- What other tasks does ARS get good results on?\n",
    "- One way to make this faster is to employ multi-processing to utilize more than one CPU core to run several episodes in parallel\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video: Augmented Random Search Tutorial - How to Train Robots to Walk!](https://www.youtube.com/watch?time_continue=1&v=2P2Dj5PX5cg)\n",
    "- [Code Link: ARS](https://github.com/colinskow/move37/tree/master/ars)\n",
    "- [Original Research Paper](https://arxiv.org/abs/1803.07055)\n",
    "- [MathisFun: Dot Product Tutorial](https://www.mathsisfun.com/algebra/matrix-multiplying.html)\n",
    "- [MathisFun: Standard Deviation Tutorial](https://www.mathsisfun.com/data/standard-deviation.html)\n",
    "- [Github: ARRS with multiprocessing](https://github.com/modestyachts/ARS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Midterm Assignment (Make a Bipedal Robot Walk)\n",
    "\n",
    "Its midterm season Wizards! The midterm is to make a bipedal humanoid robot walk in a simulation. You can use OpenAI Gym for the environment, this [link](https://github.com/search?q=bipedal+gym) shows some potential solutions that you can use to help you when you build your own. Submit your repository to schoolofaigrading@gmail.com . We will review your work and send back grades! We’re looking for good documentation, readable code, and bonus points for using reinforcement learning in a novel way for this challenge. Due date for all midterms is October 29, 2018 at 12 PM PST. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverse Kinematics\n",
    "\n",
    "### Introduction\n",
    "\n",
    "If you were doing Kinematics you would be calculating the position of an object from its rotations/velocities/forces on it. Inverse Kinematics is the opposite. You’re given the position and need to calculate how to reach it. In the case of this tutorial Inverse Kinematics means calculating the rotations of the joints in the skeleton for a certain part of the skeleton to reach a given point.\n",
    "\n",
    "- [Inverse Kinematics for Humanoid Skeletons Tutorial](http://www.3dkingdoms.com/ik.htm)\n",
    "\n",
    "### Forward Kinematics\n",
    "\n",
    "\n",
    "### Math\n",
    "\n",
    "\n",
    "### Inverse Kinematics\n",
    "\n",
    "\n",
    "### Gradient Descent\n",
    "\n",
    "\n",
    "### Real-world Applications\n",
    "\n",
    "Inverse Kinematics is mainly used in animations, games, and robots.\n",
    "\n",
    "    - Feet Placement\n",
    "    - Hands Animation\n",
    "    - Grasping in Robotics\n",
    "    - Making realistic Tentacles\n",
    "\n",
    "- [Youtube: Hand animation using Unity](https://www.youtube.com/watch?time_continue=43&v=GEnAaWNp3r4)\n",
    "- [Demo 1 for feet placement (Unity Web)](http://static.o3n.org/demo/o3nSimpleHumanoidFootIK/index.html)\n",
    "- [Demo 2 for feet placement (Unity Web)](http://static.o3n.org/demo/anyik3footlook/index.html)\n",
    "- [Youtube: Robotics Simulation : Grasping Cups](https://www.youtube.com/watch?v=Zech1Y2CdRo)\n",
    "- [Tentacles Part 1 (implemented by js)](https://www.youtube.com/watch?v=xXjRlEr7AGk)\n",
    "- [Tentacles Part 2 (implemented by js)](https://www.youtube.com/watch?v=hbgDqyy8bIw)\n",
    "- [Tentacles Part 3 (implemented by js)](https://www.youtube.com/watch?v=RTc6i-7N3ms)\n",
    "- [Tentacles Part 4 (implemented by js)](https://www.youtube.com/watch?v=10st01Z0jxc)\n",
    "\n",
    "\n",
    "\n",
    "### More Readings\n",
    "\n",
    "- [CSE169: Inverse Kinematics Part 1](https://cseweb.ucsd.edu/classes/sp16/cse169-a/slides/CSE169_08.pdf)\n",
    "- [CSE169: Inverse Kinematics Part 2](https://cseweb.ucsd.edu/classes/sp16/cse169-a/slides/CSE169_09.pdf)\n",
    "\n",
    "\n",
    "### Bonus\n",
    "\n",
    "- [Youtube: Forward Kinematics (without IK)](https://www.youtube.com/watch?v=h6v5CZBp5mw)\n",
    "\n",
    "\n",
    "### References\n",
    "This article is mainly citing Alan Zucconi's blog:\n",
    "    - [Forward Kinematics](https://www.alanzucconi.com/2017/04/06/forward-kinematics)\n",
    "    - [Implementing Forward Kinematics](https://www.alanzucconi.com/2017/04/06/implementing-forward-kinematics)\n",
    "    - [Gradient Descent](https://www.alanzucconi.com/2017/04/10/gradient-descent)\n",
    "    - [Robotic Arms](https://www.alanzucconi.com/2017/04/10/robotic-arms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Kalman Filters\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=QROMJbXuEHg)\n",
    "- [Sathishruw Gist - Kalman Filter Tracking Plane](https://gist.github.com/Sathishruw/8f351c331b67267eacef12f9a96bc20f)\n",
    "- [Kalman Filter Math Deep Dive PDF](https://www.theschool.ai/wp-content/uploads/2018/10/kalmanfilter.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous Action Space - Study Guide\n",
    "\n",
    "\n",
    "### Continuous vs. Discrete Action Spaces\n",
    "\n",
    "- [Continuous Action Space - Study Guide](https://www.theschool.ai/wp-content/uploads/2018/10/Week-5-Study-Guide-2.pdf)\n",
    "\n",
    "\n",
    "In reinforcement learning, we deal with choosing optimal actions over time. Actions can either be discrete or continuous. A discrete action is an action taken in which the choices are countable in number. If the options are innumerable, we call the action space continuous.\n",
    "\n",
    "Choosing items from a menu is (generally) a discrete action space problem because each item is a separate choice from each other item. If we wanted to frame a menu choice as continuous, we could ask for a mix of two options in a certain proportion such as an ice-cream cup with 63.5% sea salt caramel, and 36.5% mango. By choosing different proportions, we have in effect an infinite variety of ice cream flavors to choose (while staying within two overall scoops).\n",
    "\n",
    "_Notice that if a problem has both discrete and continuous elements, as in the example\n",
    "above, we can call the overall framing continuous._\n",
    "\n",
    "\n",
    "### Algorithms for Continuous Action Spaces\n",
    "\n",
    "- [Continuous Actor Critic Learning Automation (CALCA) 2007](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.75.7658&rep=rep1&type=pdf)\n",
    "- [Newton's Method State Action Reward State Action (NM-SARSA) 2014](https://core.ac.uk/download/pdf/42489553.pdf)\n",
    "- [Deterministic Policy Gradient (DPG) 2014](http://proceedings.mlr.press/v32/silver14.pdf)\n",
    "- [Deep Deterministic Policy Gradient (DDPG) 2015](https://arxiv.org/pdf/1509.02971v2.pdf)\n",
    "- [Asynchronous Advantage Actor Critic (A3C) 2016](https://arxiv.org/pdf/1602.01783.pdf)\n",
    "- [See Deep Reinforcement Learning Hands-On by Maxim Lapan](https://medium.com/@shmuma/my-deep-rl-book-has-been-published-fc5adb648fc1)\n",
    "- [Kernel Regression Upper Confidence Trees (KR-UCT) 2016](https://www.ijcai.org/Proceedings/16/Papers/104.pdf)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quiz: Continuous Action Space\n",
    "\n",
    "- Know which algorithms to use when in a discrete/continuous action space\n",
    "\n",
    "### Question 1\n",
    "\n",
    "You are teaching an agent to play a game in which we have a 1000 boxes, each with an unknown chance of giving reward. The agent gets 60 seconds to maximize its score. It can look in 1 box at a time, and instantly receives a reward. Is the action space continuous or discrete?\n",
    "\n",
    "    - [] Continuous\n",
    "    - [x] Discrete\n",
    "    - [] Both\n",
    "    - [] Neither\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "Each of our 1000 boxes is an option. Therefore, if we are choosing a single box we have 1000 discrete actions we could take at any given moment.\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Suppose you want to teach a robotic controller to balance a ball on a flat platform by tilting the platform forward, back, left, or right. You decide to use Reinforcement Learning to solve your problem by rewarding your agent for every second the ball stays balanced and giving a negative reward if the ball falls off. Should you frame the action-space as continuous or discrete?\n",
    "\n",
    "    - [x] Continuous\n",
    "    - [] Discrete\n",
    "    - [] Both\n",
    "    - [] Neither\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "If we were to define this as a discrete action space, our robots movements would tend to be jerky. A continuous action space would allow for more precise commands to be given to the motors and we could expect more fluid behavior.\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Which of the following algorithms does not facilitate a continuous action-space?\n",
    "\n",
    "    - [] Asynchronous Advantage Actor Critic\n",
    "    - [] Continuous Actor Critic Learning Automation\n",
    "    - [x] Q-Learning\n",
    "    - [] Deep Deterministic Policy Gradient\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "Q-Learning relies on defining a lookup table (Q-table) which uses discrete cells for deciding which action to take in a given state.\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Which of the following algorithms would be best suited to the cart-pole balancing problem?\n",
    "\n",
    "    - [] SARSA\n",
    "    - [x] NM-SARSA\n",
    "    - [] Q-Learning\n",
    "    - [] Monte-Carlo\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "The cart-pole balance is a continuous action space type problem. NM-SARSA is the only algorithm from the choices that handles for this.\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Suppose that you want to teach a robotic arm to skip stones on the water. Which of the following algorithms would be the least suited to this task? (which would perform the worst)\n",
    "\n",
    "    - [] NM-SARSA\n",
    "    - [] KR-UCT\n",
    "    - [x] Monte Carlo Tree Search\n",
    "    - [] Deep Deterministic Policy Gradient\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "The problem of skipping a stone is a continuous action space problem. Therefore, Monte Carlo Tree Search, which is exclusively for discrete action spaces would be the least suited to the task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quantum Machine Learning (Live Stream)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "I'm going to use a quantum processor to help accelerate and extend a machine learning algorithm! Thanks to D-Wave's Leap software, we can access a QPU in the browser. In this live stream, I'll explain how the Leap toolkit works, what a quantum processor offers machine learning engineers, and we'll go through a few relevant examples. Get hype!\n",
    "\n",
    "**Learning Resources**:\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=AAO4oq2M_48)\n",
    "- [Code Link](https://github.com/llSourcell/quantum_machine_learning_LIVE)\n",
    "- [D-Wave: Leap](https://cloud.dwavesys.com/leap)\n",
    "- [D-Wave: Leap Learning Docs](https://cloud.dwavesys.com/leap/resources/learning-docs/)\n",
    "- [HackerNoon: A Brief Introduction to Quantum Computing](https://hackernoon.com/a-brief-introduction-to-quantum-computing-d21e578cb7ed)\n",
    "- [Xanadu: Quantum Machine Learning 1.0](https://medium.com/xanaduai/quantum-machine-learning-1-0-76a525c8cf69)\n",
    "- [Nature: Quantum Machine Learning](https://www.nature.com/articles/nature23474)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
