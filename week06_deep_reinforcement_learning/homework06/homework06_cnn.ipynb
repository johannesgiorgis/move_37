{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 6: Deep Q Learning <a class=\"tocSkip\">\n",
    "\n",
    "This weeks homework assignment is build a deep q network to defeat the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment in OpenAI’s Gym environment. Use this [repository](https://github.com/AndersonJo/dqn-pytorch) as a helpful guide. Train it and test it, if your algorithm successfully learns how to beat the environment, you’ve successfully completed the assignment. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Exercise\" data-toc-modified-id=\"Exercise-1\">Exercise</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Network-design\" data-toc-modified-id=\"Network-design-1.0.1\">Network design</a></span></li><li><span><a href=\"#Agent-Design\" data-toc-modified-id=\"Agent-Design-1.0.2\">Agent Design</a></span></li><li><span><a href=\"#Construct-evaluate\" data-toc-modified-id=\"Construct-evaluate-1.0.3\">Construct evaluate</a></span></li><li><span><a href=\"#Experience-Replay\" data-toc-modified-id=\"Experience-Replay-1.0.4\">Experience Replay</a></span></li><li><span><a href=\"#Learning-with...-Q-learning\" data-toc-modified-id=\"Learning-with...-Q-learning-1.0.5\">Learning with... Q-learning</a></span></li><li><span><a href=\"#Main-Loop\" data-toc-modified-id=\"Main-Loop-1.0.6\">Main Loop</a></span></li><li><span><a href=\"#Continuing-with-CPU\" data-toc-modified-id=\"Continuing-with-CPU-1.0.7\">Continuing with CPU</a></span></li></ul></li></ul></li><li><span><a href=\"#Research\" data-toc-modified-id=\"Research-2\">Research</a></span></li><li><span><a href=\"#Big-Questions\" data-toc-modified-id=\"Big-Questions-3\">Big Questions</a></span></li><li><span><a href=\"#Resources\" data-toc-modified-id=\"Resources-4\">Resources</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exercise\n",
    "\n",
    "This version utilizes a Convolutional Neural Network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "import gym\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "USE_CPU = False # can be True or False\n",
    "GPU = 1 # can be 0 or 1\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7fa8901af2e8>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v2\")\n",
    "env.reset()\n",
    "img = env.render(mode='rgb_array')\n",
    "env.close()\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network design\n",
    "\n",
    "1. the data within the model will move through the layers\n",
    "2. the \"forward\" method will pull our 'x' through the layers and then apply ReLU functions to keep our values positive\n",
    "3. $ input (x) -> layer_1 -> ReLU -> layer_2 -> ReLU -> output $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNModel(nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        super(DQNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(3, 16, kernel_size=5, stride=2)\n",
    "        self.bn1 = nn.BatchNorm2d(16)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=5, stride=2)\n",
    "        self.bn2 = nn.BatchNorm2d(32)\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=5, stride=2)\n",
    "        self.bn3 = nn.BatchNorm2d(32)\n",
    "        self.head = nn.Linear(448, output_size)\n",
    "        \n",
    "        # self.layer1 = nn.Linear(input_size, 64)\n",
    "        # self.layer2 = nn.Linear(64,128)\n",
    "        # self.output = nn.Linear(128, output_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.float() # matmul requires tensors of type float\n",
    "        x = F.relu(self.bn1(self.conv1(x)))\n",
    "        x = F.relu(self.bn2(self.conv2(x)))       \n",
    "        x = F.relu(self.bn3(self.conv3(x)))               \n",
    "        \"\"\" original\n",
    "        x = self.layer1(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.relu(self.layer2(x))\n",
    "        \"\"\"\n",
    "        return self.output(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Agent Design\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_shape, n_actions, model, epsilon=0):\n",
    "        \"\"\"Doc-string\"\"\"\n",
    "        self.epsilon = epsilon\n",
    "        self.n_actions = n_actions\n",
    "        self.state_shape = state_shape\n",
    "        self.model = model #can accept different models\n",
    "        \n",
    "    def get_q_values(self, state):\n",
    "        \"\"\"\n",
    "        state: observation from gym environment\n",
    "        pytorch needs [batch, s], so add a batch dim in state\n",
    "        \"\"\"\n",
    "        \n",
    "        if torch.cuda.is_available() and not USE_CPU:\n",
    "            device = torch.device(f\"cuda:{GPU}\")\n",
    "        else:\n",
    "            device = torch.device(\"cpu\")\n",
    "            \n",
    "        expanded_state = np.expand_dims(state, axis=0)\n",
    "        print(f\"Expanded state: {expanded_state}\")\n",
    "        s = torch.from_numpy(expanded_state).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            q = self.model.forward(s)\n",
    "       \n",
    "        squeezed = torch.squeeze(q.data)\n",
    "\n",
    "        return squeezed\n",
    "    \n",
    "    def sample_action(self, q_values):\n",
    "        \"\"\"pick actions given q_values.  Uses epsilon-greedy exploration strategy\n",
    "        suppose self.epsilon = 0.5\n",
    "        suppose self.n_actions = 5\n",
    "        line 31: p = [0.1, 0.1, 0.1, 0.1, 0.1]\n",
    "        suppose q_values = [1.0, 2.0, 10.0, 3.0, 4.0]\n",
    "        then q_values.argmax(axis=-1) = 2 #counting from 0\n",
    "        then on line 32:\n",
    "        p[2] += (1 - self.epsilon) or p[2] += 0.5 or p[2] = 0.1 + 0.5 = 0.6\n",
    "        so now\n",
    "        p = [0.1, 0.1, 0.6, 0.1, 0.1]\n",
    "        \"\"\"       \n",
    "        p = np.ones(self.n_actions, dtype=np.float) * self.epsilon / self.n_actions\n",
    "        #p[q_values.argmax(axis=-1)] += 1 - self.epsilon\n",
    "        p[torch.argmax(q_values)] += 1 - self.epsilon\n",
    "        action = np.random.choice(n_actions, p=p)\n",
    "        return action\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(env, agent, n_games=1, greedy=True):\n",
    "    \"\"\"Plays n_games full games.  If greedy, picks actions as argmax(q_values).\n",
    "    Returns mean reward. \"\"\"\n",
    "    t_max = env.spec.timestep_limit or 10000 # maximum_timesteps\n",
    "    rewards = []\n",
    "    old_epsilon = agent.epsilon\n",
    "    if greedy:\n",
    "        agent.epsilon = 0.0 # take the best move according to q_values\n",
    "        \n",
    "    for _ in range(n_games):\n",
    "        s = env.reset()\n",
    "        # print(f\"type(s): {type(s)}\")\n",
    "        reward = 0\n",
    "        for _ in range(t_max):\n",
    "            q_values = agent.get_q_values(s)\n",
    "            action = agent.sample_action(q_values)\n",
    "            s, r, done, _ = env.step(action)\n",
    "            reward += r\n",
    "            if done: break\n",
    "                \n",
    "        rewards.append(reward)\n",
    "        \n",
    "    agent.epsilon = old_epsilon\n",
    "    \n",
    "    return np.mean(rewards) # we could also have just said mean(rewards)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience Replay\n",
    "\n",
    "**The interface:**\n",
    "\n",
    "- `exp_replay.push(state, action, reward, next_state, done)`\n",
    "    - saves (s, a, r, s', done) tuple into the buffer\n",
    "- `exp_replay.sample(batch_size)`\n",
    "    - returns observations, actions, rewards, next_observations, and is_done for batch_size random samples\n",
    "- `len(exp_replay)`\n",
    "    - returns number of elements stored in the replay buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from collections import namedtuple\n",
    "\n",
    "Transition = namedtuple(\n",
    "    'Transition', ('state', 'action', 'reward', 'next_state', 'done'))\n",
    "\n",
    "class ReplayMemory(object):\n",
    "    \n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.memory = []\n",
    "        self.position = 0\n",
    "        \n",
    "    def push(self, *args):\n",
    "        \"\"\"Saves a transition\"\"\"\n",
    "        if len(self.memory) < self.capacity:\n",
    "            self.memory.append(None)\n",
    "        self.memory[self.position] = Transition(*args)\n",
    "        self.position = (self.position + 1) % self.capacity\n",
    "        \n",
    "    def sample(self, batch_size):\n",
    "        return random.sample(self.memory, batch_size)\n",
    "    \n",
    "    def head(self):\n",
    "        return self.memory[-1]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)\n",
    "    \n",
    "            \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning with... Q-learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def td_loss(target_net, policy_net, states, actions, rewards, next_states, is_dones, GAMMA=0.99):\n",
    "    device = None\n",
    "    if torch.cuda.is_available() and not USE_CPU: #if we want to use a GPU\n",
    "        device = GPU \n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "    s_ = torch.as_tensor(states, device=device)\n",
    "    a_ = torch.as_tensor(actions, device=device)\n",
    "    r_ = torch.as_tensor(rewards, device=device)\n",
    "    # ns_ = torch.as_tensor(next_states, device=device)\n",
    "    d_ = torch.as_tensor(is_dones, device=device)\n",
    "    not_d_ = 1 - d_\n",
    "\n",
    "    #predict Q values\n",
    "    predict_q_ = policy_net(s_).gather(dim=1, index=a_.view(-1, 1)).squeeze()\n",
    "    \n",
    "    #predict Next Q Values with target network\n",
    "    with torch.no_grad():\n",
    "        predict_next_q_ = target_net(s_)\n",
    "        predict_v_ = torch.max(predict_next_q_, dim=1)[0] * not_d_\n",
    "        reference_q_ = r_ + GAMMA * predict_v_\n",
    "        \n",
    "    loss = F.mse_loss(predict_q_, reference_q_)\n",
    "    \n",
    "    return loss\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Loop\n",
    "\n",
    "Let's roll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import trange\n",
    "from IPython.display import clear_output\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "mean_rw_history = []\n",
    "td_loss_history = []\n",
    "td_loss_m_history = []\n",
    "BATCH_SIZE = 64\n",
    "TIME_STEPS = 2*10**5\n",
    "LEARNING_RATE = 0.0001 #0.0001\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = None\n",
    "if torch.cuda.is_available() and not USE_CPU:\n",
    "    device = torch.device(f\"cuda:{GPU}\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using cpu device\")\n",
    "    \n",
    "\n",
    "state_dim, n_actions = env.observation_space.shape[0], env.action_space.n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "print(device)\n",
    "policy_net = DQNModel(state_dim, n_actions).to(device)\n",
    "target_net = DQNModel(state_dim, n_actions).to(device)\n",
    "target_net.load_state_dict(policy_net.state_dict())\n",
    "agent = DQNAgent(state_dim, n_actions, policy_net, epsilon=0.5)\n",
    "exp_replay = ReplayMemory(10**4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expanded state: [[-0.004597    0.93871413 -0.4656394  -0.12662746  0.00533353  0.10547423\n",
      "   0.          0.        ]]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected 4-dimensional input for 4-dimensional weight [16, 3, 5, 5], but got input of size [1, 8] instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-7126254d954d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_games\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-f045a833627c>\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(env, agent, n_games, greedy)\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_max\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m             \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_q_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0ms\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-4-a2792318774a>\u001b[0m in \u001b[0;36mget_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m             \u001b[0mq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0msqueezed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9698b748f2b3>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# matmul requires tensors of type float\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/move_37/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/move_37/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected 4-dimensional input for 4-dimensional weight [16, 3, 5, 5], but got input of size [1, 8] instead"
     ]
    }
   ],
   "source": [
    "evaluate(env, agent, n_games=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Continuing with CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(policy_net.parameters(),lr=LEARNING_RATE)\n",
    "\n",
    "#for training\n",
    "env = gym.make(\"LunarLander-v2\")\n",
    "#for evaluation\n",
    "eval_env = gym.make(\"LunarLander-v2\")\n",
    "s = env.reset()\n",
    "\n",
    "for i in trange(TIME_STEPS):\n",
    "    # play\n",
    "    q_values = agent.get_q_values(s)\n",
    "    a = agent.sample_action(q_values)\n",
    "    s_next, r, done, _ = env.step(a)\n",
    "    \n",
    "    exp_replay.push(s, a, r, s_next, done)\n",
    "    \n",
    "    s = s_next\n",
    "    \n",
    "    if done:\n",
    "        s = env.reset()\n",
    "        \n",
    "    if len(exp_replay) < BATCH_SIZE:\n",
    "        continue\n",
    "        \n",
    "    # train\n",
    "    # < sample data from experience replay >\n",
    "    transitions = exp_replay.sample(BATCH_SIZE)\n",
    "    # transpose the batch\n",
    "    batch = Transition(*zip(*transitions))\n",
    "    \n",
    "    # loss = < compute TD loss >\n",
    "    nd_states = np.array(batch.state, dtype=np.float32)\n",
    "    nd_actions = np.array(batch.action, dtype=np.int64)\n",
    "    nd_rewards = np.array(batch.reward, dtype=np.float32)\n",
    "    nd_next_states = np.array(batch.next_state, dtype=np.float32)\n",
    "    nd_is_dones = np.array(batch.done).astype(np.float32)\n",
    "    \n",
    "    loss = td_loss(target_net, policy_net, nd_states, nd_actions, nd_rewards, nd_next_states, nd_is_dones)\n",
    "    \n",
    "    # <minimize loss by gradient descent>\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "  \n",
    "    loss_data = loss.data\n",
    "    if torch.cuda.is_available() and not USE_CPU:\n",
    "        loss_data = loss.data.cpu()\n",
    "\n",
    "    td_loss_history.append(loss_data.numpy())\n",
    "    count = min(len(td_loss_history), 100)\n",
    "    \n",
    "    td_loss_m_history.append(np.mean(td_loss_history[-count:]))\n",
    "  \n",
    "    # adjust agent parameters\n",
    "    if i % 500 == 0:\n",
    "        agent.epsilon = max(agent.epsilon * 0.99, 0.01)\n",
    "        \n",
    "        mean_rw_history.append(evaluate(eval_env, agent, n_games=3))\n",
    "    \n",
    "        #Load agent weights into target_network\n",
    "        target_net.load_state_dict(policy_net.state_dict())\n",
    "    \n",
    "        clear_output(True)\n",
    "        print(f\"buffer size = {len(exp_replay)}, epsilon = {agent.epsilon:2.5}\")\n",
    "        plt.figure(figsize=[12, 4])\n",
    "        plt.subplot(1, 2, 1)\n",
    "        plt.title(\"mean reward per game\")\n",
    "        plt.plot(mean_rw_history)\n",
    "        plt.grid()\n",
    "\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.title(\"TD loss history (moving average)\")\n",
    "        plt.plot(td_loss_m_history)\n",
    "        plt.grid()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Research"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir(torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(type(torch.device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.get_device_name(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.device_ctx_manager(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Questions\n",
    "\n",
    "1.  Are we copying data from cpu tensors to gpu tensors in our Model class?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources\n",
    "\n",
    "- [Github 1](https://github.com/etendue/move37/blob/master/dqn_box2d.ipynb)\n",
    "- [Github 2](https://github.com/johannesharmse/move_37_course/blob/master/src/dqn/lunar_land.py)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
