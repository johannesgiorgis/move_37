{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06 Notes - Deep Reinforcement Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-RL-for-Database-Optimization\" data-toc-modified-id=\"Deep-RL-for-Database-Optimization-1\">Deep RL for Database Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.2\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.3\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Deep-Q-Learning-Pong-Tutorial\" data-toc-modified-id=\"Deep-Q-Learning-Pong-Tutorial-2\">Deep Q Learning Pong Tutorial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-2.1\">Notes</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-2.2\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Prioritized-Experience-Replay-(PER)\" data-toc-modified-id=\"Prioritized-Experience-Replay-(PER)-3\">Prioritized Experience Replay (PER)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-3.1\">Theory</a></span></li><li><span><a href=\"#Priority-$p_t$\" data-toc-modified-id=\"Priority-$p_t$-3.2\">Priority $p_t$</a></span></li><li><span><a href=\"#Probability-$P(i)$\" data-toc-modified-id=\"Probability-$P(i)$-3.3\">Probability $P(i)$</a></span></li><li><span><a href=\"#Importance-Sampling-Weights-(IS)\" data-toc-modified-id=\"Importance-Sampling-Weights-(IS)-3.4\">Importance Sampling Weights (IS)</a></span></li><li><span><a href=\"#Google-DeepMind-Paper\" data-toc-modified-id=\"Google-DeepMind-Paper-3.5\">Google DeepMind Paper</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-3.6\">Implementation</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-3.7\">Summary</a></span></li><li><span><a href=\"#More-Papers-(Intermediate)\" data-toc-modified-id=\"More-Papers-(Intermediate)-3.8\">More Papers (Intermediate)</a></span></li></ul></li><li><span><a href=\"#Dueling-DQN\" data-toc-modified-id=\"Dueling-DQN-4\">Dueling DQN</a></span></li><li><span><a href=\"#Neural-Networks-Study-Guide\" data-toc-modified-id=\"Neural-Networks-Study-Guide-5\">Neural Networks Study Guide</a></span></li><li><span><a href=\"#Quiz:-Neural-Networks\" data-toc-modified-id=\"Quiz:-Neural-Networks-6\">Quiz: Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-6.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-6.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-6.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-6.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-6.5\">Question 5</a></span></li></ul></li><li><span><a href=\"#Reading-Assignments-(DQN-Improvements)\" data-toc-modified-id=\"Reading-Assignments-(DQN-Improvements)-7\">Reading Assignments (DQN Improvements)</a></span></li><li><span><a href=\"#Homework-Assignment:-Deep-Q-Learning\" data-toc-modified-id=\"Homework-Assignment:-Deep-Q-Learning-8\">Homework Assignment: Deep Q Learning</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RL for Database Optimization\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "We can use deep reinforcement learning to optimize a SQL database, and in this video we'll optimize the ordering of a series of SQL queries such that it involves the minimum possible memory/computation footprint. Deep RL involves using a neural network to approximate reinforcement learning functions, like the Q (quality) function. After we frame our database as a Markov Decision Process, I'll use Python to build a Deep Q Network to optimize SQL queries. Enjoy!\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "SQL:\n",
    "\n",
    "$ SQL\\ Statement \\rightarrow Parsing \\rightarrow (Parse\\ Tree) \\rightarrow Binding \\rightarrow (Algebrized\\ Tree) \\rightarrow Query\\ Optimization \\rightarrow (Execution\\ Plan) \\rightarrow Query\\ Execution \\rightarrow Query\\ Results $\n",
    "\n",
    "\n",
    "### Take Aways\n",
    "\n",
    "- Deep Reinforcement Learning involves using a Neural Network to Approximate Reinforcement Learning Functions like the Q Function\n",
    "- We can assess the quality or Q of State Action Pairs by computing A Q Table\n",
    "- Q Learning involves approximating the relationship between State Action Pairs and Q Values in this table using Neural Networks\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=Rw3ewEXOKC8)\n",
    "- [Code Link: SQL Database Optimization](https://github.com/llSourcell/SQL_Database_Optimization)\n",
    "- [Irselab: SQL Query Optimization Meets Deep Reinforcement Learning](https://rise.cs.berkeley.edu/blog/sql-query-optimization-meets-deep-reinforcement-learning/)\n",
    "- [MLDB: Machine Learning Database](https://mldb.ai/)\n",
    "- [Microsoft: Machine Learning Services in SQL Server 2017](https://docs.microsoft.com/en-us/sql/advanced-analytics/what-is-sql-server-machine-learning?view=sql-server-2017)\n",
    "- [Towards Data Science: Mchine Learning in your Database](https://towardsdatascience.com/machine-learning-in-your-database-the-case-for-and-against-bigquery-ml-4f2309282fda)\n",
    "- [Quora: Which database is best for machine learning](https://www.quora.com/Which-database-is-best-for-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Pong Tutorial\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Learn how to build an AI that plays Pong like a boss, with Deep Q Learning. Discover how neural networks can learn to play challenging video games at superhuman levels by looking at raw pixels.\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "**How to Play Pong Like a BOSS with Deep Q Learning**\n",
    "\n",
    "- If we can write code which masters complex video games we can use that same code to master complex real-life problems\n",
    "\n",
    "\n",
    "**Taking Q DEEP**\n",
    "\n",
    "- In vanilla Q Learning, we are storing a huge lookup table whose size is the number of possible states times the number of possible actions\n",
    "- **PROBLEM:** possible game states are astronomically large!\n",
    "- Bellman Equation remains the same\n",
    "- **SOLUTION:** replace the Q lookup table with a neural network, which approximates a function for Q of state and action\n",
    "- **BEFORE:** we use the most recent value calculation and a learning rate to update our Q table\n",
    "- **NOW:** Update the Q Network with stochastic gradient descent (SGD) and Back propagation\n",
    "\n",
    "\n",
    "**Over-simplified Q Learning Algorithm**\n",
    "\n",
    "1. Initialize $Q(s, a)$ randomly\n",
    "2. Interact with the environment to obtain (s, a, r, s')\n",
    "3. Calculate loss: $ L\\ =\\ (Q_{s,a} - r)^2 $  \n",
    "   otherwise: $ L\\ =\\ (Q_{s,a}\\ -\\ (r\\ +\\ \\gamma max_a [Q_{s',a'}]))^2  $\n",
    "4. Update $Q(s, a)$ using SGD, minimizing the loss function\n",
    "5. Repeat steps 2 - 4 until converged\n",
    "\n",
    "\n",
    "**Explore vs. Exploit**\n",
    "\n",
    "- To reach an optimal policy, we need to balance exploration with exploitation\n",
    "- Continue to use Epsilon greedy method\n",
    "- Start with epsilon = 1 -> taking all random actions\n",
    "- Gradually taper to lower value over a fixed number of game frames\n",
    "\n",
    "\n",
    "**Replay Buffer**\n",
    "\n",
    "- SGD optimization requires Independent and Identically Distributed training data\n",
    "- Our state transitions are highly correlated\n",
    "- Store a long list of (s, a, r, s') transitions\n",
    "- Randomly sample batches to train on from the buffer\n",
    "- New data kicks off old data\n",
    "- By randomly sampling from a long list, it breaks the correlation that comes from sampling values right next to each other and allows the model to converge\n",
    "\n",
    "\n",
    "**Target Network**\n",
    "\n",
    "- Loss function, $ L\\ =\\ (Q_{s,a}\\ -\\ (r\\ +\\ \\gamma max_a [Q_{s',a'}]))^2  $\n",
    "- We're updating Q(s, a) and Q(s', a') in the same step\n",
    "- We'll store them in a different network (target_network)\n",
    "- Copy the weights from main to target at a fixed interval\n",
    "\n",
    "\n",
    "**Predicting Motion**\n",
    "\n",
    "- Markov Property: The Past Doesn't Matter Baby!\n",
    "- But when there's motion it does matter\n",
    "- Solution: stack several recent frames together as input\n",
    "\n",
    "\n",
    "**Network Architecture**\n",
    "\n",
    "- How do we take a large number of pixels from the screen and pick out which objects are important to achieving results in a video game? The same neural network used in cutting edge image recognition is perfect for applying reinforcement learning to screen pixels\n",
    "- Use 3 layers of convolutions with each one passing through a ReLu activation\n",
    "- Input\n",
    "- Conv2D $\\rightarrow$ ReLu (x3)\n",
    "- Fully connected (512) $\\rightarrow$ ReLu\n",
    "- Actions: Output layer spits out the values of each action\n",
    "\n",
    "\n",
    "**Deep Q Network Algorithm**\n",
    "\n",
    "1. Initialize $Q(s,a)$ and Q^(s,a) (target network) with random weights\n",
    "2. With probability epsilon, select random action $a$, otherwise $ a = max_a(Q_{s,a}) $\n",
    "3. Execute action $a$ in the game, observe reward r, next state s'\n",
    "4. Store transition (s, a, r, s') in the replay buffer\n",
    "5. Sample a random mini-batch of transitions from the replay buffer\n",
    "6. For every transition in the buffer, calculate target $ y = r $ if episode is over, otherwise $ y = r\\ +\\ \\gamma max_a(Q_{s',a'}) $\n",
    "7. Calculate loss: $ L\\ =\\ (Q_{s,a}\\ -\\ y)^2  $\n",
    "8. Update Q(s, a) using SGD, minimizing the loss function\n",
    "9. Every N steps copy weights from Q to Q^\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "Deep Learning: use either PyTorch (easier) or Tensorflow (great for production, steeper learning curve)\n",
    "\n",
    "\n",
    "**Simple then Expand**\n",
    "\n",
    "- Reinforcement Learning started out with simple applications of the Bellman Equation\n",
    "- Gradually involved enhancements and workarounds when that performed poorly on tasks\n",
    "- Basic implementation of Q Learning can only handle fairly simple tasks so we're going to start out with Pong\n",
    "- Learning Deep Q enhancements, we can try out more complex games like Doom\n",
    "\n",
    "\n",
    "**Wrappers**\n",
    "\n",
    "- In OpenAI Gym, Wrappers are a layer of code that takes observations raw pixels from the environment and processes them before they enter the neural network\n",
    "- A layer of code around OpenAI gym\n",
    "- Transforms observations before passing them to the network\n",
    "- Transforms actions before passing them to the environment\n",
    "\n",
    "\n",
    "**How to Run Deep Q Pong**\n",
    "\n",
    "- ```dqn_basic.py --cuda```\n",
    "- ```tensorboard --logdir runs```\n",
    "- took ~2 hours to train on a Nvidia GTX 1080ti GPU\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video: Deep Q Learning Pong Tutorial](https://www.youtube.com/watch?v=pST6caY3mu8)\n",
    "- [Code Link: DQN Pong](https://github.com/colinskow/move37/tree/master/dqn)\n",
    "- [Siraj: Image Recognition Tutorial](https://www.youtube.com/watch?v=cAICT4Al5Ow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay (PER)\n",
    "\n",
    "This article is mainly citing Thomas Simonini's blog\n",
    "    - Thomas Simonini's blog about Improvements in Deep Q Learning\n",
    "    - PRIORITIZED EXPERIENCE REPLAY by Google DeepMind\n",
    "    - SLM Lab@School of AI Github\n",
    "    - OpenAI Github\n",
    "    - Patrick Emami's blog\n",
    "    - Jaromiru's blog about LET’S MAKE A DQN\n",
    "\n",
    "\n",
    "### Theory\n",
    "\n",
    "- Some experiences may be more important than others for our training, but might occur less frequently\n",
    "- Because we sample the batch uniformly (selecting the experiences randomly) these rich experiences that occur rarely have practically no chance to be selected.\n",
    "\n",
    "\n",
    "### Priority $p_t$\n",
    "\n",
    "- We want to take in a priority experience where there is a big difference between our prediction and the TD target, since it means that we have a lot of learn about it.\n",
    "- Define Priority $p_t$ as:\n",
    "\n",
    "$$ \\large p_t = |\\delta_{t}| + e $$\n",
    "\n",
    "$|\\delta_{t}|$: Magnitude of our TD error  \n",
    "$e$: Constant assures that no experience has 0 probability to be taken\n",
    "\n",
    "- TD Error: $error\\ = |Q(s,a)\\ - T(S)|$ where $T(S)\\ =\\ r + \\gamma Q\\big(s', argmax_aQ(s',a)\\big)$\n",
    "\n",
    "\n",
    "### Probability $P(i)$\n",
    "\n",
    "Priority is translated to probability of being chosen for replay.\n",
    "\n",
    "A sample _i_ has a probability of being picked during the experience replay determined by a formula:\n",
    "\n",
    "$$ \\large P(i) = \\frac{p_i^a}{\\sum{k} p_k^a}$$\n",
    "\n",
    "where\n",
    "- $p_i$ is the Priority value\n",
    "- $\\sum{k} p_k^a$ - All priority values in Replay Buffer\n",
    "- $a$ - Hyperparameter used to reintroduce some randomness in the experience selection for the replay buffer.\n",
    "    - If $a\\ = 0 \\rightarrow $ pure uniform randomness\n",
    "    - If $a\\ = 1 \\rightarrow $ only select the experience with the highest priorities\n",
    "\n",
    "- $P(i)$ is probability and $p_i$ is priority\n",
    "- We defined modified probability to pick more experiences with higher priorities\n",
    "\n",
    "\n",
    "### Importance Sampling Weights (IS)\n",
    "\n",
    "- Samples that have high priority are likely to be used for training many times in comparison with low priority experiences (=bias)\n",
    "- Therefore, we will update our weights with only a small portion of experiences that we consider to be really interesting\n",
    "- To correct this bias, we use i**mportance sampling weights** (IS) that will **adjust the updating by reducing the weights** of the often seen samples\n",
    "\n",
    "$$ \\large W_i\\ =\\ \\big(\\frac{1}{N}\\ *\\ \\frac{1}{P(i)}\\big)^b $$\n",
    "\n",
    "where\n",
    "- $\\frac{1}{N}$ is Replay Buffer Size\n",
    "- $P(i)$ is Sampling probability\n",
    "- $b$ controls how much the IS w affects learning\n",
    "- Close to 0 at the beginning of learning and annealed up to 1 over the duration of training because **these weights are more important in the end of learning when our Q values begin to converge**\n",
    "\n",
    "\n",
    "### Google DeepMind Paper\n",
    "\n",
    "Define Priority $p_i$, pick Probability of $P(j)$ and update with importance sampling weight $w_i$.\n",
    "\n",
    "![Google DeepMind PER Paper Algorithm 1](imgs/move_37_google_deepmind_per_paper_algorithm1.jpg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- TODO: Add relevant code parts!\n",
    "\n",
    "- Priority Part ([SLM Lab](https://github.com/kengz/SLM-Lab/blob/master/slm_lab/agent/memory/prioritized.py))\n",
    "- Priority Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "- Probability and IS Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Uniform sampling from replay memories is not an efficient way to learn. Rather, using a clever prioritization scheme to label the experiences in reply memory, learning can be carried out much faster and more effectively. However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this. It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results.\n",
    "\n",
    "\n",
    "### More Papers (Intermediate)\n",
    "\n",
    "- [Distributed Prioritized Experience Replay ICLR 2018](https://arxiv.org/abs/1803.00933)\n",
    "- [A Deeper Look at Planning as Learning from Replay (Richard Sutton 2015)](http://proceedings.mlr.press/v37/vanseijen15.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "\n",
    "**Refresher**\n",
    "\n",
    "Ordinary Q learning was not sufficient enough to solve a simple game environment so we moved towards Q-Table and to make it much better, we moved towards Deep Q-Networks.\n",
    "\n",
    "Let's see how an ordinary Q-Network is transformed into a Deep Q-Network:\n",
    "\n",
    "- Move from a single layer network to a multi-layer CNN\n",
    "- Introduce experience replay to make the network train itself from the past knowledge stored in the memory\n",
    "- Use a Target network to compute target Q-values while updating\n",
    "\n",
    "![Sample DQN Flow](imgs/move_37_sample_dqn_flow.jpg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "These 3 major improvements allowed the [Google DeepMind](https://arxiv.org/pdf/1312.5602v1.pdf) team to achieve great performance on many Atari games using DQN agent. DQN was introduced in 2014. Since then, a lot of improvements have been made and DQN is no longer the most advanced agent anymore.\n",
    "\n",
    "2 simple additions to DQN, which allow for improved performance, stability and faster training time:\n",
    "    1. Double DQN\n",
    "    2. Dueling DQN\n",
    "\n",
    "\n",
    "**Architecture**\n",
    "\n",
    "- The Q-value $Q(s,a)$ we know correspond to how good it is to take a certain action in a certain state. This value can be further decomposed into two more basic notions value:\n",
    "\n",
    "    1. function - $Vs$. Simple value of how good it is to be in any state.\n",
    "    2. advantage function - $Aa$. Advantage function explains how taking certain action would be better compared to others.\n",
    "\n",
    "Then $Q(s,a)$ can be a combination of V and A as follows:\n",
    "\n",
    "$$ \\large Q(s,a)\\ =\\ A(s,a)\\ +\\ V(s) \\\\ $$\n",
    "\n",
    "To make changes to DQN to make it better, we need is a network that separately computes the advantage and value functions and combines them back into a single Q-function at the final layer:\n",
    "\n",
    "_Above: Regular DQN with a single stream for Q-values._  \n",
    "_Below: Dueling DQN where the value and advantage are calculated separately and then combined only at the final layer into a Q value._\n",
    "![Regular DQN vs Dueling DQN](imgs/move_37_regular_dqn_and_dueling_dqn.jpg)\n",
    "\n",
    "**Additional Resources**:\n",
    "\n",
    "- [Medium: Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)\n",
    "\n",
    "<br/>\n",
    "\n",
    "The key is to realize the benefits and appreciate that the agent can learn which states are valuable and which are not without having to learn the effect of each action at each state as it is calculating $Vs$. \n",
    "\n",
    "![Q_s_a_bad](imgs/move_37_Q_s_a_bad.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Not being able to find $V(s)$ and $A(s,a)$ given $Q(s,a)$ will be a problem for our back-propagation. To avoid this problem, we can force our **advantage function estimator** to have 0 advantage at the chosen action. To do that, we subtract the average advantage of all actions possible of the state:\n",
    "\n",
    "$$ \\large Q(s,a;\\color{purple}\\theta,\\color{green}\\alpha,\\color{blue}\\beta)\\ =\\ \\color{blue}{V(s;\\theta,\\beta)}\\ +\\ \\big (\\color{green}{A(s,a;\\theta,\\alpha)} -\\ \\color{red}{\\frac{1}{\\mathcal{A}} \\sum_{a'} A(s,a';\\theta\\alpha)} \\big )\\\\ $$\n",
    "\n",
    "Where:\n",
    "\n",
    "$\\color{purple}\\theta$: Common Network parameters\n",
    "\n",
    "$\\color{green}{A(s,a)}$: Advantage function\n",
    "\n",
    "$\\color{green}\\alpha$: Advantage stream parameters\n",
    "    \n",
    "$\\color{blue}{V(s)}$: State Value Function\n",
    "\n",
    "$\\color{blue}\\beta$: Value stream parameters\n",
    "\n",
    "$\\color{red}{\\frac{1}{\\mathcal{A}} \\sum_{a'} A(s,a';\\theta,\\alpha)}$: Average Advantage\n",
    "\n",
    "\n",
    "![Q_s_a_theta_alpha_beta](imgs/move_37_Q_s_a_theta_alpha_beta.png)\n",
    "\n",
    "<br/>\n",
    "\n",
    "Therefore, this architecture helps us accelerate the training. We can calculate the value of a state without calculating the $Q(s,a)$ for each action at that state. And it can help us find much more reliable Q values for each action by decoupling the estimation between two streams.\n",
    "\n",
    "\n",
    "**Implementation**:\n",
    "\n",
    "TODO: Get implementation below!\n",
    "\n",
    "**Conclusion**:\n",
    "\n",
    "Dueling DQN works much better than DQN in the case of training time and performance. Create your own model by modifying this implementation and try it for different ATARI Environments to realize the power of Dueling DQN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dueling DQN Implementation using PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Study Guide\n",
    "\n",
    "[Neural Networks Study Guide](https://www.theschool.ai/wp-content/uploads/2018/10/Move-37-Week-6-Study-Guide.pdf)\n",
    "\n",
    "\n",
    "**Feed Forward Neural Networks**\n",
    "\n",
    "_Multilayer Perceptron_\n",
    "\n",
    "- historical name for a feed forward neural network, the fundamental algorithm for deep learning\n",
    "- Modeled after the brain, from a 1950's level of understanding\n",
    "- Commonly represent the network as a graph of interconnected nodes\n",
    "- Each hidden layer performs a set of multidimensional transformations of the data\n",
    "- Each hidden unit (node) is compromised of a set of weights, which are adjusted through training to learn the mapping between sets of input and output\n",
    "\n",
    "\n",
    "**Recurrent Neural Network (1987)**\n",
    "\n",
    "[Generalization of Backpropagation to Recurrent and Higher Order Neural Networks](https://papers.nips.cc/paper/67-generalization-of-back-propagation-to-recurrent-and-higher-order-neural-networks.pdf)\n",
    "\n",
    "**Long Short-Term Memory (1997)**\n",
    "\n",
    "[Long Short-Term Memory](https://www.bioinf.jku.at/publications/older/2604.pdf)\n",
    "\n",
    "**Gated Recurrent Unit (2014)**\n",
    "\n",
    "[Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation](https://arxiv.org/abs/1406.1078)\n",
    "\n",
    "**Recent Models**\n",
    "\n",
    "- [Hierarchical Multiscale Recurrent Neural Networks (2017)](https://arxiv.org/pdf/1609.01704.pdf)\n",
    "- [LARNN: Linear Attention Recurrent Neural Network (2018)](https://arxiv.org/abs/1808.05578v1)\n",
    "- [On Extended Long Short-term Memory and Dependent Bidirectional Recurrent Neural Network (2018)](https://arxiv.org/abs/1803.01686v2)\n",
    "\n",
    "\n",
    "**RNN explainability:**\n",
    "\n",
    "- [Visualizing and Understanding Recurrent Networks (2015)](https://arxiv.org/abs/1506.02078)\n",
    "- [LISA: Explaining Recurrent Neural Network Judgments via Layer-wIse Semantic Accumulation and Example to Pattern Transformation (2018)](https://arxiv.org/abs/1808.01591v1)\n",
    "\n",
    "\n",
    "**Also see:**\n",
    "\n",
    "- [Empirical Evaluation of Gated Recurrent Neural Networks on Sequence Modeling (2014)](https://arxiv.org/abs/1412.3555)\n",
    "- [Understanding LSTM Networks (2015)](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)\n",
    "- [An Empirical Exploration of Recurrent Network Architectures (2015)](http://proceedings.mlr.press/v37/jozefowicz15.pdf)\n",
    "- [Deep Learning: Feedforward Neural Network (2017)](https://towardsdatascience.com/deep-learning-feedforward-neural-network-26a6705dbdc7)\n",
    "- [Stanford CS231n Lecture 10 (2017)](https://www.youtube.com/watch?v=6niqTuYFZLQ)\n",
    "- [Understanding Gated Recurrent Unit (GRU) Deep Neural Network](https://www.youtube.com/watch?v=pYRIOGTPRPU)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quiz: Neural Networks\n",
    "\n",
    "Test your knowledge on FFNN, RNN, LSTM, and GRU architectures\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Given a feed forward network with one hidden layer, an activation function, and as many hidden units as we need, which of the following statements are true?\n",
    "\n",
    "    - [x] This network can approximate any continuous function (to an arbitrary level of precision)\n",
    "    - [ ] This network can approximate any linear function (to an arbitrary level of precision)\n",
    "    - [ ] This network can approximate any function (to an arbitrary level of precision)\n",
    "    - [x] This network can approximate a nonlinear function\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "The universal approximation theorem allows our hypothetical network to approximate any continuous function. The activation function makes our model robust to non-linearity.\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "\n",
    "Which of the following tasks are suited to an RNN?\n",
    "\n",
    "    - [ ] Image classification\n",
    "    - [x] Image captioning\n",
    "    - [x] Sentiment analysis\n",
    "    - [x] Machine translation\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "Image classification is best left to the CNN architecture. Image captioning is a fixed input, variable size output problem. Sentiment analysis is a variable input, fixed size output problem. Machine translation is a variable input, variable size output problem.\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "\n",
    "Which of the following statements are true about the LSTM?\n",
    "\n",
    "    - [ ] LSTMs were first developed in 2006\n",
    "    - [x] LSTMs were first developed in the late 90s\n",
    "    - [ ] The input gate controls how much to write to a cell\n",
    "    - [x] The input gate controls whether to write to a cell\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "The LSTM architecture was first developed in 1997. The i-gate controls whether to write to a cell, while the g-gate controls how much to write.\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "\n",
    "Which of the following statements are true about the GRU architecture?\n",
    "\n",
    "    - [x] The GRU is simpler than the LSTM\n",
    "    - [x] The GRU uses the ‘reset’ gate to allow forgetting the previous state\n",
    "    - [ ] The GRU uses extra hidden units\n",
    "    - [x] The GRU uses the ‘update’ gate to modulate between the current and candidate activations\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "For information concerning the GRU be sure to check out this very informative [talk](https://www.youtube.com/watch?v=pYRIOGTPRPU) by Andrew Ng \n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "\n",
    "Which of the following statements are true about vanishing/exploding gradients?\n",
    "\n",
    "    - [x] Backpropagation through time can lead to vanishing/exploding gradients\n",
    "    - [x] RNNs can develop problems with vanishing/exploding gradients\n",
    "    - [x] gradient clipping is a simple hack for vanishing/exploding gradients\n",
    "    - [x] LSTMs and GRUs help to mitigate problems with vanishing/exploding gradients\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "For more information on RNNs, LSTMs, and GRUs be sure to check out this [talk](https://www.youtube.com/watch?v=6niqTuYFZLQ) from Stanford University"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignments (DQN Improvements)\n",
    "\n",
    "3 pivotal papers on Deep Q Learning:\n",
    "\n",
    "- [Paper 1 – Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "\n",
    "- [Paper 2 – Episodic Memory with Deep Q Networks](https://www.ijcai.org/proceedings/2018/0337.pdf)\n",
    "\n",
    "- [Paper 3 – Dueling Network Architectures with Deep Reinforcement Learning](http://proceedings.mlr.press/v48/wangf16.pdf)\n",
    "\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- [Medium: Deep Reinforcement Learning: Deep Q Learning and Policy Gradients (Towards AGI)](https://medium.com/deep-math-machine-learning-ai/ch-13-deep-reinforcement-learning-deep-q-learning-and-policy-gradients-towards-agi-a2a0b611617e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment: Deep Q Learning\n",
    "\n",
    "This weeks homework assignment is build a deep q network to defeat the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment in OpenAI’s Gym environment. Use this [repository](https://github.com/AndersonJo/dqn-pytorch) as a helpful guide. Train it and test it, if your algorithm successfully learns how to beat the environment, you’ve successfully completed the assignment. Good luck!\n",
    "\n",
    "See [homework 06 notebook](homework06/homework06.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
