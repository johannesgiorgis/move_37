{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 06 Notes - Deep Reinforcement Learning <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Deep-RL-for-Database-Optimization\" data-toc-modified-id=\"Deep-RL-for-Database-Optimization-1\">Deep RL for Database Optimization</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-1.1\">Notes</a></span></li><li><span><a href=\"#Take-Aways\" data-toc-modified-id=\"Take-Aways-1.2\">Take Aways</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-1.3\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Deep-Q-Learning-Pong-Tutorial\" data-toc-modified-id=\"Deep-Q-Learning-Pong-Tutorial-2\">Deep Q Learning Pong Tutorial</a></span><ul class=\"toc-item\"><li><span><a href=\"#Notes\" data-toc-modified-id=\"Notes-2.1\">Notes</a></span></li><li><span><a href=\"#Learning-Resources\" data-toc-modified-id=\"Learning-Resources-2.2\">Learning Resources</a></span></li></ul></li><li><span><a href=\"#Prioritized-Experience-Replay-(PER)\" data-toc-modified-id=\"Prioritized-Experience-Replay-(PER)-3\">Prioritized Experience Replay (PER)</a></span><ul class=\"toc-item\"><li><span><a href=\"#Theory\" data-toc-modified-id=\"Theory-3.1\">Theory</a></span></li><li><span><a href=\"#Priority-$p_t$\" data-toc-modified-id=\"Priority-$p_t$-3.2\">Priority $p_t$</a></span></li><li><span><a href=\"#Probability-$P(i)$\" data-toc-modified-id=\"Probability-$P(i)$-3.3\">Probability $P(i)$</a></span></li><li><span><a href=\"#Importance-Sampling-Weights-(IS)\" data-toc-modified-id=\"Importance-Sampling-Weights-(IS)-3.4\">Importance Sampling Weights (IS)</a></span></li><li><span><a href=\"#Google-DeepMind-Paper\" data-toc-modified-id=\"Google-DeepMind-Paper-3.5\">Google DeepMind Paper</a></span></li><li><span><a href=\"#Implementation\" data-toc-modified-id=\"Implementation-3.6\">Implementation</a></span></li><li><span><a href=\"#Summary\" data-toc-modified-id=\"Summary-3.7\">Summary</a></span></li><li><span><a href=\"#More-Papers-(Intermediate)\" data-toc-modified-id=\"More-Papers-(Intermediate)-3.8\">More Papers (Intermediate)</a></span></li></ul></li><li><span><a href=\"#Dueling-DQN\" data-toc-modified-id=\"Dueling-DQN-4\">Dueling DQN</a></span></li><li><span><a href=\"#Neural-Networks-Study-Guide\" data-toc-modified-id=\"Neural-Networks-Study-Guide-5\">Neural Networks Study Guide</a></span></li><li><span><a href=\"#Quiz:-Neural-Networks\" data-toc-modified-id=\"Quiz:-Neural-Networks-6\">Quiz: Neural Networks</a></span><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-6.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-6.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-6.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-6.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-6.5\">Question 5</a></span></li></ul></li><li><span><a href=\"#Reading-Assignments-(DQN-Improvements)\" data-toc-modified-id=\"Reading-Assignments-(DQN-Improvements)-7\">Reading Assignments (DQN Improvements)</a></span></li><li><span><a href=\"#Homework-Assignment:-Deep-Q-Learning\" data-toc-modified-id=\"Homework-Assignment:-Deep-Q-Learning-8\">Homework Assignment: Deep Q Learning</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep RL for Database Optimization\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "We can use deep reinforcement learning to optimize a SQL database, and in this video we'll optimize the ordering of a series of SQL queries such that it involves the minimum possible memory/computation footprint. Deep RL involves using a neural network to approximate reinforcement learning functions, like the Q (quality) function. After we frame our database as a Markov Decision Process, I'll use Python to build a Deep Q Network to optimize SQL queries. Enjoy!\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "SQL:\n",
    "\n",
    "$ SQL\\ Statement \\rightarrow Parsing \\rightarrow (Parse\\ Tree) \\rightarrow Binding \\rightarrow (Algebrized\\ Tree) \\rightarrow Query\\ Optimization \\rightarrow (Execution\\ Plan) \\rightarrow Query\\ Execution \\rightarrow Query\\ Results $\n",
    "\n",
    "\n",
    "### Take Aways\n",
    "\n",
    "- Deep Reinforcement Learning involves using a Neural Network to Approximate Reinforcement Learning Functions like the Q Function\n",
    "- We can assess the quality or Q of State Action Pairs by computing A Q Table\n",
    "- Q Learning involves approximating the relationship between State Action Pairs and Q Values in this table using Neural Networks\n",
    "\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=Rw3ewEXOKC8)\n",
    "- [Code Link: SQL Database Optimization](https://github.com/llSourcell/SQL_Database_Optimization)\n",
    "- [Irselab: SQL Query Optimization Meets Deep Reinforcement Learning](https://rise.cs.berkeley.edu/blog/sql-query-optimization-meets-deep-reinforcement-learning/)\n",
    "- [MLDB: Machine Learning Database](https://mldb.ai/)\n",
    "- [Microsoft: Machine Learning Services in SQL Server 2017](https://docs.microsoft.com/en-us/sql/advanced-analytics/what-is-sql-server-machine-learning?view=sql-server-2017)\n",
    "- [Towards Data Science: Mchine Learning in your Database](https://towardsdatascience.com/machine-learning-in-your-database-the-case-for-and-against-bigquery-ml-4f2309282fda)\n",
    "- [Quora: Which database is best for machine learning](https://www.quora.com/Which-database-is-best-for-machine-learning)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Q Learning Pong Tutorial\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Learn how to build an AI that plays Pong like a boss, with Deep Q Learning. Discover how neural networks can learn to play challenging video games at superhuman levels by looking at raw pixels.\n",
    "\n",
    "\n",
    "### Notes\n",
    "\n",
    "**How to Play Pong Like a BOSS with Deep Q Learning**\n",
    "\n",
    "- If we can write code which masters complex video games we can use that same code to master complex real-life problems\n",
    "\n",
    "\n",
    "**Taking Q DEEP**\n",
    "\n",
    "- In vanilla Q Learning, we are storing a huge lookup table whose size is the number of possible states times the number of possible actions\n",
    "- PROBLEM: possible game states are astronomically large!\n",
    "- Bellman Equation remains the same\n",
    "- SOLUTION: replace the Q lookup table with a neural network, which approximates a function for Q of state and action\n",
    "- BEFORE: we use the most recent value calculation and a learning rate to update our Q table\n",
    "- NOW: Update the Q Network with stochastic gradient descent (SGD) and Back propagation\n",
    "\n",
    "\n",
    "**Over-simplified Q Learning Algorithm**\n",
    "\n",
    "1. Initialize Q(s, a) randomly\n",
    "2. Interact with the environment to obtain (s, a, r, s')\n",
    "3. Calculate loss: $ L\\ =\\ (Q_{s,a} - r)^2 $  \n",
    "   otherwise: $ L\\ =\\ (Q_{s,a}\\ -\\ (r\\ +\\ \\gamma max_a [Q_{s',a'}]))^2  $\n",
    "4. Update Q(s, a) using SGD, minimizing the loss function\n",
    "5. Repeat steps 2 - 4 until converged\n",
    "\n",
    "\n",
    "**Explore vs. Exploit**\n",
    "\n",
    "- To reach an optimal policy, we need to balance exploration with exploitation\n",
    "- Continue to use Epsilon greedy method\n",
    "- Start with epsilon = 1 -> taking all random actions\n",
    "- Gradually taper to lower value over a fixed number of game frames\n",
    "\n",
    "\n",
    "**Replay Buffer**\n",
    "\n",
    "- SGD optimization requires Independent and Identically Distributed training data\n",
    "- Our state transitions are highly correlated\n",
    "- Store a long list of (s, a, r, s') transitions\n",
    "- Randomly sample batches to train on from the buffer\n",
    "- New data kicks off old data\n",
    "- By randomly sampling from a long list, it breaks the correlation that comes from sampling values right next to each other and allows the model to converge\n",
    "\n",
    "\n",
    "**Target Network**\n",
    "\n",
    "- Loss function, $ L\\ =\\ (Q_{s,a}\\ -\\ (r\\ +\\ \\gamma max_a [Q_{s',a'}]))^2  $\n",
    "- We're updating Q(s, a) and Q(s', a') in the same step\n",
    "- We'll store them in a different network (target_network)\n",
    "- Copy the weights from main to target at a fixed interval\n",
    "\n",
    "\n",
    "**Predicting Motion**\n",
    "\n",
    "- Markov Property: The Past Doesn't Matter Baby!\n",
    "- But when there's motion it does matter\n",
    "- Solution: stack several recent frames together as input\n",
    "\n",
    "\n",
    "**Network Architecture**\n",
    "\n",
    "- How do we take a large number of pixels from the screen and pick out which objects are important to achieving results in a video game? The same neural network used in cutting edge image recognition is perfect for applying reinforcement learning to screen pixels\n",
    "- Use 3 layers of convolutions with each one passing through a ReLu activation\n",
    "- Input\n",
    "- Conv2D -> ReLu (x3)\n",
    "- Fully connected (512) -> ReLu\n",
    "- Actions: Output layer spits out the values of each action\n",
    "\n",
    "\n",
    "**Deep Q Network Algorithm**\n",
    "\n",
    "1. Initialize Q(s,a) and Q^(s,a) (target network) with random weights\n",
    "2. With probability epsilon, select random action $a$, otherwise $ a = max_a(Q_{s,a}) $\n",
    "3. Execute action $a$ in the game, observe reward r, next state s'\n",
    "4. Store transition (s, a, r, s') in the replay buffer\n",
    "5. Sample a random mini-batch of transitions from the replay buffer\n",
    "6. For every transition in the buffer, calculate target $ y = r $ if episode is over, otherwise $ y = r\\ +\\ \\gamma max_a(Q_{s',a'}) $\n",
    "7. Calculate loss: $ L\\ =\\ (Q_{s,a}\\ -\\ y)^2  $\n",
    "8. Update Q(s, a) using SGD, minimizing the loss function\n",
    "9. Every N steps copy weights from Q to Q^\n",
    "10. Repeat from step 2 until converged\n",
    "\n",
    "Deep Learning: use either PyTorch (easier) or Tensorflow (great for production, steeper learning curve)\n",
    "\n",
    "\n",
    "**Simple then Expand**\n",
    "\n",
    "- Reinforcement Learning started out with simple applications of the Bellman Equation\n",
    "- Gradually involved enhancements and workarounds when that performed poorly on tasks\n",
    "- Basic implementation of Q Learning can only handle fairly simple tasks so we're going to start out with Pong\n",
    "- Learning Deep Q enhancements, we can try out more complex games like Doom\n",
    "\n",
    "\n",
    "**Wrappers**\n",
    "\n",
    "- In OpenAI Gym, Wrappers are a layer of code that takes observations raw pixels from the environemnt and processes them before they enter the neural network\n",
    "- A layer of code around OpenAI gym\n",
    "- Transforms observations before passing them to the network\n",
    "- Transforms actions before passing them to the environment\n",
    "\n",
    "\n",
    "**How to Run Deep Q Pong**\n",
    "\n",
    "- ```dqn_basic.py --cuda```\n",
    "- ```tensorboard --logdir runs```\n",
    "- took ~2 hours to train on a Nvidia GTX 1080ti GPU\n",
    "\n",
    "### Learning Resources\n",
    "\n",
    "- [Youtube Video: Deep Q Learning Pong Tutorial](https://www.youtube.com/watch?v=pST6caY3mu8)\n",
    "- [Code Link: DQN Pong](https://github.com/colinskow/move37/tree/master/dqn)\n",
    "- [Siraj: Image Recognition Tutorial](https://www.youtube.com/watch?v=cAICT4Al5Ow)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prioritized Experience Replay (PER)\n",
    "\n",
    "This article is mainly citing Thomas Simonini's blog\n",
    "    - Thomas Simonini's blog about Improvements in Deep Q Learning\n",
    "    - PRIORITIZED EXPERIENCE REPLAY by Google DeepMind\n",
    "    - SLM Lab@School of AI Github\n",
    "    - OpenAI Github\n",
    "    - Patrick Emami's blog\n",
    "    - Jaromiru's blog about LET’S MAKE A DQN\n",
    "\n",
    "\n",
    "### Theory\n",
    "\n",
    "- Some experiences may be more important than others for our training, but might occur less frequently\n",
    "- Because we sample the batch uniformly (selecting the experiences randomly) these rich experiences that occur rarely have practically no chance to be selected.\n",
    "\n",
    "\n",
    "### Priority $p_t$\n",
    "\n",
    "- We want to take in a priority experience where there is a big difference between our prediction and the TD target, since it means that we have a lot of learn about it.\n",
    "- Define Priority $p_t$ as:\n",
    "\n",
    "$$ \\large p_t = |\\delta_{t}| + e $$\n",
    "\n",
    "$|\\delta_{t}|$: Magnitude of our TD error  \n",
    "$e$: Constant assures that no experience has 0 probability to be taken\n",
    "\n",
    "- TD Error: $error\\ = |Q(s,a)\\ - T(S)|$ where $T(S)\\ =\\ r + \\gamma Q(s', argmax_aQ(s',a))$\n",
    "\n",
    "\n",
    "### Probability $P(i)$\n",
    "\n",
    "Priority is translated to probability of being chosen for replay.\n",
    "\n",
    "A sample _i_ has a probability of being picked during the experience replay determined by a formula:\n",
    "\n",
    "$$ \\large P(i) = \\frac{p_i^a}{\\sum{k} p_k^a}$$\n",
    "\n",
    "where\n",
    "- $p_i$ is the Priority value\n",
    "- $\\sum{k} p_k^a$ - All priority values in Replay Buffer\n",
    "- $a$ - Hyperparameter used to reintroduce some randomness in the experience selection for the replay buffer.\n",
    "    - If $a\\ = 0 \\rightarrow $ pure uniform randomness\n",
    "    - If $a\\ = 1 \\rightarrow $ only select the experience with the highest priorities\n",
    "\n",
    "- $P(i)$ is probability and $p_i$ is priority\n",
    "- We defined modified probability to pick more experiences with higher priorities\n",
    "\n",
    "\n",
    "### Importance Sampling Weights (IS)\n",
    "\n",
    "- Samples that have high priority are likely to be used for training many times in comparison with low priority experiences (=bias)\n",
    "- Therefore, we will update our weights with only a small portion of experiences that we consider to be really interesting\n",
    "- To correct this bias, we use importance sampling weights (IS) that will **adjust the updating by reducing the weights** of the often seen samples\n",
    "\n",
    "$$ \\larger W_i\\ =\\ (\\frac{1}{N}\\ *\\ \\frac{1}{P(i)})^b $$\n",
    "\n",
    "where\n",
    "- $\\frac{1}{N}$ is Replay Buffer Size\n",
    "- $P(i)$ is Sampling probability\n",
    "- $b$ controls how much the IS w affects learning\n",
    "- Close to 0 at the beginning of learning and annealed up to 1 over the duration of training because **these weights are more important in the end of learning when our Q values begin to converge**\n",
    "\n",
    "\n",
    "### Google DeepMind Paper\n",
    "\n",
    "Define Priority $p_i$, pick Probability of $P(j)$ and update with importance sampling weight $w_i$.\n",
    "\n",
    "![Google DeepMind PER Paper Algorithm 1](imgs/move_37_google_deepmind_per_paper_algorithm1.jpg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "### Implementation\n",
    "\n",
    "- TODO: Add relevant code parts!\n",
    "\n",
    "- Priority Part ([SLM Lab](https://github.com/kengz/SLM-Lab/blob/master/slm_lab/agent/memory/prioritized.py))\n",
    "- Priority Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "- Probability and IS Part ([OpenAI](https://github.com/openai/baselines/blob/master/baselines/deepq/replay_buffer.py))\n",
    "\n",
    "\n",
    "### Summary\n",
    "\n",
    "Uniform sampling from replay memories is not an efficient way to learn. Rather, using a clever prioritization scheme to label the experiences in reply memory, learning can be carried out much faster and more effectively. However, certain biases are introduced by this non-uniform sampling; hence, weighted importance sampling must be employed in order to correct for this. It is shown through experimentation with the Atari Learning Environment that prioritized sampling with Double DQN significantly outperforms the previous state-of-the-art Atari results.\n",
    "\n",
    "\n",
    "### More Papers (Intermediate)\n",
    "\n",
    "- [Distributed Prioritized Experience Replay ICLR 2018](https://arxiv.org/abs/1803.00933)\n",
    "- [A Deeper Look at Planning as Learning from Replay (Richard Sutton 2015)](http://proceedings.mlr.press/v37/vanseijen15.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dueling DQN\n",
    "\n",
    "\n",
    "**Additional Resources**:\n",
    "\n",
    "- [Medium: Improvements in Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks Study Guide\n",
    "\n",
    "[Neural Networks Study Guide](https://www.theschool.ai/wp-content/uploads/2018/10/Move-37-Week-6-Study-Guide.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "## Quiz: Neural Networks\n",
    "\n",
    "Test your knowledge on FFNN, RNN, LSTM, and GRU architectures\n",
    "\n",
    "### Question 1\n",
    "\n",
    "\n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "\n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "\n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "\n",
    "\n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "- [] \n",
    "\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading Assignments (DQN Improvements)\n",
    "\n",
    "3 pivotal papers on Deep Q Learning:\n",
    "\n",
    "- [Paper 1 – Playing Atari with Deep Reinforcement Learning](https://arxiv.org/pdf/1312.5602.pdf)\n",
    "\n",
    "- [Paper 2 – Episodic Memory with Deep Q Networks](https://www.ijcai.org/proceedings/2018/0337.pdf)\n",
    "\n",
    "- [Paper 3 – Dueling Network Architectures with Deep Reinforcement Learning](http://proceedings.mlr.press/v48/wangf16.pdf)\n",
    "\n",
    "\n",
    "**Additional Resources**\n",
    "\n",
    "- [Medium: Deep Reinforcement Learning: Deep Q Learning and Policy Gradients (Towards AGI)](https://medium.com/deep-math-machine-learning-ai/ch-13-deep-reinforcement-learning-deep-q-learning-and-policy-gradients-towards-agi-a2a0b611617e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Homework Assignment: Deep Q Learning\n",
    "\n",
    "This weeks homework assignment is build a deep q network to defeat the [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment in OpenAI’s Gym environment. Use this [repository](https://github.com/AndersonJo/dqn-pytorch) as a helpful guide. Train it and test it, if your algorithm successfully learns how to beat the environment, you’ve successfully completed the assignment. Good luck!\n",
    "\n",
    "See [homework 06 notebook](homework06/homework06.ipynb)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
