{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 07 Notes - Policy Based Methods <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Neuroevolution-Meta-Learning\" data-toc-modified-id=\"Neuroevolution-Meta-Learning-1\">Neuroevolution Meta-Learning</a></span></li><li><span><a href=\"#Policy-Search-Algorithms\" data-toc-modified-id=\"Policy-Search-Algorithms-2\">Policy Search Algorithms</a></span></li><li><span><a href=\"#Evolutionary-Algorithms-Study-Guide\" data-toc-modified-id=\"Evolutionary-Algorithms-Study-Guide-3\">Evolutionary Algorithms Study Guide</a></span></li><li><span><a href=\"#Evolutionary-Algorithms-Quiz\" data-toc-modified-id=\"Evolutionary-Algorithms-Quiz-4\">Evolutionary Algorithms Quiz</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-4.0.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-4.0.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-4.0.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-4.0.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-4.0.5\">Question 5</a></span></li><li><span><a href=\"#Question-6\" data-toc-modified-id=\"Question-6-4.0.6\">Question 6</a></span></li><li><span><a href=\"#Question-7\" data-toc-modified-id=\"Question-7-4.0.7\">Question 7</a></span></li><li><span><a href=\"#Question-8\" data-toc-modified-id=\"Question-8-4.0.8\">Question 8</a></span></li><li><span><a href=\"#Question-9\" data-toc-modified-id=\"Question-9-4.0.9\">Question 9</a></span></li></ul></li></ul></li><li><span><a href=\"#Homework-Assignment-(Neuroevolution)\" data-toc-modified-id=\"Homework-Assignment-(Neuroevolution)-5\">Homework Assignment (Neuroevolution)</a></span></li><li><span><a href=\"#Control-Theory\" data-toc-modified-id=\"Control-Theory-6\">Control Theory</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neuroevolution Meta-Learning\n",
    "\n",
    "\n",
    "**Video Description:**\n",
    "\n",
    "Meta learning describes the concept of 'learning to learn'. What if we could have AI learn how to optimize itself? An AI could learn the optimal hyper-parameters, architecture, and even dataset! Its a really interesting topic, and in this video I'll describe some meta learning techniques and focus on one in particular; deep neuro-evolution. We'll build an image classifier using a deep neuro-evolutionary algorithm. Enjoy!\n",
    "\n",
    "\n",
    "**Notes**\n",
    "\n",
    "- Meta-Learning: When a Meta-Level AI trains a Bottom-level AI\n",
    "- Why Meta Learning?\n",
    "    - Faster AI Systems\n",
    "    - More Adaptable to Environmental Changes\n",
    "    - Generalizes to more tasks\n",
    "- Neuroevolution, a specific meta-learning technique is the process of using an evolutionary algorithm to learn neural architectures\n",
    "- Evolutionary Algorithms:\n",
    "    1. Initial Population\n",
    "    2. The environment changes\n",
    "    3. Only one sub-population is suited to the new environment\n",
    "    4. The sub-population has a higher probability of reproduction\n",
    "    5. The sub-population spreads in the environment\n",
    "\n",
    "![evolutionary algorithms process](imgs/move_37_evolutionary_algorithms_process.jpg)\n",
    "\n",
    "<br/>\n",
    "\n",
    "- Intra-Life Learning: A process of evolution via natural selection. E.g. Evolutionary Algorithms.\n",
    "- Inter-Life Learning: Relates to how an animal learns during its lifetime through interacting with its environment. E.g. Neural Networks.\n",
    "- Google's AmoebaNet\n",
    "\n",
    "\n",
    "**Take Aways**\n",
    "\n",
    "- Meta Learning is the process of learning to learn, where an AI optimizes one or several other AIs\n",
    "- Evolutionary algorithms use concepts from the evolutionary process like mutation and natural selection to solve complex problems\n",
    "- A Meta Learning technique called Neuroevolution uses Evolutionary Algorithms to optimize Neural Networks specifically\n",
    "\n",
    "\n",
    "**Learning Resources**\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=2z0ofe2lpz4)\n",
    "- [Code Link: Neural Network Genetic Algorithm](https://github.com/harvitronix/neural-network-genetic-algorithm)\n",
    "- [UTexas: Tutorial on Evolution of Neural Networks (2013)](http://nn.cs.utexas.edu/?neuroevolution-tutorial-ijcnn2013)\n",
    "- [Medium: Let's evolve a neural network with a genetic algorithm](https://blog.coast.ai/lets-evolve-a-neural-network-with-a-genetic-algorithm-code-included-8809bece164)\n",
    "- [Medium: Paper Repro: Deep Neuroevolution](https://towardsdatascience.com/paper-repro-deep-neuroevolution-756871e00a66)\n",
    "- [Youtube: Introduction to Neuroevolution - The Nature of Code](https://www.youtube.com/watch?v=lu5ul7z4icQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Search Algorithms\n",
    "\n",
    "[Quora: What's the difference between Policy Iteration and Policy Search?](https://www.quora.com/Whats-the-difference-between-policy-iteration-and-policy-search)\n",
    "\n",
    "Policy search refers to methods that directly learn the policy for solving a Markov Decision Process (MDP). Policy Gradient methods are a subset of this wide class of algorithms.\n",
    "\n",
    "Q-Learning, Fitted-Q, LSTD-Q...etc. are examples of action-search algorithms. They find the optimal policy by maximizing individual actions for every state (or feature vector if we consider value function approximation).\n",
    "\n",
    "This is different from Policy Search algorithms, where one searches the policy space directly.\n",
    "\n",
    "Policy Gradient methods learn the value function and use these values to learn a policy for a given MDP, hence why they are considered a subclass of Policy Search Algorithms. Actor-Critic methods is another subclass of Policy Search Algorithms which is related to Policy Gradient, with the only difference being that we reduce variance by comparing policies based on an estimate of the value function.\n",
    "\n",
    "Global Optimization Algorithms are another subclass of Policy Search Algorithms - the Cross Entropy method for optimization or Optimistic Optimization methods.\n",
    "\n",
    "\n",
    "**Difference between Policy Search and Policy Iteration:**\n",
    "\n",
    "Policy Search seeks the optimal policy in the policy space.\n",
    "- A policy $\\pi\\ :\\ S \\rightarrow A$ is a mapping from states to actions\n",
    "- If state and action sets are finite, there are finite number of possible policies and the optimal policy relies somewhere within this set\n",
    "- If the state-action sets are continuous (e.g. subsets of vector spaces), then the optimal policy may lie in an infinite dimensional vector space\n",
    "- The common approach is to parameterize the policy with some parameter $w \\in W$, so that the search is performed over the parameter space $W$, i.e. a subset of the finite dimensional vector space.\n",
    "- In either discrete of continuous policies, we can formulate the search as an optimization problem or use any optimization algorithm to obtain a good local optimum that maximizes the return:\n",
    "    - Monte Carlo tree search\n",
    "    - Cross-Entropy\n",
    "    - Genetic Algorithms\n",
    "    - Gradient Descent\n",
    "- Policy Gradient: gradient ascent in the Policy Space\n",
    "- Expectation-Maximization: Consider a Bayesian framework where the optimal policy is a latent random variable that has to be estimated as a maximum-likelihood problem\n",
    "\n",
    "Policy Iteration and Policy Search are **not** exclusive categories\n",
    "- If PS is considered an optimization problem, we need to define the objective of the problem (some function of the policy that we aim to optimize). Value function is an interesting candidate for the objective, since it can be estimated from samples (e.g. TD methods) with less variance than by averaging the sample return.\n",
    "- This observation led to _actor-critic_ methods where the _critic_ estimates the value for the current policy (policy evaluation) and the _actor_ uses this value to guide the policy gradient (policy improvement). From this point of view, _actor-critic_ methods are usually considered both as Policy Search and as approximate Policy Iteration algorithms.\n",
    "\n",
    "\n",
    "**Why use Policy-Based Methods?**\n",
    "\n",
    "- Policy can be either deterministic or stochastic\n",
    "- Deterministic Policy: maps state to actions. If given a state, the function returns an action to take\n",
    "- Deterministic policies are used in deterministic environments where the actions taken determine the outcome. There is no uncertainty\n",
    "- Stochastic Policy outputs a probability distribution over actions. Instead of being sure of taking an action, there is a probability that we'll take a different one. This is used when the environment is uncertain. This process is called a Partially Observable Markov Decision Process (POMDP)\n",
    "- Most of the time, we use the second type of policy\n",
    "\n",
    "**Advantages**\n",
    "\n",
    "- **Convergence**: Policy based methods have better convergence properties\n",
    "- Policy gradients are more effective in high dimensional action spaces\n",
    "- Policy gradients can learn stochastic policies\n",
    "\n",
    "**Disadvantages**\n",
    "\n",
    "- 1 one disadvantage: Most of the time, they converge on a local maximum rather than on the global optimum\n",
    "- Instead of Deep Q-Learning, which always tries to reach the maximum, policy gradients converge slower, step by step. They can take longer to train.\n",
    "\n",
    "\n",
    "**Two sub divisions of Policy Search Methods**\n",
    "\n",
    "_Model-Free Policy Search Methods_\n",
    "\n",
    "- Policy Gradients\n",
    "- Likelihood Gradients: REINFORCE [Williams, 1992], PGPE [Rückstiesset al, 2009]\n",
    "- Natural Gradients: episodic Natural Actor Critic (eNAC), [Peters & Schaal, 2006]\n",
    "- Weighted Maximum Likelihood Approaches\n",
    "- Success-Matching Principle [Kober & Peters, 2006]\n",
    "- Information Theoretic Methods [Daniel, Neumann & Peters, 2012]\n",
    "- Extensions: Contextual and Hierarchical Policy Search\n",
    "\n",
    "_Model-Based Policy Search Methods_\n",
    "\n",
    "- Greedy Updates: PILCO [Deisenroth& Rasmussen, 2011]  \n",
    "- Bounded Updates:\n",
    "    - Model-Based REPS [Peters at al., 2010]\n",
    "    - Guided Policy Search by Trajectory Optimization [Levine & Koltun, 2010] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms Study Guide\n",
    "\n",
    "[Evolutionary Algorithms - Study Guide](https://www.theschool.ai/wp-content/uploads/2018/10/Evolutionary-Algorithms-Study-Guide.pdf)\n",
    "\n",
    "\n",
    "**What are Evolutionary algorithms?**\n",
    "\n",
    "- A style of optimization inspired by the study of genetics and evolution\n",
    "    - A population of solutions are proposed for a given problem\n",
    "    - The best solutions are held aside, and the rest are removed\n",
    "    - By introducing change through crossever and/or mutation, the algorithm explores the search space\n",
    "    - If no new genetic material via mutation is introduced, the process tends to stagnate due to a limited gene pool\n",
    "\n",
    "\n",
    "**What kinds of Evolutionary Algorithms are out there?\n",
    "\n",
    "- **Genetic Algorithm**: formulate a string of numbers (traditionally binary) to represent a genome and iteratively improve based on a fitness function in order to solve a problem\n",
    "- **Evolution Strategies**: Genetic algorithm with a vector of real numbers\n",
    "- **Genetic programming**: Genetic algorithms applied to generating programs\n",
    "- **Neuroevolution**: Genetic programming applied to neural networks\n",
    "\n",
    "\n",
    "**What are the steps in an Evolutionary Algorithm?**\n",
    "\n",
    "1. **Initialization**: create initial population of solutions\n",
    "2. **Selection**: evaluate members based on a fitness function\n",
    "3. **Genetic Operators**: ('A' or 'A and B')\n",
    "    A) mutation: vary the genes based on random noise\n",
    "    B) crossover: swap genes between successful members of the population\n",
    "    Repeat steps 2 and 3 until...\n",
    "4. **Termination**: End after reaching max runtime or a threshold of performance\n",
    "\n",
    "\n",
    "**What are the advantages of Evolutionary Algorithms over other methods?**\n",
    "\n",
    "- They cover a large search space\n",
    "- Highly creative approaches\n",
    "- Doesn't require a gradient\n",
    "\n",
    "\n",
    "**What kinds of problems are Evolution Algorithms suited for?**\n",
    "\n",
    "- Problems with large search space for solutions\n",
    "- Problems where you can't calculate a gradient\n",
    "- Black box engineering: problems where you don't have a very informative model\n",
    "- Quantum computing: designing quantum algorithms can be counter-intuitive\n",
    "\n",
    "\n",
    "**How can Evolutionary Algorithms be combined with neural networks?**\n",
    "\n",
    "- The weights can be trained with an evolutionary algorithm (conventional neuroevolution)\n",
    "- Neural Architecture search: can be used to find optimal Neural Net architectures\n",
    "- Use a Neural Network to get the features and evolution to get the policy\n",
    "\n",
    "\n",
    "**Further Information**\n",
    "\n",
    "- [Introduction to Evolutionary Algorithms](https://towardsdatascience.com/introduction-to-evolutionary-algorithms-a8594b484ac)\n",
    "- [An introduction to Evolutionary Algorithms - Dr. Shahin Rostami](https://www.youtube.com/watch?v=L--IxUH4fac)\n",
    "- [Multi-Objective Problems - Dr. Shahin Rostami](https://www.youtube.com/watch?v=56JOMkPvoKs)\n",
    "- [What exactly are genetic algorithms and what sort of problems are they good for?](https://ai.stackexchange.com/questions/240/what-exactly-are-genetic-algorithms-and-what-sort-of-problems-are-they-good-for)\n",
    "- [Evolutionary algorithms: A critical review and its future prospects (2016, pay-wall)](https://ieeexplore.ieee.org/document/7955308?reload=true)\n",
    "- [Evolutionary-Neural Hybrid Agents for Architecture Search (2019, under review)](https://openreview.net/pdf?id=S1eBzhRqK7)\n",
    "\n",
    "\n",
    "**Researchers**\n",
    "\n",
    "- [Jeff Clune](https://www.youtube.com/watch?v=eCy-vUXXF_g)\n",
    "- [Kenneth Stanley](https://www.youtube.com/watch?v=XWUsl24zYOU)\n",
    "- [Wolfgang Banzhaf](https://www.youtube.com/watch?v=tj5-H6ECxyM)\n",
    "- [Lee Spector](https://www.youtube.com/watch?v=UWoWBiMowLI)\n",
    "- [Publications of Dr. A.E. Eiben](https://www.cs.vu.nl/~gusz/index.php/my-publications/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolutionary Algorithms Quiz\n",
    "\n",
    "Let’s **evolve** our knowledge of these algorithms to the next level.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Suppose that we have a neural network and want to train the weights with an evolutionary algorithm. Which of the following terms best describes this strategy?\n",
    "\n",
    "    - [ ] Genetic algorithm\n",
    "    - [x] Neuroevolution\n",
    "    - [ ] Evolution Strategies\n",
    "    - [ ] Genetic programming\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "For a better understanding of Neuroevolution, familiarize yourself with the work of [Jeff Clune](https://www.youtube.com/watch?v=eCy-vUXXF_g)\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Which of the following can be omitted from our algorithm, while still allowing us to find a reasonable answer?\n",
    "\n",
    "    - [ ] Fitness function\n",
    "    - [x] Crossover\n",
    "    - [ ] Mutation\n",
    "    - [ ] Termination\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "A fitness function is needed in order to 'achieve' a goal. That's because we can't achieve a goal unless we have some kind of metric for that goal. Mutation is needed in order to prevent our algorithm from stagnating due to a limited gene pool. Termination just means we need the algorithm to stop eventually and give us an answer. Although crossover is very useful for evolution, it is not strictly necessary. The biological equivalent of this scenario is known as 'a-sexual reproduction'.\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Which of the following is not an advantage of evolutionary algorithms?\n",
    "\n",
    "    - [x] They are very resilient to overfitting\n",
    "    - [ ] They can cover a large search space\n",
    "    - [ ] They tend to produce highly creative approaches\n",
    "    - [ ] They don’t require a gradient\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "[Stack Overflow: How to avoid overfitting with Genertic Algorithm](https://stackoverflow.com/questions/27764825/how-to-avoid-overfitting-with-genetic-algorithm)\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Which of the following statements about evolutionary algorithms is false?\n",
    "\n",
    "    - [ ] EA can be applied to designing quantum algorithms\n",
    "    - [x] EA requires a gradient\n",
    "    - [ ] EA does not require a well defined model\n",
    "    - [ ] EA can stagnate if new genetic material is not introduced\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "EA does NOT require a gradient.\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Which of the following statements is true? In conventional neuroevolution…\n",
    "\n",
    "    - [ ] ...we evolve the optimal architecture of a neural network\n",
    "    - [ ] ...we use a neural network to get the features and evolution to get the policy\n",
    "    - [ ] ...we evolve the initial weights of a neural network\n",
    "    - [x] ...we train the weights of a neural network by evolving them\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "For a better understanding of Neuroevolution, familiarize yourself with the work of [Jeff Clune](https://www.youtube.com/watch?v=eCy-vUXXF_g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment (Neuroevolution)\n",
    "\n",
    "This weeks homework assignment is to design a neuro-evolution algorithm  that will learn how to optimally play any one of the OpenAI Gym environments. Here is an [example notebook](https://github.com/ikergarcia1996/NeuroEvolution-Flappy-Bird) to get started. If your agent can learn how to play the game using both neural networks and an evolutionary algorithm, you’ll successfully complete the assignment. Good luck!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Control Theory\n",
    "\n",
    "\n",
    "**Video Description:**\n",
    "\n",
    "Boston Dynamics released yet another incredible video of its bipedal humanoid robot, this time performing parkour by jumping on a series of boxes. In this video, I'll explain how it works at both a hardware and software level. Their real value lies in the specific type of software they are using, we don't know for sure what it is but we can take some educated guesses based on a combination of whats been revealed so far and what's worked in other humanoid robots. Prepare yourselves for some mechanical engineering and control theory Wizards, enjoy!\n",
    "\n",
    "\n",
    "**Take Aways**\n",
    "\n",
    "- Boston Dynamics is powered using a very efficient Hydraulic Power Unit, which converts Mechanical Energy into Hydraulic Energy\n",
    "- The Zero Moment Point Algorithm is used to help Bipedal Robots walk, run, and jump by computing a stabilized trajectory for the robot\n",
    "- Boston Dynamics likely uses a stack of different Dynamic Control Algorithms to help its robot perform motions\n",
    "\n",
    "\n",
    "**Learning Resources**\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=lXZ6y3lMymM)\n",
    "- [Code Link: Boston Dynamics Atlas Explained](https://github.com/llSourcell/Boston_Dynamics_Atlas_Explained)\n",
    "- [Bostn Dynamics Atlas](https://www.bostondynamics.com/atlas)\n",
    "- [PDF: CMU Human-Supervised Control of the ATLAS Humanoid Robot for Traversing Doors](https://www.cs.cmu.edu/~cga/drc/door-submitted.pdf)\n",
    "- [BDI Atlas Robot Interface 3.0.0](http://gazebosim.org/tutorials?tut=drcsim_atlas_robot_interface&branch=issue_24_atlas_robot_interface_drcsim_4)\n",
    "- [PDF: Introduction to Control Systems](http://www.ent.mrt.ac.lk/~rohan/teaching/EN5001/Reading/DORFCH1.pdf)\n",
    "- [PDF: Introduction to Robotics](http://engineering.nyu.edu/mechatronics/smart/Archive/intro_to_rob/Intro2Robotics.pdf)\n",
    "- [Youtube: Parkour Atlas](https://www.youtube.com/watch?v=LikxFZZO2sk)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
