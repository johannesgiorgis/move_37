{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment 8 - Monte Carlo Policy Gradients <a class=\"tocSkip\">\n",
    "\n",
    "See Thomas Simonini’s example [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb): replicate it but for a different environment — Lunar Lander!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Mountain-Car:-REINFORCE-Monte-Carlo-Policy-Gradients\" data-toc-modified-id=\"Mountain-Car:-REINFORCE-Monte-Carlo-Policy-Gradients-1\">Mountain Car: REINFORCE Monte Carlo Policy Gradients</a></span></li><li><span><a href=\"#This-is-a-notebook-from-Deep-Reinforcement-Learning-Course-with-Tensorflow\" data-toc-modified-id=\"This-is-a-notebook-from-Deep-Reinforcement-Learning-Course-with-Tensorflow-2\">This is a notebook from <a href=\"https://simoninithomas.github.io/Deep_reinforcement_learning_Course/\" target=\"_blank\">Deep Reinforcement Learning Course with Tensorflow</a></a></span><ul class=\"toc-item\"><li><span><a href=\"#Step-1:-Import-the-libraries\" data-toc-modified-id=\"Step-1:-Import-the-libraries-2.1\">Step 1: Import the libraries</a></span></li><li><span><a href=\"#Step-2:-Create-our-environment\" data-toc-modified-id=\"Step-2:-Create-our-environment-2.2\">Step 2: Create our environment</a></span></li><li><span><a href=\"#Step-3:-Set-up-our-hyperparameters\" data-toc-modified-id=\"Step-3:-Set-up-our-hyperparameters-2.3\">Step 3: Set up our hyperparameters</a></span></li><li><span><a href=\"#Step-4-:-Define-the-preprocessing-functions️\" data-toc-modified-id=\"Step-4-:-Define-the-preprocessing-functions️-2.4\">Step 4 : Define the preprocessing functions️</a></span></li><li><span><a href=\"#Step-5:-Create-our-Policy-Gradient-Neural-Network-model\" data-toc-modified-id=\"Step-5:-Create-our-Policy-Gradient-Neural-Network-model-2.5\">Step 5: Create our Policy Gradient Neural Network model</a></span></li><li><span><a href=\"#Step-6:-Set-up-Tensorboard\" data-toc-modified-id=\"Step-6:-Set-up-Tensorboard-2.6\">Step 6: Set up Tensorboard</a></span></li><li><span><a href=\"#Step-7:-Train-our-Agent\" data-toc-modified-id=\"Step-7:-Train-our-Agent-2.7\">Step 7: Train our Agent</a></span></li><li><span><a href=\"#Step-8:-Evaluate-our-trained-model\" data-toc-modified-id=\"Step-8:-Evaluate-our-trained-model-2.8\">Step 8: Evaluate our trained model</a></span></li></ul></li><li><span><a href=\"#Report\" data-toc-modified-id=\"Report-3\">Report</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mountain Car: REINFORCE Monte Carlo Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we'll implement an agent that plays **[MountainCar-v0](https://gym.openai.com/envs/MountainCar-v0/)**\n",
    "\n",
    "<br/>\n",
    "\n",
    "<video controls src=\"../assets/mountain_car.mp4\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This is a notebook from [Deep Reinforcement Learning Course with Tensorflow](https://simoninithomas.github.io/Deep_reinforcement_learning_Course/)\n",
    "\n",
    "The original notebook, with a solution for CartPole is [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Create our environment\n",
    "This time we use <a href=\"https://gym.openai.com/\">OpenAI Gym</a> which has a lot of great environments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARN: gym.spaces.Box autodetected dtype as <class 'numpy.float32'>. Please provide explicit dtype.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# env_name = 'CartPole-v0'\n",
    "env_name = 'MountainCar-v0'\n",
    "# env = gym.make('MountainCar-v0')\n",
    "env = gym.make(env_name)\n",
    "env = env.unwrapped\n",
    "# Policy gradient has high variance, seed for reproducability\n",
    "env.seed(1);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Set up our hyperparameters "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "## ENVIRONMENT Hyperparameters\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "# print(action_size, state_size)\n",
    "## TRAINING Hyperparameters\n",
    "max_episodes = 300\n",
    "learning_rate = 0.01\n",
    "STEP_MULTIPLE = 3.0\n",
    "gamma = 0.95 # Discount rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4 : Define the preprocessing functions️\n",
    "This function takes <b>the rewards and perform discounting.</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discount_and_normalize_rewards(episode_rewards):\n",
    "    discounted_episode_rewards = np.zeros_like(episode_rewards)\n",
    "    cumulative = 0.0\n",
    "    for i in reversed(range(len(episode_rewards))):\n",
    "        cumulative = cumulative * gamma + episode_rewards[i]\n",
    "        discounted_episode_rewards[i] = cumulative\n",
    "    \n",
    "    mean = np.mean(discounted_episode_rewards)\n",
    "    std = np.std(discounted_episode_rewards)\n",
    "    discounted_episode_rewards = (discounted_episode_rewards - mean) / (std)\n",
    "    \n",
    "    return discounted_episode_rewards"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Create our Policy Gradient Neural Network model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea is simple:\n",
    "- Our state which is an array of 2 values, **position** and **velocity**, which will be used as an input.\n",
    "- Our NN is 3 fully connected layers.\n",
    "- Our output activation function is **softmax** that squashes the outputs to a probability distribution:\n",
    "    - for instance: $ softmax(4,\\ 2,\\ 6) \\rightarrow (0.117,\\ 0.016,\\ 0.867) $\n",
    "\n",
    "<br/>\n",
    "\n",
    "<img src=\"../assets/mountain_car.jpeg\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tf.device(\"/device:GPU:1\"):\n",
    "    with tf.name_scope(\"inputs\"):\n",
    "        input_ = tf.placeholder(tf.float32, [None, state_size], name=\"input_\")\n",
    "        actions = tf.placeholder(tf.int32, [None, action_size], name=\"actions\")\n",
    "        discounted_episode_rewards_ = tf.placeholder(\n",
    "            tf.float32, [None,], name=\"discounted_episode_rewards\")\n",
    "\n",
    "        # Add this placeholder for having this variable in tensorboard\n",
    "        mean_reward_ = tf.placeholder(tf.float32 , name=\"mean_reward\")\n",
    "\n",
    "        with tf.name_scope(\"fc1\"):\n",
    "            fc1 = tf.contrib.layers.fully_connected(\n",
    "                inputs = input_, num_outputs = 10,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer()) \n",
    "        with tf.name_scope(\"fc2\"):\n",
    "            fc2 = tf.contrib.layers.fully_connected(\n",
    "                inputs = fc1, num_outputs = action_size,\n",
    "                activation_fn=tf.nn.relu,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        with tf.name_scope(\"fc3\"):\n",
    "            fc3 = tf.contrib.layers.fully_connected(\n",
    "                inputs = fc2, num_outputs = action_size,\n",
    "                activation_fn= None,\n",
    "                weights_initializer=tf.contrib.layers.xavier_initializer()) \n",
    "            \n",
    "        with tf.name_scope(\"softmax\"):\n",
    "            action_distribution = tf.nn.softmax(fc3)\n",
    "\n",
    "        with tf.name_scope(\"loss\"):\n",
    "            # tf.nn.softmax_cross_entropy_with_logits computes the cross entropy \n",
    "            # of the result after applying the softmax function\n",
    "            # If you have single-class labels, where an object can only belong to one class,\n",
    "            # you might now consider using tf.nn.sparse_softmax_cross_entropy_with_logits \n",
    "            # so that you don't have to convert your labels to a dense one-hot array. \n",
    "            neg_log_prob = tf.nn.softmax_cross_entropy_with_logits_v2(\n",
    "                logits = fc3, labels = actions)\n",
    "            loss = tf.reduce_mean(neg_log_prob * discounted_episode_rewards_) \n",
    "\n",
    "\n",
    "        with tf.name_scope(\"train\"):\n",
    "            train_opt = tf.train.AdamOptimizer(learning_rate).minimize(loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Set up Tensorboard\n",
    "For more information about tensorboard, please watch this <a href=\"https://www.youtube.com/embed/eBbEDRsCmv4\">excellent 30min tutorial</a> <br><br>\n",
    "To launch tensorboard : `tensorboard --logdir=/tensorboard/pg/1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup TensorBoard Writer\n",
    "# writer = tf.summary.FileWriter(\"/tensorboard/pg/1\")\n",
    "!rm -Rf ./tensorboard\n",
    "writer = tf.summary.FileWriter(\"./tensorboard/pg/1\")\n",
    "\n",
    "## Losses\n",
    "tf.summary.scalar(\"Loss\", loss)\n",
    "\n",
    "## Reward mean\n",
    "tf.summary.scalar(\"Reward_mean\", mean_reward_)\n",
    "\n",
    "write_op = tf.summary.merge_all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Train our Agent "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Create the NN\n",
    "maxReward = 0 # Keep track of maximum reward\n",
    "For episode in range(max_episodes):\n",
    "    episode + 1\n",
    "    reset environment\n",
    "    reset stores (states, actions, rewards)\n",
    "    \n",
    "    For each step:\n",
    "        Choose action a\n",
    "        Perform action a\n",
    "        Store s, a, r\n",
    "        If done:\n",
    "            Calculate sum reward\n",
    "            Calculate gamma Gt\n",
    "            Optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "The starting state is : [-0.43852191  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "Step: 6000\n",
      "Step: 7000\n",
      "Step: 8000\n",
      "Step: 9000\n",
      "Step: 10000\n",
      "Step: 11000\n",
      "Step: 12000\n",
      "Step: 13000\n",
      "Step: 14000\n",
      "Step: 15000\n",
      "==========================================\n",
      "Total Time: 0:00:12.609279\n",
      "Step max: 20000\n",
      "Fail: False\n",
      "Episode:  0\n",
      "Reward:  -15466.0\n",
      "Mean Reward -15466.0\n",
      "Max reward so far:  -15466.0\n",
      "Model saved\n",
      "==========================================\n",
      "The starting state is : [-0.49709999  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "Step: 6000\n",
      "Step: 7000\n",
      "Step: 8000\n",
      "Step: 9000\n",
      "Step: 10000\n",
      "Step: 11000\n",
      "Step: 12000\n",
      "Step: 13000\n",
      "Step: 14000\n",
      "Step: 15000\n",
      "Step: 16000\n",
      "Step: 17000\n",
      "Step: 18000\n",
      "Step: 19000\n",
      "Step: 20000\n",
      "==========================================\n",
      "Total Time: 0:00:15.978589\n",
      "Step max: 20000\n",
      "Fail: True\n",
      "Episode:  1\n",
      "Reward:  -20488.0\n",
      "Mean Reward -17977.0\n",
      "Max reward so far:  -15466.0\n",
      "==========================================\n",
      "The starting state is : [-0.56177637  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "Step: 6000\n",
      "Step: 7000\n",
      "Step: 8000\n",
      "Step: 9000\n",
      "Step: 10000\n",
      "Step: 11000\n",
      "Step: 12000\n",
      "Step: 13000\n",
      "Step: 14000\n",
      "Step: 15000\n",
      "Step: 16000\n",
      "Step: 17000\n",
      "Step: 18000\n",
      "Step: 19000\n",
      "Step: 20000\n",
      "==========================================\n",
      "Total Time: 0:00:16.158884\n",
      "Step max: 20000\n",
      "Fail: True\n",
      "Episode:  2\n",
      "Reward:  -20636.0\n",
      "Mean Reward -18863.333333333332\n",
      "Max reward so far:  -15466.0\n",
      "==========================================\n",
      "The starting state is : [-0.56262504  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "Step: 6000\n",
      "Step: 7000\n",
      "Step: 8000\n",
      "Step: 9000\n",
      "Step: 10000\n",
      "Step: 11000\n",
      "Step: 12000\n",
      "Step: 13000\n",
      "Step: 14000\n",
      "Step: 15000\n",
      "Step: 16000\n",
      "Step: 17000\n",
      "Step: 18000\n",
      "Step: 19000\n",
      "Step: 20000\n",
      "==========================================\n",
      "Total Time: 0:00:16.047869\n",
      "Step max: 20000\n",
      "Fail: True\n",
      "Episode:  3\n",
      "Reward:  -20586.0\n",
      "Mean Reward -19294.0\n",
      "Max reward so far:  -15466.0\n",
      "==========================================\n",
      "The starting state is : [-0.44549567  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "Step: 5000\n",
      "Step: 6000\n",
      "Step: 7000\n",
      "Step: 8000\n",
      "Step: 9000\n",
      "Step: 10000\n",
      "Step: 11000\n",
      "Step: 12000\n",
      "Step: 13000\n",
      "Step: 14000\n",
      "Step: 15000\n",
      "Step: 16000\n",
      "Step: 17000\n",
      "Step: 18000\n",
      "Step: 19000\n",
      "Step: 20000\n",
      "==========================================\n",
      "Total Time: 0:00:16.328225\n",
      "Step max: 20000\n",
      "Fail: True\n",
      "Episode:  4\n",
      "Reward:  -20560.0\n",
      "Mean Reward -19547.2\n",
      "Max reward so far:  -15466.0\n",
      "==========================================\n",
      "The starting state is : [-0.58963442  0.        ]\n",
      "Step: 1000\n",
      "The new step_max is : 4905.0\n",
      "==========================================\n",
      "Total Time: 0:00:01.392595\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  5\n",
      "Reward:  -1393.0\n",
      "Mean Reward -16521.5\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58609476  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.987374\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  6\n",
      "Reward:  -4689.0\n",
      "Mean Reward -14831.142857142857\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.49696498  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.061989\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  7\n",
      "Reward:  -4773.0\n",
      "Mean Reward -13573.875\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.46229127  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.012911\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  8\n",
      "Reward:  -4999.0\n",
      "Mean Reward -12621.111111111111\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.50807438  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.080947\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  9\n",
      "Reward:  -4893.0\n",
      "Mean Reward -11848.3\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.41959775  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.051693\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  10\n",
      "Reward:  -4887.0\n",
      "Mean Reward -11215.454545454546\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.44494651  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.164090\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  11\n",
      "Reward:  -4871.0\n",
      "Mean Reward -10686.75\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.56585521  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.221914\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  12\n",
      "Reward:  -5015.0\n",
      "Mean Reward -10250.461538461539\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.59424642  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.994300\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  13\n",
      "Reward:  -4913.0\n",
      "Mean Reward -9869.214285714286\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.42209713  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.028975\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  14\n",
      "Reward:  -4873.0\n",
      "Mean Reward -9536.133333333333\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.51723819  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.929008\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  15\n",
      "Reward:  -4653.0\n",
      "Mean Reward -9230.9375\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.5057014  0.       ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.786721\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  16\n",
      "Reward:  -4656.0\n",
      "Mean Reward -8961.823529411764\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.49826861  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.960709\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  17\n",
      "Reward:  -4843.0\n",
      "Mean Reward -8733.0\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43523978  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.904729\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  18\n",
      "Reward:  -4937.0\n",
      "Mean Reward -8533.21052631579\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43473911  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.242522\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  19\n",
      "Reward:  -4757.0\n",
      "Mean Reward -8344.4\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.48338127  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.076199\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  20\n",
      "Reward:  -4777.0\n",
      "Mean Reward -8174.523809523809\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.59068543  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.036404\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  21\n",
      "Reward:  -4717.0\n",
      "Mean Reward -8017.363636363636\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43856025  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.154285\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  22\n",
      "Reward:  -4749.0\n",
      "Mean Reward -7875.260869565217\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43618024  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.913585\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  23\n",
      "Reward:  -3136.0\n",
      "Mean Reward -7677.791666666667\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58680719  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.951491\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  24\n",
      "Reward:  -4799.0\n",
      "Mean Reward -7562.64\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.47480625  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:01.709857\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  25\n",
      "Reward:  -1880.0\n",
      "Mean Reward -7344.076923076923\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.55947751  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.115505\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  26\n",
      "Reward:  -4751.0\n",
      "Mean Reward -7248.037037037037\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.44888659  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.106099\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  27\n",
      "Reward:  -4933.0\n",
      "Mean Reward -7165.357142857143\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.44773308  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.160237\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  28\n",
      "Reward:  -4755.0\n",
      "Mean Reward -7082.241379310345\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.46429472  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.064772\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  29\n",
      "Reward:  -4739.0\n",
      "Mean Reward -7004.133333333333\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.40214002  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.972765\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  30\n",
      "Reward:  -4943.0\n",
      "Mean Reward -6937.645161290323\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.44331105  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.595916\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  31\n",
      "Reward:  -4205.0\n",
      "Mean Reward -6852.25\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.56581547  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.064305\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  32\n",
      "Reward:  -4963.0\n",
      "Mean Reward -6795.0\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.53840909  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.561743\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  33\n",
      "Reward:  -2897.0\n",
      "Mean Reward -6680.35294117647\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.48106451  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.835761\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  34\n",
      "Reward:  -3488.0\n",
      "Mean Reward -6589.142857142857\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43394749  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.280676\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  35\n",
      "Reward:  -3845.0\n",
      "Mean Reward -6512.916666666667\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.45980061  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.408842\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  36\n",
      "Reward:  -4122.0\n",
      "Mean Reward -6448.2972972972975\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.56417498  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:03.056289\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  37\n",
      "Reward:  -3589.0\n",
      "Mean Reward -6373.0526315789475\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43181159  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.098609\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  38\n",
      "Reward:  -4803.0\n",
      "Mean Reward -6332.794871794872\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.4707206  0.       ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.077202\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  39\n",
      "Reward:  -4839.0\n",
      "Mean Reward -6295.45\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58002233  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.705544\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  40\n",
      "Reward:  -4382.0\n",
      "Mean Reward -6248.780487804878\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.52399641  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.022328\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  41\n",
      "Reward:  -4815.0\n",
      "Mean Reward -6214.642857142857\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.55029596  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.079955\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  42\n",
      "Reward:  -4741.0\n",
      "Mean Reward -6180.372093023256\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.54056967  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:02.053958\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  43\n",
      "Reward:  -2309.0\n",
      "Mean Reward -6092.386363636364\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.49278466  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:03.017538\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  44\n",
      "Reward:  -3428.0\n",
      "Mean Reward -6033.177777777778\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58852754  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:01.607297\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  45\n",
      "Reward:  -1864.0\n",
      "Mean Reward -5942.54347826087\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58679356  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.055904\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  46\n",
      "Reward:  -4975.0\n",
      "Mean Reward -5921.95744680851\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.52621903  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.072598\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  47\n",
      "Reward:  -4883.0\n",
      "Mean Reward -5900.3125\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.54119598  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.622706\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  48\n",
      "Reward:  -1738.0\n",
      "Mean Reward -5815.367346938776\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.47937539  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:01.719199\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  49\n",
      "Reward:  -1956.0\n",
      "Mean Reward -5738.18\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.56101593  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.630490\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  50\n",
      "Reward:  -4189.0\n",
      "Mean Reward -5707.803921568628\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.43464614  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.959416\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  51\n",
      "Reward:  -3399.0\n",
      "Mean Reward -5663.403846153846\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.4386806  0.       ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.672737\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  52\n",
      "Reward:  -4455.0\n",
      "Mean Reward -5640.603773584906\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.46175162  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.662484\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  53\n",
      "Reward:  -3011.0\n",
      "Mean Reward -5591.907407407408\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.5531364  0.       ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.424026\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  54\n",
      "Reward:  -3914.0\n",
      "Mean Reward -5561.4\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.59615709  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.375669\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  55\n",
      "Reward:  -3894.0\n",
      "Mean Reward -5531.625\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.5134401  0.       ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.625523\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  56\n",
      "Reward:  -3111.0\n",
      "Mean Reward -5489.1578947368425\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.57819141  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:04.091142\n",
      "Step max: 4905.0\n",
      "Fail: True\n",
      "Episode:  57\n",
      "Reward:  -4913.0\n",
      "Mean Reward -5479.224137931034\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.48640294  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.653273\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  58\n",
      "Reward:  -2986.0\n",
      "Mean Reward -5436.966101694915\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.41886629  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.463091\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  59\n",
      "Reward:  -1666.0\n",
      "Mean Reward -5374.116666666667\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.49407592  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "Step: 4000\n",
      "==========================================\n",
      "Total Time: 0:00:03.374927\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  60\n",
      "Reward:  -3964.0\n",
      "Mean Reward -5351.0\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.40316309  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:02.497681\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  61\n",
      "Reward:  -2848.0\n",
      "Mean Reward -5310.629032258064\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.4367252  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.384969\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  62\n",
      "Reward:  -1544.0\n",
      "Mean Reward -5250.841269841269\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.41205373  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.563078\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  63\n",
      "Reward:  -2961.0\n",
      "Mean Reward -5215.0625\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.49459813  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.503659\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  64\n",
      "Reward:  -1746.0\n",
      "Mean Reward -5161.692307692308\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.52665702  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:01.911905\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  65\n",
      "Reward:  -2142.0\n",
      "Mean Reward -5115.939393939394\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.47148952  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.495048\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  66\n",
      "Reward:  -1709.0\n",
      "Mean Reward -5065.089552238806\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.58930207  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.589246\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  67\n",
      "Reward:  -2973.0\n",
      "Mean Reward -5034.323529411765\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.53910871  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.403195\n",
      "Step max: 4905.0\n",
      "Fail: False\n",
      "Episode:  68\n",
      "Reward:  -1497.0\n",
      "Mean Reward -4983.057971014493\n",
      "Max reward so far:  -1393.0\n",
      "==========================================\n",
      "The starting state is : [-0.59131773  0.        ]\n",
      "Step: 1000\n",
      "The new step_max is : 3963.0\n",
      "==========================================\n",
      "Total Time: 0:00:01.084750\n",
      "Step max: 3963.0\n",
      "Fail: False\n",
      "Episode:  69\n",
      "Reward:  -1173.0\n",
      "Mean Reward -4928.628571428571\n",
      "Max reward so far:  -1173.0\n",
      "==========================================\n",
      "The starting state is : [-0.55551468  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:02.351837\n",
      "Step max: 3963.0\n",
      "Fail: False\n",
      "Episode:  70\n",
      "Reward:  -2839.0\n",
      "Mean Reward -4899.197183098591\n",
      "Max reward so far:  -1173.0\n",
      "==========================================\n",
      "The starting state is : [-0.47807128  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:03.099509\n",
      "Step max: 3963.0\n",
      "Fail: False\n",
      "Episode:  71\n",
      "Reward:  -3558.0\n",
      "Mean Reward -4880.569444444444\n",
      "Max reward so far:  -1173.0\n",
      "==========================================\n",
      "The starting state is : [-0.46192186  0.        ]\n",
      "Step: 1000\n",
      "The new step_max is : 3339.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.870034\n",
      "Step max: 3339.0\n",
      "Fail: False\n",
      "Episode:  72\n",
      "Reward:  -933.0\n",
      "Mean Reward -4826.493150684932\n",
      "Max reward so far:  -933.0\n",
      "==========================================\n",
      "The starting state is : [-0.59372187  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.696751\n",
      "Step max: 3339.0\n",
      "Fail: True\n",
      "Episode:  73\n",
      "Reward:  -3269.0\n",
      "Mean Reward -4805.445945945946\n",
      "Max reward so far:  -933.0\n",
      "==========================================\n",
      "The starting state is : [-0.53743937  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.223847\n",
      "Step max: 3339.0\n",
      "Fail: False\n",
      "Episode:  74\n",
      "Reward:  -1231.0\n",
      "Mean Reward -4757.786666666667\n",
      "Max reward so far:  -933.0\n",
      "==========================================\n",
      "The starting state is : [-0.58652846  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.028558\n",
      "Step max: 3339.0\n",
      "Fail: False\n",
      "Episode:  75\n",
      "Reward:  -1056.0\n",
      "Mean Reward -4709.078947368421\n",
      "Max reward so far:  -933.0\n",
      "==========================================\n",
      "The starting state is : [-0.44589269  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step: 1000\n",
      "Step: 2000\n",
      "Step: 3000\n",
      "==========================================\n",
      "Total Time: 0:00:02.541368\n",
      "Step max: 3339.0\n",
      "Fail: False\n",
      "Episode:  76\n",
      "Reward:  -2967.0\n",
      "Mean Reward -4686.454545454545\n",
      "Max reward so far:  -933.0\n",
      "==========================================\n",
      "The starting state is : [-0.55060091  0.        ]\n",
      "The new step_max is : 2907.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.827583\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  77\n",
      "Reward:  -765.0\n",
      "Mean Reward -4636.179487179487\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.54791808  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.647247\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  78\n",
      "Reward:  -1823.0\n",
      "Mean Reward -4600.569620253164\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.46029927  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.169001\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  79\n",
      "Reward:  -1214.0\n",
      "Mean Reward -4558.2375\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.47634839  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.825651\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  80\n",
      "Reward:  -891.0\n",
      "Mean Reward -4512.962962962963\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.40781419  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.651236\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  81\n",
      "Reward:  -1818.0\n",
      "Mean Reward -4480.09756097561\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.57695327  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.951239\n",
      "Step max: 2907.0\n",
      "Fail: False\n",
      "Episode:  82\n",
      "Reward:  -1013.0\n",
      "Mean Reward -4438.325301204819\n",
      "Max reward so far:  -765.0\n",
      "==========================================\n",
      "The starting state is : [-0.56700321  0.        ]\n",
      "The new step_max is : 2661.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.706757\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  83\n",
      "Reward:  -687.0\n",
      "Mean Reward -4393.666666666667\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.43899274  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.107563\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  84\n",
      "Reward:  -1034.0\n",
      "Mean Reward -4354.141176470588\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.59419787  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:01.696895\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  85\n",
      "Reward:  -1805.0\n",
      "Mean Reward -4324.5\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.55419084  0.        ]\n",
      "Step: 1000\n",
      "Step: 2000\n",
      "==========================================\n",
      "Total Time: 0:00:02.320329\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  86\n",
      "Reward:  -2534.0\n",
      "Mean Reward -4303.919540229885\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.58503842  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.208776\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  87\n",
      "Reward:  -1308.0\n",
      "Mean Reward -4269.875\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.529405  0.      ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.777725\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  88\n",
      "Reward:  -770.0\n",
      "Mean Reward -4230.550561797752\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.46058981  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.487937\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  89\n",
      "Reward:  -1656.0\n",
      "Mean Reward -4201.944444444444\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.58561331  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.746847\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  90\n",
      "Reward:  -720.0\n",
      "Mean Reward -4163.681318681319\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.43395858  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.513959\n",
      "Step max: 2661.0\n",
      "Fail: False\n",
      "Episode:  91\n",
      "Reward:  -1767.0\n",
      "Mean Reward -4137.630434782609\n",
      "Max reward so far:  -687.0\n",
      "==========================================\n",
      "The starting state is : [-0.50453042  0.        ]\n",
      "The new step_max is : 2355.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.617735\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  92\n",
      "Reward:  -595.0\n",
      "Mean Reward -4099.537634408603\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.4659317  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.440625\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  93\n",
      "Reward:  -1764.0\n",
      "Mean Reward -4074.691489361702\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.41105122  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.826383\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  94\n",
      "Reward:  -853.0\n",
      "Mean Reward -4040.778947368421\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.40153211  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.730412\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  95\n",
      "Reward:  -720.0\n",
      "Mean Reward -4006.1875\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.49451918  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.337896\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  96\n",
      "Reward:  -1433.0\n",
      "Mean Reward -3979.659793814433\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.54148034  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.063501\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  97\n",
      "Reward:  -1032.0\n",
      "Mean Reward -3949.581632653061\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.42060146  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.131556\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  98\n",
      "Reward:  -1241.0\n",
      "Mean Reward -3922.222222222222\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.44980311  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.782186\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  99\n",
      "Reward:  -771.0\n",
      "Mean Reward -3890.71\n",
      "Max reward so far:  -595.0\n",
      "==========================================\n",
      "The starting state is : [-0.48024659  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.736458\n",
      "Step max: 2355.0\n",
      "Fail: False\n",
      "Episode:  100\n",
      "Reward:  -739.0\n",
      "Mean Reward -3859.5049504950493\n",
      "Max reward so far:  -595.0\n",
      "Model saved\n",
      "==========================================\n",
      "The starting state is : [-0.54189416  0.        ]\n",
      "The new step_max is : 2004.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.551245\n",
      "Step max: 2004.0\n",
      "Fail: False\n",
      "Episode:  101\n",
      "Reward:  -552.0\n",
      "Mean Reward -3827.078431372549\n",
      "Max reward so far:  -552.0\n",
      "==========================================\n",
      "The starting state is : [-0.49602622  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.747211\n",
      "Step max: 2004.0\n",
      "Fail: False\n",
      "Episode:  102\n",
      "Reward:  -694.0\n",
      "Mean Reward -3796.660194174757\n",
      "Max reward so far:  -552.0\n",
      "==========================================\n",
      "The starting state is : [-0.51139265  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.765842\n",
      "Step max: 2004.0\n",
      "Fail: False\n",
      "Episode:  103\n",
      "Reward:  -758.0\n",
      "Mean Reward -3767.4423076923076\n",
      "Max reward so far:  -552.0\n",
      "==========================================\n",
      "The starting state is : [-0.55963149  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.193131\n",
      "Step max: 2004.0\n",
      "Fail: False\n",
      "Episode:  104\n",
      "Reward:  -1110.0\n",
      "Mean Reward -3742.133333333333\n",
      "Max reward so far:  -552.0\n",
      "==========================================\n",
      "The starting state is : [-0.48330968  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new step_max is : 1995.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.578056\n",
      "Step max: 1995.0\n",
      "Fail: False\n",
      "Episode:  105\n",
      "Reward:  -489.0\n",
      "Mean Reward -3711.443396226415\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.56510815  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.207080\n",
      "Step max: 1995.0\n",
      "Fail: False\n",
      "Episode:  106\n",
      "Reward:  -1245.0\n",
      "Mean Reward -3688.392523364486\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.54550371  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.068920\n",
      "Step max: 1995.0\n",
      "Fail: False\n",
      "Episode:  107\n",
      "Reward:  -1101.0\n",
      "Mean Reward -3664.435185185185\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.58676268  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.601630\n",
      "Step max: 1995.0\n",
      "Fail: False\n",
      "Episode:  108\n",
      "Reward:  -1762.0\n",
      "Mean Reward -3646.9816513761466\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.45143215  0.        ]\n",
      "The new step_max is : 1890.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.626118\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  109\n",
      "Reward:  -500.0\n",
      "Mean Reward -3618.3727272727274\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.5412673  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.097936\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  110\n",
      "Reward:  -1173.0\n",
      "Mean Reward -3596.3423423423424\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.59890201  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.669028\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  111\n",
      "Reward:  -649.0\n",
      "Mean Reward -3570.026785714286\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.42326125  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.988360\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  112\n",
      "Reward:  -1039.0\n",
      "Mean Reward -3547.6283185840707\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.40594064  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.994182\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  113\n",
      "Reward:  -1144.0\n",
      "Mean Reward -3526.5438596491226\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.46476249  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.971192\n",
      "Step max: 1890.0\n",
      "Fail: False\n",
      "Episode:  114\n",
      "Reward:  -1033.0\n",
      "Mean Reward -3504.8608695652174\n",
      "Max reward so far:  -489.0\n",
      "==========================================\n",
      "The starting state is : [-0.48659616  0.        ]\n",
      "The new step_max is : 1638.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.443776\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  115\n",
      "Reward:  -436.0\n",
      "Mean Reward -3478.405172413793\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.48895179  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.660533\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  116\n",
      "Reward:  -640.0\n",
      "Mean Reward -3454.1452991452993\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.48344761  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.891511\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  117\n",
      "Reward:  -887.0\n",
      "Mean Reward -3432.3898305084745\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.51013126  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.333317\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  118\n",
      "Reward:  -1445.0\n",
      "Mean Reward -3415.6890756302523\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.56442655  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.712959\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  119\n",
      "Reward:  -636.0\n",
      "Mean Reward -3392.525\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.42332268  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.051016\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  120\n",
      "Reward:  -1127.0\n",
      "Mean Reward -3373.801652892562\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.46210939  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.411990\n",
      "Step max: 1638.0\n",
      "Fail: True\n",
      "Episode:  121\n",
      "Reward:  -1540.0\n",
      "Mean Reward -3358.7704918032787\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.54483743  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.658418\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  122\n",
      "Reward:  -533.0\n",
      "Mean Reward -3335.79674796748\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.47701319  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.970344\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  123\n",
      "Reward:  -886.0\n",
      "Mean Reward -3316.0403225806454\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.40718322  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.582371\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  124\n",
      "Reward:  -544.0\n",
      "Mean Reward -3293.864\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.51623031  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.387653\n",
      "Step max: 1638.0\n",
      "Fail: True\n",
      "Episode:  125\n",
      "Reward:  -1452.0\n",
      "Mean Reward -3279.246031746032\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.45020654  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.833269\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  126\n",
      "Reward:  -812.0\n",
      "Mean Reward -3259.818897637795\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.49578756  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.343746\n",
      "Step max: 1638.0\n",
      "Fail: True\n",
      "Episode:  127\n",
      "Reward:  -1512.0\n",
      "Mean Reward -3246.1640625\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.55559656  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.967985\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  128\n",
      "Reward:  -1057.0\n",
      "Mean Reward -3229.1937984496126\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.50520134  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.521190\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  129\n",
      "Reward:  -492.0\n",
      "Mean Reward -3208.1384615384613\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.5565984  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.782943\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  130\n",
      "Reward:  -745.0\n",
      "Mean Reward -3189.335877862595\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.53511011  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.264563\n",
      "Step max: 1638.0\n",
      "Fail: False\n",
      "Episode:  131\n",
      "Reward:  -1327.0\n",
      "Mean Reward -3175.2272727272725\n",
      "Max reward so far:  -436.0\n",
      "==========================================\n",
      "The starting state is : [-0.53594305  0.        ]\n",
      "The new step_max is : 1542.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.428881\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  132\n",
      "Reward:  -374.0\n",
      "Mean Reward -3154.1654135338345\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.53729253  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.848722\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  133\n",
      "Reward:  -830.0\n",
      "Mean Reward -3136.820895522388\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.58462349  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.562486\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  134\n",
      "Reward:  -562.0\n",
      "Mean Reward -3117.748148148148\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.50122239  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.271635\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  135\n",
      "Reward:  -1318.0\n",
      "Mean Reward -3104.514705882353\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.58020971  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.908136\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  136\n",
      "Reward:  -880.0\n",
      "Mean Reward -3088.2773722627735\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.55176632  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.841464\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  137\n",
      "Reward:  -781.0\n",
      "Mean Reward -3071.557971014493\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.5862043  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.354735\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  138\n",
      "Reward:  -1496.0\n",
      "Mean Reward -3060.223021582734\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.52805017  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.261457\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  139\n",
      "Reward:  -1330.0\n",
      "Mean Reward -3047.864285714286\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.45886578  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.882276\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  140\n",
      "Reward:  -964.0\n",
      "Mean Reward -3033.0851063829787\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.46601575  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.777728\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  141\n",
      "Reward:  -788.0\n",
      "Mean Reward -3017.274647887324\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.57908535  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.602342\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  142\n",
      "Reward:  -552.0\n",
      "Mean Reward -3000.034965034965\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.47997915  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.207813\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  143\n",
      "Reward:  -1230.0\n",
      "Mean Reward -2987.7430555555557\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.55215573  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.010028\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  144\n",
      "Reward:  -1059.0\n",
      "Mean Reward -2974.441379310345\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.50112136  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.954887\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  145\n",
      "Reward:  -939.0\n",
      "Mean Reward -2960.5\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.53958805  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.307311\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  146\n",
      "Reward:  -1302.0\n",
      "Mean Reward -2949.21768707483\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.55598642  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.281105\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  147\n",
      "Reward:  -1426.0\n",
      "Mean Reward -2938.925675675676\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.4798058  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.293278\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  148\n",
      "Reward:  -1498.0\n",
      "Mean Reward -2929.2550335570468\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.56403276  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.972157\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  149\n",
      "Reward:  -1050.0\n",
      "Mean Reward -2916.7266666666665\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.4288605  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.073925\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  150\n",
      "Reward:  -1087.0\n",
      "Mean Reward -2904.609271523179\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.58537171  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.748320\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  151\n",
      "Reward:  -733.0\n",
      "Mean Reward -2890.3223684210525\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.41169249  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.239055\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  152\n",
      "Reward:  -1180.0\n",
      "Mean Reward -2879.143790849673\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.52883509  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.292342\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  153\n",
      "Reward:  -1518.0\n",
      "Mean Reward -2870.305194805195\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.46523449  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.819288\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  154\n",
      "Reward:  -920.0\n",
      "Mean Reward -2857.722580645161\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.41554468  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.626662\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  155\n",
      "Reward:  -674.0\n",
      "Mean Reward -2843.724358974359\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.59774227  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.673208\n",
      "Step max: 1542.0\n",
      "Fail: False\n",
      "Episode:  156\n",
      "Reward:  -561.0\n",
      "Mean Reward -2829.184713375796\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.4357553  0.       ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.262651\n",
      "Step max: 1542.0\n",
      "Fail: True\n",
      "Episode:  157\n",
      "Reward:  -1434.0\n",
      "Mean Reward -2820.3544303797466\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.42071234  0.        ]\n",
      "The new step_max is : 1464.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.399347\n",
      "Step max: 1464.0\n",
      "Fail: False\n",
      "Episode:  158\n",
      "Reward:  -388.0\n",
      "Mean Reward -2805.056603773585\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.43611643  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.763895\n",
      "Step max: 1464.0\n",
      "Fail: False\n",
      "Episode:  159\n",
      "Reward:  -788.0\n",
      "Mean Reward -2792.45\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.59702542  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.593457\n",
      "Step max: 1464.0\n",
      "Fail: False\n",
      "Episode:  160\n",
      "Reward:  -525.0\n",
      "Mean Reward -2778.3664596273293\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.43980108  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.622582\n",
      "Step max: 1464.0\n",
      "Fail: False\n",
      "Episode:  161\n",
      "Reward:  -615.0\n",
      "Mean Reward -2765.0123456790125\n",
      "Max reward so far:  -374.0\n",
      "==========================================\n",
      "The starting state is : [-0.50089338  0.        ]\n",
      "The new step_max is : 1359.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.414167\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  162\n",
      "Reward:  -363.0\n",
      "Mean Reward -2750.276073619632\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.58069601  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.462438\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  163\n",
      "Reward:  -434.0\n",
      "Mean Reward -2736.1524390243903\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.43309979  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.783392\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  164\n",
      "Reward:  -773.0\n",
      "Mean Reward -2724.2545454545457\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.5476218  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.837899\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  165\n",
      "Reward:  -833.0\n",
      "Mean Reward -2712.8614457831327\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.4052487  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.633508\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  166\n",
      "Reward:  -630.0\n",
      "Mean Reward -2700.3892215568862\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.56601517  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.736637\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  167\n",
      "Reward:  -819.0\n",
      "Mean Reward -2689.190476190476\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.52368288  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.580822\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  168\n",
      "Reward:  -530.0\n",
      "Mean Reward -2676.414201183432\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.54219374  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.579786\n",
      "Step max: 1359.0\n",
      "Fail: False\n",
      "Episode:  169\n",
      "Reward:  -513.0\n",
      "Mean Reward -2663.6882352941175\n",
      "Max reward so far:  -363.0\n",
      "==========================================\n",
      "The starting state is : [-0.44050693  0.        ]\n",
      "The new step_max is : 834.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.219115\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  170\n",
      "Reward:  -218.0\n",
      "Mean Reward -2649.3859649122805\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.42357002  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.288942\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  171\n",
      "Reward:  -271.0\n",
      "Mean Reward -2635.5581395348836\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.48284055  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.666915\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  172\n",
      "Reward:  -658.0\n",
      "Mean Reward -2624.127167630058\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53204509  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.689271\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  173\n",
      "Reward:  -648.0\n",
      "Mean Reward -2612.770114942529\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44731093  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.428523\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  174\n",
      "Reward:  -377.0\n",
      "Mean Reward -2599.9942857142855\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41033305  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.631290\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  175\n",
      "Reward:  -651.0\n",
      "Mean Reward -2588.9204545454545\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45607699  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.505522\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  176\n",
      "Reward:  -464.0\n",
      "Mean Reward -2576.915254237288\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58426204  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.500004\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  177\n",
      "Reward:  -515.0\n",
      "Mean Reward -2565.3314606741574\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.54159801  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.669697\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  178\n",
      "Reward:  -630.0\n",
      "Mean Reward -2554.5195530726255\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.461882  0.      ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.617793\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  179\n",
      "Reward:  -558.0\n",
      "Mean Reward -2543.427777777778\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58338837  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.576931\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  180\n",
      "Reward:  -583.0\n",
      "Mean Reward -2532.596685082873\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4987181  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.710000\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  181\n",
      "Reward:  -790.0\n",
      "Mean Reward -2523.021978021978\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51620502  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.577280\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  182\n",
      "Reward:  -622.0\n",
      "Mean Reward -2512.6338797814205\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53152702  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.579570\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  183\n",
      "Reward:  -528.0\n",
      "Mean Reward -2501.8478260869565\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55450002  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.674058\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  184\n",
      "Reward:  -694.0\n",
      "Mean Reward -2492.0756756756755\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53967474  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.666855\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  185\n",
      "Reward:  -704.0\n",
      "Mean Reward -2482.462365591398\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.46393871  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.654306\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  186\n",
      "Reward:  -627.0\n",
      "Mean Reward -2472.5401069518716\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49209488  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.703577\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  187\n",
      "Reward:  -634.0\n",
      "Mean Reward -2462.7606382978724\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51239169  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.667491\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  188\n",
      "Reward:  -716.0\n",
      "Mean Reward -2453.5185185185187\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43659224  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.696021\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  189\n",
      "Reward:  -612.0\n",
      "Mean Reward -2443.8263157894735\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.5560865  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.706624\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  190\n",
      "Reward:  -716.0\n",
      "Mean Reward -2434.780104712042\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57808833  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.696873\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  191\n",
      "Reward:  -696.0\n",
      "Mean Reward -2425.7239583333335\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45326137  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.716904\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  192\n",
      "Reward:  -702.0\n",
      "Mean Reward -2416.79274611399\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.54566319  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.752246\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  193\n",
      "Reward:  -722.0\n",
      "Mean Reward -2408.056701030928\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53097521  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.730654\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  194\n",
      "Reward:  -834.0\n",
      "Mean Reward -2399.9846153846156\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52710979  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.719929\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  195\n",
      "Reward:  -804.0\n",
      "Mean Reward -2391.841836734694\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51708582  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.734926\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  196\n",
      "Reward:  -696.0\n",
      "Mean Reward -2383.233502538071\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52280662  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.657062\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  197\n",
      "Reward:  -680.0\n",
      "Mean Reward -2374.631313131313\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4025149  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.731703\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  198\n",
      "Reward:  -676.0\n",
      "Mean Reward -2366.095477386935\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4548555  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.668246\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  199\n",
      "Reward:  -672.0\n",
      "Mean Reward -2357.625\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57305519  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.638202\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  200\n",
      "Reward:  -673.0\n",
      "Mean Reward -2349.2437810945275\n",
      "Max reward so far:  -218.0\n",
      "Model saved\n",
      "==========================================\n",
      "The starting state is : [-0.55044181  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.712851\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  201\n",
      "Reward:  -740.0\n",
      "Mean Reward -2341.2772277227723\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4443935  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.435018\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  202\n",
      "Reward:  -403.0\n",
      "Mean Reward -2331.729064039409\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52062517  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.777502\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  203\n",
      "Reward:  -694.0\n",
      "Mean Reward -2323.700980392157\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59372256  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.717767\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  204\n",
      "Reward:  -828.0\n",
      "Mean Reward -2316.4048780487806\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45961581  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.689283\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  205\n",
      "Reward:  -674.0\n",
      "Mean Reward -2308.4320388349515\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49316866  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.672317\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  206\n",
      "Reward:  -654.0\n",
      "Mean Reward -2300.43961352657\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58982888  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.734892\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  207\n",
      "Reward:  -699.0\n",
      "Mean Reward -2292.7403846153848\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43792242  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.681232\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  208\n",
      "Reward:  -666.0\n",
      "Mean Reward -2284.956937799043\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43465095  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.524996\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  209\n",
      "Reward:  -517.0\n",
      "Mean Reward -2276.538095238095\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50439142  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.773937\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  210\n",
      "Reward:  -610.0\n",
      "Mean Reward -2268.6398104265404\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52081645  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.794725\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  211\n",
      "Reward:  -682.0\n",
      "Mean Reward -2261.1556603773583\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51490674  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.705575\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  212\n",
      "Reward:  -594.0\n",
      "Mean Reward -2253.3286384976527\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.455104  0.      ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.587143\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  213\n",
      "Reward:  -561.0\n",
      "Mean Reward -2245.4205607476633\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53552797  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.747489\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  214\n",
      "Reward:  -832.0\n",
      "Mean Reward -2238.846511627907\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59405943  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.708667\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  215\n",
      "Reward:  -701.0\n",
      "Mean Reward -2231.7268518518517\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41110381  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.695335\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  216\n",
      "Reward:  -702.0\n",
      "Mean Reward -2224.6774193548385\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45695547  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.586899\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  217\n",
      "Reward:  -542.0\n",
      "Mean Reward -2216.9587155963304\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53477993  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.702467\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  218\n",
      "Reward:  -633.0\n",
      "Mean Reward -2209.72602739726\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.47953224  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.730699\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  219\n",
      "Reward:  -688.0\n",
      "Mean Reward -2202.809090909091\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52012349  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.525958\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  220\n",
      "Reward:  -413.0\n",
      "Mean Reward -2194.710407239819\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50196121  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.720714\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  221\n",
      "Reward:  -662.0\n",
      "Mean Reward -2187.8063063063064\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55158552  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.681765\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  222\n",
      "Reward:  -684.0\n",
      "Mean Reward -2181.0627802690583\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57880933  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.689083\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  223\n",
      "Reward:  -734.0\n",
      "Mean Reward -2174.6026785714284\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59722247  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.677511\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  224\n",
      "Reward:  -694.0\n",
      "Mean Reward -2168.0222222222224\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57750542  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.731707\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  225\n",
      "Reward:  -864.0\n",
      "Mean Reward -2162.2522123893805\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44138187  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.414235\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  226\n",
      "Reward:  -355.0\n",
      "Mean Reward -2154.2907488986784\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.5477451  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.659824\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  227\n",
      "Reward:  -756.0\n",
      "Mean Reward -2148.157894736842\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50265473  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.516463\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  228\n",
      "Reward:  -446.0\n",
      "Mean Reward -2140.7248908296942\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.5856988  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.765350\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  229\n",
      "Reward:  -710.0\n",
      "Mean Reward -2134.504347826087\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40158207  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.682434\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  230\n",
      "Reward:  -716.0\n",
      "Mean Reward -2128.3636363636365\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43725766  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.659296\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  231\n",
      "Reward:  -776.0\n",
      "Mean Reward -2122.5344827586205\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40895158  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.739760\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  232\n",
      "Reward:  -692.0\n",
      "Mean Reward -2116.3948497854076\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.42311422  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.732670\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  233\n",
      "Reward:  -657.0\n",
      "Mean Reward -2110.1581196581196\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58434572  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.744883\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  234\n",
      "Reward:  -580.0\n",
      "Mean Reward -2103.6468085106385\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58469137  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.713499\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  235\n",
      "Reward:  -694.0\n",
      "Mean Reward -2097.673728813559\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49298389  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.426275\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  236\n",
      "Reward:  -393.0\n",
      "Mean Reward -2090.481012658228\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50637393  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.606958\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  237\n",
      "Reward:  -588.0\n",
      "Mean Reward -2084.168067226891\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41681995  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.550127\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  238\n",
      "Reward:  -561.0\n",
      "Mean Reward -2077.794979079498\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52321669  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.409469\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  239\n",
      "Reward:  -398.0\n",
      "Mean Reward -2070.795833333333\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44382314  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.657126\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  240\n",
      "Reward:  -672.0\n",
      "Mean Reward -2064.991701244813\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44520586  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.715853\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  241\n",
      "Reward:  -690.0\n",
      "Mean Reward -2059.309917355372\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55252  0.     ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.639481\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  242\n",
      "Reward:  -649.0\n",
      "Mean Reward -2053.5061728395062\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.47954271  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.404542\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  243\n",
      "Reward:  -334.0\n",
      "Mean Reward -2046.4590163934427\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41739152  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.500849\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  244\n",
      "Reward:  -414.0\n",
      "Mean Reward -2039.795918367347\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40727915  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.635650\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  245\n",
      "Reward:  -629.0\n",
      "Mean Reward -2034.060975609756\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49601217  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.613757\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  246\n",
      "Reward:  -544.0\n",
      "Mean Reward -2028.0283400809717\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51604573  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.705647\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  247\n",
      "Reward:  -678.0\n",
      "Mean Reward -2022.5846774193549\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53356665  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.424671\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  248\n",
      "Reward:  -371.0\n",
      "Mean Reward -2015.9518072289156\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53699469  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.697976\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  249\n",
      "Reward:  -668.0\n",
      "Mean Reward -2010.56\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.54486898  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.659383\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  250\n",
      "Reward:  -630.0\n",
      "Mean Reward -2005.0597609561753\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49988696  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.521445\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  251\n",
      "Reward:  -438.0\n",
      "Mean Reward -1998.8412698412699\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44900267  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.661643\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  252\n",
      "Reward:  -676.0\n",
      "Mean Reward -1993.6126482213438\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55789341  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.582284\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  253\n",
      "Reward:  -572.0\n",
      "Mean Reward -1988.015748031496\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43054406  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.676646\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  254\n",
      "Reward:  -768.0\n",
      "Mean Reward -1983.2313725490196\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53626377  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.566223\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  255\n",
      "Reward:  -551.0\n",
      "Mean Reward -1977.63671875\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50305372  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.625758\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  256\n",
      "Reward:  -511.0\n",
      "Mean Reward -1971.929961089494\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.5582609  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.459604\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  257\n",
      "Reward:  -448.0\n",
      "Mean Reward -1966.0232558139535\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51077817  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.742370\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  258\n",
      "Reward:  -664.0\n",
      "Mean Reward -1960.996138996139\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43147024  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.673596\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  259\n",
      "Reward:  -668.0\n",
      "Mean Reward -1956.0230769230768\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50477744  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.729099\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  260\n",
      "Reward:  -672.0\n",
      "Mean Reward -1951.103448275862\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55368458  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.645921\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  261\n",
      "Reward:  -678.0\n",
      "Mean Reward -1946.2442748091603\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43245638  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.422670\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  262\n",
      "Reward:  -398.0\n",
      "Mean Reward -1940.3574144486693\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51743894  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.681830\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  263\n",
      "Reward:  -676.0\n",
      "Mean Reward -1935.5681818181818\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55168207  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.610854\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  264\n",
      "Reward:  -605.0\n",
      "Mean Reward -1930.5471698113208\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50890098  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.708551\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  265\n",
      "Reward:  -676.0\n",
      "Mean Reward -1925.8308270676691\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.48642329  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.428258\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  266\n",
      "Reward:  -428.0\n",
      "Mean Reward -1920.2209737827716\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45650482  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.681816\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  267\n",
      "Reward:  -680.0\n",
      "Mean Reward -1915.5932835820895\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40026149  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.713510\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  268\n",
      "Reward:  -832.0\n",
      "Mean Reward -1911.5650557620818\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44536633  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.750089\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  269\n",
      "Reward:  -876.0\n",
      "Mean Reward -1907.7296296296297\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.46083686  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.737917\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  270\n",
      "Reward:  -768.0\n",
      "Mean Reward -1903.5239852398524\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58992724  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.712185\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  271\n",
      "Reward:  -754.0\n",
      "Mean Reward -1899.297794117647\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57912563  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.678516\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  272\n",
      "Reward:  -774.0\n",
      "Mean Reward -1895.1758241758241\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.44548508  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.698503\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  273\n",
      "Reward:  -662.0\n",
      "Mean Reward -1890.6751824817518\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.56035469  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.735769\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  274\n",
      "Reward:  -714.0\n",
      "Mean Reward -1886.3963636363637\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53264989  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.691925\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  275\n",
      "Reward:  -716.0\n",
      "Mean Reward -1882.1557971014493\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41391267  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.667587\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  276\n",
      "Reward:  -878.0\n",
      "Mean Reward -1878.5306859205775\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50927609  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.666946\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  277\n",
      "Reward:  -614.0\n",
      "Mean Reward -1873.9820143884892\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41729381  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.692646\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  278\n",
      "Reward:  -656.0\n",
      "Mean Reward -1869.616487455197\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.41651655  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.588910\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  279\n",
      "Reward:  -550.0\n",
      "Mean Reward -1864.9035714285715\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55141476  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.696618\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  280\n",
      "Reward:  -732.0\n",
      "Mean Reward -1860.8718861209964\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4799  0.    ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.699198\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  281\n",
      "Reward:  -718.0\n",
      "Mean Reward -1856.8191489361702\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40244111  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.689944\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  282\n",
      "Reward:  -590.0\n",
      "Mean Reward -1852.3427561837457\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.50494587  0.        ]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==========================================\n",
      "Total Time: 0:00:00.698314\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  283\n",
      "Reward:  -692.0\n",
      "Mean Reward -1848.2570422535211\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.46885567  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.692920\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  284\n",
      "Reward:  -686.0\n",
      "Mean Reward -1844.178947368421\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58684818  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.705595\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  285\n",
      "Reward:  -690.0\n",
      "Mean Reward -1840.1433566433566\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.52540056  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.491438\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  286\n",
      "Reward:  -460.0\n",
      "Mean Reward -1835.334494773519\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.5127037  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.619065\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  287\n",
      "Reward:  -573.0\n",
      "Mean Reward -1830.951388888889\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45494638  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.530832\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  288\n",
      "Reward:  -584.0\n",
      "Mean Reward -1826.6366782006921\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4234854  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.674898\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  289\n",
      "Reward:  -650.0\n",
      "Mean Reward -1822.5793103448275\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.42918564  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.720851\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  290\n",
      "Reward:  -594.0\n",
      "Mean Reward -1818.3573883161512\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51140829  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.745554\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  291\n",
      "Reward:  -686.0\n",
      "Mean Reward -1814.4794520547946\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59796411  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.673157\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  292\n",
      "Reward:  -718.0\n",
      "Mean Reward -1810.7372013651877\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.4023982  0.       ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.675209\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  293\n",
      "Reward:  -692.0\n",
      "Mean Reward -1806.9319727891157\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.51834808  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.690986\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  294\n",
      "Reward:  -596.0\n",
      "Mean Reward -1802.8271186440677\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58604336  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.726847\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  295\n",
      "Reward:  -672.0\n",
      "Mean Reward -1799.0067567567567\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.58706453  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.659316\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  296\n",
      "Reward:  -666.0\n",
      "Mean Reward -1795.1919191919192\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.54817  0.     ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.634290\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  297\n",
      "Reward:  -576.0\n",
      "Mean Reward -1791.1006711409395\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.54307059  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.477389\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  298\n",
      "Reward:  -431.0\n",
      "Mean Reward -1786.551839464883\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.45938691  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.368619\n",
      "Step max: 834.0\n",
      "Fail: False\n",
      "Episode:  299\n",
      "Reward:  -317.0\n",
      "Mean Reward -1781.6533333333334\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.43838763  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.740151\n",
      "Step max: 834.0\n",
      "Fail: True\n",
      "Episode:  300\n",
      "Reward:  -680.0\n",
      "Mean Reward -1777.9933554817276\n",
      "Max reward so far:  -218.0\n",
      "Model saved\n",
      "==========================================\n",
      "The starting state is : [-0.58101977  0.        ]\n",
      "Step: 1000\n",
      "The new step_max is : 3756.0\n",
      "==========================================\n",
      "Total Time: 0:00:01.021757\n",
      "Step max: 3756.0\n",
      "Fail: False\n",
      "Episode:  301\n",
      "Reward:  -1062.0\n",
      "Mean Reward -1775.6225165562914\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59851602  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.068084\n",
      "Step max: 3756.0\n",
      "Fail: False\n",
      "Episode:  302\n",
      "Reward:  -1100.0\n",
      "Mean Reward -1773.3927392739274\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.42132449  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:01.116718\n",
      "Step max: 3756.0\n",
      "Fail: False\n",
      "Episode:  303\n",
      "Reward:  -1213.0\n",
      "Mean Reward -1771.5493421052631\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.57885238  0.        ]\n",
      "The new step_max is : 2424.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.675149\n",
      "Step max: 2424.0\n",
      "Fail: False\n",
      "Episode:  304\n",
      "Reward:  -656.0\n",
      "Mean Reward -1767.8918032786885\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.53209883  0.        ]\n",
      "Step: 1000\n",
      "==========================================\n",
      "Total Time: 0:00:00.992445\n",
      "Step max: 2424.0\n",
      "Fail: False\n",
      "Episode:  305\n",
      "Reward:  -1047.0\n",
      "Mean Reward -1765.5359477124182\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.49805775  0.        ]\n",
      "The new step_max is : 2163.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.614770\n",
      "Step max: 2163.0\n",
      "Fail: False\n",
      "Episode:  306\n",
      "Reward:  -557.0\n",
      "Mean Reward -1761.599348534202\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.59699254  0.        ]\n",
      "==========================================\n",
      "Total Time: 0:00:00.667257\n",
      "Step max: 2163.0\n",
      "Fail: False\n",
      "Episode:  307\n",
      "Reward:  -642.0\n",
      "Mean Reward -1757.9642857142858\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.40867846  0.        ]\n",
      "The new step_max is : 1968.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.588336\n",
      "Step max: 1968.0\n",
      "Fail: False\n",
      "Episode:  308\n",
      "Reward:  -518.0\n",
      "Mean Reward -1753.9514563106795\n",
      "Max reward so far:  -218.0\n",
      "==========================================\n",
      "The starting state is : [-0.55517287  0.        ]\n",
      "The new step_max is : 1938.0\n",
      "==========================================\n",
      "Total Time: 0:00:00.569117\n",
      "Step max: 1938.0\n",
      "Fail: False\n",
      "Episode:  309\n",
      "Reward:  -514.0\n",
      "Mean Reward -1749.9516129032259\n",
      "Max reward so far:  -218.0\n"
     ]
    }
   ],
   "source": [
    "allRewards = []\n",
    "total_rewards = 0\n",
    "maximumRewardRecorded = 0\n",
    "episode = 0\n",
    "episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "import datetime\n",
    "\n",
    "step_max = 20000\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    old_position, old_velocity = None, None\n",
    "    for episode in range(max_episodes + 10):\n",
    "        \n",
    "        episode_rewards_sum = 0\n",
    "\n",
    "        # Launch the game\n",
    "        state = env.reset()\n",
    "        start_time = datetime.datetime.now()\n",
    "        print(\"==========================================\")\n",
    "        print(f\"The starting state is : {state}\")\n",
    "        \n",
    "        #env.render()\n",
    "        counter = 0\n",
    "        episode_max_pos, episode_min_pos = float(\"-2.0\"), float(\"2.0\")\n",
    "        direction_change_counter = 0\n",
    "        fail = False\n",
    "        while True:\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT,\n",
    "            # WE'RE OUTPUT PROBABILITIES.\n",
    "\n",
    "            action_probability_distribution = sess.run(\n",
    "                action_distribution, feed_dict={input_: state.reshape([1,state_size])})\n",
    "                # select action w.r.t the actions prob \n",
    "            action = np.random.choice(\n",
    "                range(\n",
    "                    action_probability_distribution.shape[1]),\n",
    "                    p=action_probability_distribution.ravel())\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            if old_position is None:\n",
    "                old_position, old_velocity = new_state\n",
    "            else:\n",
    "                old_position, old_velocity = position, velocity\n",
    "                \n",
    "            position, velocity = new_state\n",
    "            velocity_sign = velocity * old_velocity\n",
    "            \n",
    "            bonus = 0.0\n",
    "            if velocity_sign < 0.0:\n",
    "                new_record = False\n",
    "                direction_change_counter += 1\n",
    "                if position > episode_max_pos:\n",
    "                    episode_max_pos = position\n",
    "                    new_record = True\n",
    "                elif position < episode_min_pos:\n",
    "                    episode_min_pos = position\n",
    "                    new_record = True\n",
    "\n",
    "                if new_record:\n",
    "                    bonus = 10.0  # bonus for gaining potential energy\n",
    "                else:\n",
    "                    bonus = -2.0  # penalty for wasting potential energy\n",
    "\n",
    "            reward += bonus\n",
    "            \n",
    "            counter += 1\n",
    "            #if counter == 10:\n",
    "            #    break\n",
    "            # Store s, a, r\n",
    "            episode_states.append(state)\n",
    "                        \n",
    "            # For actions because we output only one (the index) we need 2\n",
    "            # (1 is for the action taken)\n",
    "            # We need [0., 1.] (if we take right) not just the index\n",
    "            action_ = np.zeros(action_size)\n",
    "            action_[action] = 1\n",
    "            \n",
    "            episode_actions.append(action_)\n",
    "            \n",
    "            episode_rewards.append(reward)\n",
    "           \n",
    "            if counter % 1000 == 0:\n",
    "                print(f\"Step: {counter}\")\n",
    "        \n",
    "            if counter >= step_max:\n",
    "                # Bad Ending\n",
    "                if episode <= max_episodes:\n",
    "                    done = True\n",
    "                    fail = True\n",
    "                else:\n",
    "                    step_max = 1000000\n",
    "                    \n",
    "                \n",
    "            if done:\n",
    "               \n",
    "                if counter < step_max / STEP_MULTIPLE :\n",
    "                    step_max = counter * STEP_MULTIPLE\n",
    "                    print(f\"The new step_max is : {step_max}\")\n",
    "                    \n",
    "                # Calculate sum reward\n",
    "                episode_rewards_sum = np.sum(episode_rewards)\n",
    "                \n",
    "                allRewards.append(episode_rewards_sum)\n",
    "                \n",
    "                total_rewards = np.sum(allRewards)\n",
    "                \n",
    "                # Mean reward\n",
    "                mean_reward = np.divide(total_rewards, episode+1)\n",
    "                \n",
    "                \n",
    "                maximumRewardRecorded = np.amax(allRewards)\n",
    "                end_time = datetime.datetime.now()\n",
    "                print(\"==========================================\")\n",
    "                print(f\"Total Time: {end_time - start_time}\")\n",
    "                print(f\"Step max: {step_max}\")\n",
    "                print(f\"Fail: {fail}\")\n",
    "                print(\"Episode: \", episode)\n",
    "                print(\"Reward: \", episode_rewards_sum)\n",
    "                print(\"Mean Reward\", mean_reward)\n",
    "                print(\"Max reward so far: \", maximumRewardRecorded)\n",
    "                \n",
    "                # Calculate discounted reward\n",
    "                discounted_episode_rewards = discount_and_normalize_rewards(episode_rewards)\n",
    "                                \n",
    "                # Feedforward, gradient and backpropagation\n",
    "                loss_, _ = sess.run(\n",
    "                    [loss, train_opt],\n",
    "                    feed_dict={\n",
    "                        input_: np.vstack(np.array(episode_states)),\n",
    "                        actions: np.vstack(np.array(episode_actions)),\n",
    "                        discounted_episode_rewards_: discounted_episode_rewards \n",
    "                    }\n",
    "                )\n",
    "                \n",
    "                # Write TF Summaries\n",
    "                summary = sess.run(write_op,\n",
    "                   feed_dict={\n",
    "                       input_: np.vstack(np.array(episode_states)),\n",
    "                       actions: np.vstack(np.array(episode_actions)),\n",
    "                       discounted_episode_rewards_: discounted_episode_rewards,\n",
    "                       mean_reward_: mean_reward\n",
    "                   }\n",
    "                )\n",
    "                \n",
    "                writer.add_summary(summary, episode)\n",
    "                writer.flush()\n",
    "                \n",
    "                # Reset the transition stores\n",
    "                episode_states, episode_actions, episode_rewards = [],[],[]\n",
    "                \n",
    "                break\n",
    "            \n",
    "            state = new_state\n",
    "        \n",
    "        # Save Model\n",
    "        if episode % 100 == 0:\n",
    "            saver.save(sess, \"./models/model.ckpt\")\n",
    "            print(\"Model saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Evaluate our trained model\n",
    "\n",
    "Load our model and see if it generalizes well by solving 10 random games and averaging the score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./models/model.ckpt\n",
      "****************************************************\n",
      "EPISODE  0\n",
      "Score -654.0\n",
      "****************************************************\n",
      "EPISODE  1\n",
      "Score -673.0\n",
      "****************************************************\n",
      "EPISODE  2\n",
      "Score -652.0\n",
      "****************************************************\n",
      "EPISODE  3\n",
      "Score -536.0\n",
      "****************************************************\n",
      "EPISODE  4\n",
      "Score -815.0\n",
      "****************************************************\n",
      "EPISODE  5\n",
      "Score -627.0\n",
      "****************************************************\n",
      "EPISODE  6\n",
      "Score -408.0\n",
      "****************************************************\n",
      "EPISODE  7\n",
      "Score -1178.0\n",
      "****************************************************\n",
      "EPISODE  8\n",
      "Score -781.0\n",
      "****************************************************\n",
      "EPISODE  9\n",
      "Score -1017.0\n",
      "Score over time: -734.1\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    env.reset()\n",
    "    rewards = []\n",
    "    \n",
    "    # Load the model\n",
    "    saver.restore(sess, \"./models/model.ckpt\")\n",
    "\n",
    "    for episode in range(10):\n",
    "        state = env.reset()\n",
    "        step = 0\n",
    "        done = False\n",
    "        total_rewards = 0\n",
    "        print(\"****************************************************\")\n",
    "        print(\"EPISODE \", episode)\n",
    "\n",
    "        while True:\n",
    "            \n",
    "\n",
    "            # Choose action a, remember WE'RE NOT IN A DETERMINISTIC ENVIRONMENT, WE'RE OUTPUT PROBABILITIES.\n",
    "            action_probability_distribution = sess.run(action_distribution, feed_dict={input_: state.reshape([1, state_size])})\n",
    "            #print(action_probability_distribution)\n",
    "            action = np.random.choice(range(action_probability_distribution.shape[1]), p=action_probability_distribution.ravel())  # select action w.r.t the actions prob\n",
    "\n",
    "\n",
    "            new_state, reward, done, info = env.step(action)\n",
    "\n",
    "            total_rewards += reward\n",
    "\n",
    "            if done:\n",
    "                rewards.append(total_rewards)\n",
    "                print (\"Score\", total_rewards)\n",
    "                break\n",
    "            state = new_state\n",
    "    env.close()\n",
    "    print (\"Score over time: \" +  str(sum(rewards)/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Report\n",
    "\n",
    "1.  Base run with _CartPole_ environment\n",
    "2.  Changed environment to _MountainCar_\n",
    "3.  Changed Neural Network input to match new environment state_space dimensions\n",
    "4.  Fitness Function Experiments:\n",
    "     1. Score initially improved but was stuck throughout the rest of training. Not very promising\n",
    "\t 2. Designed a new metric, **potential energy (PE)**\n",
    "         1.  Successfully improving PE during a direction change grants a bonus of +10 reward\n",
    "         2.  Failure to improve PE during a direction change provides a penalty of -2 reward\n",
    "\t 3. Added a **step limit multiplier** hyperparameter to the training that constrained training episode duration to be a multiple of our fastest training episode. Initial multiple was 1.5. \n",
    "         1.  This combined with **experiment B** definitely improved the score further during training. \n",
    "         2.  Post training evaluation results were not great.  Rewards were constantly in the negative thousands (~ -5000) \n",
    "\t 4. We found that the last set of training episodes had very short training times due to the lower step limit multiplier (1.5). To loosen this constraint, we increased the step limit multiplier from 1.5 to 3.0.\n",
    "         1.  Rewards constantly improved during training as before. \n",
    "         2.  Post training evaluation results were much better. Rewards were averaging ~-500. So this change led to an order of magnitude improvement in our evaluation testing."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:move_37]",
   "language": "python",
   "name": "conda-env-move_37-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
