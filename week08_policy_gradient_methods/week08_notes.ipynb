{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08 Notes - Policy Gradient Methods <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-Gradients-Math-Primer\" data-toc-modified-id=\"Policy-Gradients-Math-Primer-1\">Policy Gradients Math Primer</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence\" data-toc-modified-id=\"A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence-1.0.1\">A Short Intro to Entropy, Cross-Entropy and KL-Divergence</a></span></li><li><span><a href=\"#Softmax-Output-Function\" data-toc-modified-id=\"Softmax-Output-Function-1.0.2\">Softmax Output Function</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Math-Quiz\" data-toc-modified-id=\"Policy-Gradients-Math-Quiz-2\">Policy Gradients Math Quiz</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-2.0.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-2.0.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-2.0.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-2.0.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-2.0.5\">Question 5</a></span></li><li><span><a href=\"#Question-6\" data-toc-modified-id=\"Question-6-2.0.6\">Question 6</a></span></li><li><span><a href=\"#Question-7\" data-toc-modified-id=\"Question-7-2.0.7\">Question 7</a></span></li><li><span><a href=\"#Question-8\" data-toc-modified-id=\"Question-8-2.0.8\">Question 8</a></span></li><li><span><a href=\"#Question-9\" data-toc-modified-id=\"Question-9-2.0.9\">Question 9</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Methods-Tutorial\" data-toc-modified-id=\"Policy-Gradients-Methods-Tutorial-3\">Policy Gradients Methods Tutorial</a></span></li><li><span><a href=\"#Policy-Gradient-Methods-(REINFORCE)\" data-toc-modified-id=\"Policy-Gradient-Methods-(REINFORCE)-4\">Policy Gradient Methods (REINFORCE)</a></span></li><li><span><a href=\"#Reading-Assignment-(Evolved-Policy-Gradients)\" data-toc-modified-id=\"Reading-Assignment-(Evolved-Policy-Gradients)-5\">Reading Assignment (Evolved Policy Gradients)</a></span></li><li><span><a href=\"#Homework-Assignment-(Monte-Carlo-Policy-Gradients)\" data-toc-modified-id=\"Homework-Assignment-(Monte-Carlo-Policy-Gradients)-6\">Homework Assignment (Monte Carlo Policy Gradients)</a></span></li><li><span><a href=\"#Artificial-Curiosity\" data-toc-modified-id=\"Artificial-Curiosity-7\">Artificial Curiosity</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Math Primer\n",
    "\n",
    "[Logarithms Refresher](https://www.mathsisfun.com/algebra/logarithms.html)\n",
    "\n",
    "### A Short Intro to Entropy, Cross-Entropy and KL-Divergence\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
    "- [Paper: A Mathematical Theory of Communication](https://pure.mpg.de/rest/items/item_2383164/component/file_2383163/content)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Entropy, Cross-Entropy and KL-Divergence are often used in Machine Learning, in particular for training classifiers. In this short video, you will understand where they come from and why we use them in ML.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Cross entropy commonly used as a cost function when training classifiers\n",
    "- Entropy measures the average amounts of information that you get from one sample drawn from a given probability distribution $p$. It tells you how unpredictable that probability distribution is.\n",
    "\n",
    "$$ \\large H(p)\\ =\\ -\\sum_i p_i log_2(p_i) \\\\ $$\n",
    "\n",
    "- Cross entropy is the average message length\n",
    "\n",
    "$$ \\large H(p, q)\\ =\\ -\\sum_i p_i log_2(q_i) \\\\ $$\n",
    "\n",
    "where:\n",
    "- $p\\ =\\ $ true distribution\n",
    "- $q\\ =\\ $ predicted distribution\n",
    "\n",
    "- If our predictions are perfect, meaning the predicted distribution is equal to the true distribution, then cross-entropy is equal to entropy\n",
    "- If the distributions differ, then the cross-entropy will be greater than the entropy by some number of bits. This amount is called the relative entropy or more commonly, the Kullback-Leibler Divergence (KL Divergence)\n",
    "\n",
    "$$ Cross\\ Entropy\\ =\\ Entropy\\ +\\ KL\\ Divergence $$\n",
    "\n",
    "- KL Divergence is equal to the cross entropy minus the entropy\n",
    "\n",
    "$$ D_{KL}(p\\ ||\\ q) = H(p,\\ q)\\ - H(p) $$\n",
    "\n",
    "- Train an image classifier to detect some animals: Cat, Dog, Fox, Cow, Red Panda, Bear or Dolphin (7 possibilities)\n",
    "- Classifier outputs an estimated probability, the predicted possibility distribution\n",
    "- Since this is a Supervised Learning problem, we know the true distribution\n",
    "- Use the cross-entropy between these two distributions as a cost function, called a cross-entropy loss or log loss\n",
    "\n",
    "- Cross Entropy Loss: uses the natural logarithm rather than the binary logarithm\n",
    "\n",
    "$$ H(p,\\ q)\\ =\\ -\\sum_i p_i log(q_i) $$\n",
    "\n",
    "\n",
    "### Softmax Output Function\n",
    "\n",
    "- [Youtube Video: Neural Networks for Machine Learning](https://www.youtube.com/watch?v=mlaLLQofmR8)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Lecture from the course Neural Networks for Machine Learning, as taught by Geoffrey Hinton (University of Toronto) on Coursera in 2012. \n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Softmax output function forces the outputs of a neural network to sum to one so that they can represent a probability distribution across discrete mutually exclusive alternatives\n",
    "\n",
    "_Problems with squared error_\n",
    "\n",
    "- The squared error measure has some drawbacks:\n",
    "    - If the desired output is 1 and the actual output is 0.000000001 there is almost no gradient for a logistic unit to fix up the error\n",
    "    - If we try to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge\n",
    "- Is there a different cost function that works better?\n",
    "    - Yes: Force the outputs to represent a probability distribution across discrete alternatives via use of Softmax Function\n",
    "\n",
    "_Softmax_\n",
    "\n",
    "- The output units in a softmax group use a non-local non-linearity\n",
    "- They each receive some total input they've accumulated from the layer below (logit, $Z_i$) and provide an output $y_i$ that depends on the $Z$'s accumulated by their rivals as well as their own.\n",
    "- Output of the ith neuron:\n",
    "\n",
    "$$ \\large y_i\\ =\\ \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}} $$\n",
    "\n",
    "- The bottom line of the equation is the sum of the top line over all possibilities, we know when you add over all possibilities you'll end up with 1 - the sum of all $y_i$'s must come to one\n",
    "- The $y_i$'s must lie between 0 and 1\n",
    "- So we force the $y_i$'s to represent a probability distribution over mutually exclusive alternatives just by using that soft max equation\n",
    "\n",
    "- Softmax Derivative:\n",
    "\n",
    "$$ \\large \\frac{\\partial y_i}{\\partial z_i}\\ =\\ y_i(1-y_i) $$\n",
    "\n",
    "\n",
    "_Cross Entropy: the right cost function to use with softmax_\n",
    "\n",
    "- Right cost function is the negative log probability of the right answer.\n",
    "\n",
    "$$ \\large C\\ =\\ -\\sum_{j} t_j log(y_i) $$\n",
    "\n",
    "where $t_j$ is the target value\n",
    "\n",
    "- C has a very big gradient when the target value is 1 and the output is almost 0\n",
    "    - A value of 0.000001 is much better than 0.000000001\n",
    "    - The steepness of dC/dy exactly balances the flatness of dy/dz\n",
    "    \n",
    "$$ \\large \\frac{\\partial C}{\\partial z_i}\\ =\\ \\sum_{j} \\frac{\\partial C}{\\partial y_j}\\ \\frac{\\partial y_j}{\\partial z_j}\\ =\\ y_i\\ -\\ t_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Math Quiz\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Check all of the following which are true about logarithms.\n",
    "\n",
    "    - []\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 6\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 7\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 8\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n",
    "### Question 9\n",
    "\n",
    "- [] \n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Methods Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment (Evolved Policy Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment (Monte Carlo Policy Gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Curiosity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
