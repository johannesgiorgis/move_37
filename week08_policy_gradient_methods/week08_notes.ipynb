{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 08 Notes - Policy Gradient Methods <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-Gradients-Math-Primer\" data-toc-modified-id=\"Policy-Gradients-Math-Primer-1\">Policy Gradients Math Primer</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence\" data-toc-modified-id=\"A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence-1.0.1\">A Short Intro to Entropy, Cross-Entropy and KL-Divergence</a></span></li><li><span><a href=\"#Softmax-Output-Function\" data-toc-modified-id=\"Softmax-Output-Function-1.0.2\">Softmax Output Function</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Math-Quiz\" data-toc-modified-id=\"Policy-Gradients-Math-Quiz-2\">Policy Gradients Math Quiz</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-2.0.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-2.0.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-2.0.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-2.0.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-2.0.5\">Question 5</a></span></li><li><span><a href=\"#Question-6\" data-toc-modified-id=\"Question-6-2.0.6\">Question 6</a></span></li><li><span><a href=\"#Question-7\" data-toc-modified-id=\"Question-7-2.0.7\">Question 7</a></span></li><li><span><a href=\"#Question-8\" data-toc-modified-id=\"Question-8-2.0.8\">Question 8</a></span></li><li><span><a href=\"#Question-9\" data-toc-modified-id=\"Question-9-2.0.9\">Question 9</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Methods-Tutorial\" data-toc-modified-id=\"Policy-Gradients-Methods-Tutorial-3\">Policy Gradients Methods Tutorial</a></span></li><li><span><a href=\"#Policy-Gradient-Methods-(REINFORCE)\" data-toc-modified-id=\"Policy-Gradient-Methods-(REINFORCE)-4\">Policy Gradient Methods (REINFORCE)</a></span></li><li><span><a href=\"#Evolved-Policy-Gradients\" data-toc-modified-id=\"Evolved-Policy-Gradients-5\">Evolved Policy Gradients</a></span></li><li><span><a href=\"#Policy-Gradients-Study-Guide\" data-toc-modified-id=\"Policy-Gradients-Study-Guide-6\">Policy Gradients Study Guide</a></span></li><li><span><a href=\"#Policy-Gradients-Quiz\" data-toc-modified-id=\"Policy-Gradients-Quiz-7\">Policy Gradients Quiz</a></span></li><li><span><a href=\"#Homework-Assignment-(Monte-Carlo-Policy-Gradients)\" data-toc-modified-id=\"Homework-Assignment-(Monte-Carlo-Policy-Gradients)-8\">Homework Assignment (Monte Carlo Policy Gradients)</a></span></li><li><span><a href=\"#Artificial-Curiosity\" data-toc-modified-id=\"Artificial-Curiosity-9\">Artificial Curiosity</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Math Primer\n",
    "\n",
    "[Logarithms Refresher](https://www.mathsisfun.com/algebra/logarithms.html)\n",
    "\n",
    "### A Short Intro to Entropy, Cross-Entropy and KL-Divergence\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=ErfnhcEV1O8)\n",
    "- [Paper: A Mathematical Theory of Communication](https://pure.mpg.de/rest/items/item_2383164/component/file_2383163/content)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Entropy, Cross-Entropy and KL-Divergence are often used in Machine Learning, in particular for training classifiers. In this short video, you will understand where they come from and why we use them in ML.\n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Cross entropy commonly used as a cost function when training classifiers\n",
    "- Entropy measures the average amounts of information that you get from one sample drawn from a given probability distribution $p$. It tells you how unpredictable that probability distribution is.\n",
    "\n",
    "$$ \\large H(p)\\ =\\ -\\sum_i p_i log_2(p_i) \\\\ $$\n",
    "\n",
    "- Cross entropy is the average message length\n",
    "\n",
    "$$ \\large H(p, q)\\ =\\ -\\sum_i p_i log_2(q_i) \\\\ $$\n",
    "\n",
    "where:\n",
    "- $p\\ =\\ $ true distribution\n",
    "- $q\\ =\\ $ predicted distribution\n",
    "\n",
    "- If our predictions are perfect, meaning the predicted distribution is equal to the true distribution, then cross-entropy is equal to entropy\n",
    "- If the distributions differ, then the cross-entropy will be greater than the entropy by some number of bits. This amount is called the relative entropy or more commonly, the Kullback-Leibler Divergence (KL Divergence)\n",
    "\n",
    "$$ Cross\\ Entropy\\ =\\ Entropy\\ +\\ KL\\ Divergence $$\n",
    "\n",
    "- KL Divergence is equal to the cross entropy minus the entropy\n",
    "\n",
    "$$ D_{KL}(p\\ ||\\ q) = H(p,\\ q)\\ - H(p) $$\n",
    "\n",
    "- Train an image classifier to detect some animals: Cat, Dog, Fox, Cow, Red Panda, Bear or Dolphin (7 possibilities)\n",
    "- Classifier outputs an estimated probability, the predicted possibility distribution\n",
    "- Since this is a Supervised Learning problem, we know the true distribution\n",
    "- Use the cross-entropy between these two distributions as a cost function, called a cross-entropy loss or log loss\n",
    "\n",
    "- Cross Entropy Loss: uses the natural logarithm rather than the binary logarithm\n",
    "\n",
    "$$ H(p,\\ q)\\ =\\ -\\sum_i p_i log(q_i) $$\n",
    "\n",
    "\n",
    "### Softmax Output Function\n",
    "\n",
    "- [Youtube Video: Neural Networks for Machine Learning](https://www.youtube.com/watch?v=mlaLLQofmR8)\n",
    "\n",
    "**Video Description**:\n",
    "\n",
    "Lecture from the course Neural Networks for Machine Learning, as taught by Geoffrey Hinton (University of Toronto) on Coursera in 2012. \n",
    "\n",
    "\n",
    "**Notes**:\n",
    "\n",
    "- Softmax output function forces the outputs of a neural network to sum to one so that they can represent a probability distribution across discrete mutually exclusive alternatives\n",
    "\n",
    "_Problems with squared error_\n",
    "\n",
    "- The squared error measure has some drawbacks:\n",
    "    - If the desired output is 1 and the actual output is 0.000000001 there is almost no gradient for a logistic unit to fix up the error\n",
    "    - If we try to assign probabilities to mutually exclusive class labels, we know that the outputs should sum to 1, but we are depriving the network of this knowledge\n",
    "- Is there a different cost function that works better?\n",
    "    - Yes: Force the outputs to represent a probability distribution across discrete alternatives via use of Softmax Function\n",
    "\n",
    "_Softmax_\n",
    "\n",
    "- The output units in a softmax group use a non-local non-linearity\n",
    "- They each receive some total input they've accumulated from the layer below (logit, $Z_i$) and provide an output $y_i$ that depends on the $Z$'s accumulated by their rivals as well as their own.\n",
    "- Output of the ith neuron:\n",
    "\n",
    "$$ \\large y_i\\ =\\ \\frac{e^{z_i}}{\\sum_{j\\in group} e^{z_i}} $$\n",
    "\n",
    "- The bottom line of the equation is the sum of the top line over all possibilities, we know when you add over all possibilities you'll end up with 1 - the sum of all $y_i$'s must come to one\n",
    "- The $y_i$'s must lie between 0 and 1\n",
    "- So we force the $y_i$'s to represent a probability distribution over mutually exclusive alternatives just by using that soft max equation\n",
    "\n",
    "- Softmax Derivative:\n",
    "\n",
    "$$ \\large \\frac{\\partial y_i}{\\partial z_i}\\ =\\ y_i(1-y_i) $$\n",
    "\n",
    "\n",
    "_Cross Entropy: the right cost function to use with softmax_\n",
    "\n",
    "- Right cost function is the negative log probability of the right answer.\n",
    "\n",
    "$$ \\large C\\ =\\ -\\sum_{j} t_j log(y_i) $$\n",
    "\n",
    "where $t_j$ is the target value\n",
    "\n",
    "- C has a very big gradient when the target value is 1 and the output is almost 0\n",
    "    - A value of 0.000001 is much better than 0.000000001\n",
    "    - The steepness of dC/dy exactly balances the flatness of dy/dz\n",
    "    \n",
    "$$ \\large \\frac{\\partial C}{\\partial z_i}\\ =\\ \\sum_{j} \\frac{\\partial C}{\\partial y_j}\\ \\frac{\\partial y_j}{\\partial z_j}\\ =\\ y_i\\ -\\ t_i $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Math Quiz\n",
    "\n",
    "This is a quiz to test your understanding of the Policy Gradients Math Primer lesson. Make sure you are confident on the material before you attempt it!\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Check all of the following which are true about logarithms.\n",
    "\n",
    "    - [ ] Log is a function which outputs the value of tensors to the console\n",
    "    - [x] A log is how many of the base number we need to multiply to get the given value\n",
    "    - [ ] log3(x) is the cubed root of x cubed\n",
    "    - [x] log base 2 of 8 means 2 to what power equals 8\n",
    "    - [x] A negative result of a log means we are dividing instead of multiplying\n",
    "    - [ ] The result of a log is always positiveion\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "What is $ log_2(64) $?\n",
    "\n",
    "    - [ ] 2\n",
    "    - [ ] 4\n",
    "    - [x] 6\n",
    "    - [ ] 8\n",
    "\n",
    "**Explanation**: $2 ^ 6 = 64$, therefore the log base 2 of 64 is 6.\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "What is $ log_e(1) $?\n",
    "\n",
    "    - [ ] - infinity\n",
    "    - [ ] -1\n",
    "    - [x] 0\n",
    "    - [ ] 1\n",
    "    - [ ] infinity\n",
    "\n",
    "**Explanation**: $e^0 = 1$, in fact any positive number to the power of zero equals one.\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Check all that are true about entropy.\n",
    "\n",
    "    - [ ] Entropy is the tendency of neural networks to move from a state of order to chaos\n",
    "    - [x] Entropy is a measure of uncertainty in a probability distribution\n",
    "    - [ ] Entropy is an objective scientific measure of how bad Siraj's hair gets on a bad hair day\n",
    "    - [ ] Entropy goes up as an RL algorithm becomes more certain of which actions to take\n",
    "    - [x] Entropy goes down as an RL algorithm becomes more certain of which actions to take\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Friday night you have a $60%$ chance of going out for food with buddies, $30%$ chance of staying home to study, and $10%$ chance of landing a really hot date. What is the (base e) entropy of your Friday night probability distribution?\n",
    "\n",
    "- [x] $- (0.6 * log(0.6) + 0.3 * log(0.3) + 0.1 * log(0.1)) = 0.8979$\n",
    "- [ ] $\\sqrt(0.6 ^ 2 + 0.3 ^ 2 + 0.1 ^2) = 0.6782$\n",
    "- [ ] $(0.6 * log(0.6) + 0.3 * log(0.3) + 0.1 * log(0.1)) ^ 2 = 0.8063$\n",
    "- [ ] $\\sqrt(0.4 ^ 2 + 0.7 ^ 2 + 0.9 ^2) = 1.2083$\n",
    "\n",
    "\n",
    "### Question 6\n",
    "\n",
    "Check all that are true about cross-entropy.\n",
    "\n",
    "    - [x] Cross entropy measures how much a probability distribution varies from the expected value\n",
    "    - [ ] Cross entropy is the same as entropy, but moving in the perpendicular direction\n",
    "    - [ ] Cross entropy is calculated as the mean squared difference between two probability distributions\n",
    "    - [x] Cross entropy is the negative sum of the expected probabilities multiplied by the log of their respective actual probabilities\n",
    "    - [ ] The field of machine learning mostly uses log base 2 to calculate cross entropy\n",
    "\n",
    "\n",
    "### Question 7\n",
    "\n",
    "Your local weatherman predicts an $80%$ chance of sun and $20%$ chance of rain for tomorrow. Based on that report you leave your umbrella at home and get soaked when it rains. What loss function are you going to back-propagate into the neurons of the weatherman’s brain so he’ll be more right next time?\n",
    "\n",
    "- [ ] $ - (1.0 * log_e(0.8)) = 0.22$\n",
    "- [ ] $\\sqrt(e^{0.8}) = 1.49$\n",
    "- [x] $- (1.0 * log_e(0.2)) = 1.61$\n",
    "- [ ] $\\sqrt(e^{0.2}) = 1.10$\n",
    "\n",
    "\n",
    "### Question 8\n",
    "\n",
    "Check all that are true about the softmax function.\n",
    "\n",
    "    - [ ] Outputs the exact inverse of the hardmin function.\n",
    "    - [x] Squeezes arbitrary numbers into probabilities between 0 and 1, all adding up to 1\n",
    "    - [ ] Softens the maximum values in a matrix to reduce outliers\n",
    "    - [x] Softmax is the exponential of each logit, divided by the sum of the exponentials of all logits\n",
    "    - [ ] Softmax is the square of each logit, divided by the square of all logits\n",
    "    - [ ] In psychology, softmax is when you want to do your best but don't want to try too hard\n",
    "\n",
    "\n",
    "### Question 9\n",
    "\n",
    "What is the softmax of the array ```[1.0, 2.0, 3.0, 4.0]```?\n",
    "\n",
    "- [ ] ```[0.002, 0.016, 0.117, 0.865]```\n",
    "- [ ] ```[0.004, 0.031, 0.234, 1.730]```\n",
    "- [ ] ```[0.1, 0.2, 0.3, 0.4]```\n",
    "- [x] ```[0.032, 0.087, 0.237, 0.644]```\n",
    "\n",
    "**Explanation**: If you are having trouble lookup \"softmax function\" in Wikipedia and scroll down to the python code. Paste the formula into a workbook to calculate the softmax of the array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Methods Tutorial\n",
    "\n",
    "**Video Description:**\n",
    "\n",
    "Dive deeper into deep reinforcement learning and learn how to improve upon Q learning with policy gradient methods!\n",
    "\n",
    "**Notes**\n",
    "\n",
    "\n",
    "**Learning Resources**\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=0c3r5EWeBvo)\n",
    "- [Code Link: CartPole PGN](https://github.com/colinskow/move37/tree/master/pg)\n",
    "- [Math is Fun: Introduction to Logarithms](https://www.mathsisfun.com/algebra/logarithms.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradient Methods (REINFORCE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evolved Policy Gradients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Study Guide\n",
    "\n",
    "[Policy Gradients Study Guide](docs/Policy-Gradients-Study-Guide.pdf)\n",
    "\n",
    "\n",
    "**Policy Gradient Methods**\n",
    "\n",
    "- Use gradient ascent to adjust toward a policy with greater reward\n",
    "- Are model-free\n",
    "- Are 'on policy'\n",
    "- Are a form of policy search\n",
    "\n",
    "\n",
    "**Main types of policy gradient methods**:\n",
    "\n",
    "- [source](http://www.scholarpedia.org/article/Policy_gradient_methods)\n",
    "- Finite Difference Methods\n",
    "- Likelihood ratio methods (REINFORCE)\n",
    "- Natural policy gradients\n",
    "\n",
    "\n",
    "**How do Policy Gradients compare to other methods?**\n",
    "\n",
    "- They are preferred to DQN [source](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- They are commonly used as an actor for actor-critic methods [source](https://www.quora.com/What-is-the-difference-between-policy-gradient-methods-and-actor-critic-methods)\n",
    "- Policy Gradients are compared to value based methods: [source](https://www.youtube.com/watch?v=KHZVXao4qXs)\n",
    "\n",
    "_Advantages_\n",
    "\n",
    "- Better convergence properties (guarantees local convergence)\n",
    "- Effectvive in high dimensional/continuous action spaces\n",
    "- Can learn stochastic policies\n",
    "\n",
    "_Disadvantages_\n",
    "\n",
    "- Tends to converge on local rather than global optimum\n",
    "- Evaluate the policy can be very inefficient\n",
    "\n",
    "\n",
    "**Extra Facts**\n",
    "\n",
    "- _AlphaGo_ uses policy gradients in combination with Monte Carlo Tree Search [source](http://karpathy.github.io/2016/05/31/rl/)\n",
    "- REINFORCE was the the first policy gradient method introduced in 1992 [source](https://www.quora.com/What-is-the-difference-between-policy-gradient-methods-and-actor-critic-methods)\n",
    "- REINFORCE is sometimes called Monte-Carlo Policy Gradient [source](https://www.youtube.com/watch?v=KHZVXao4qXs)\n",
    "- Policy Gradients can be used as an actor in Actor-Critic [source](https://www.quora.com/What-is-the-difference-between-policy-gradient-methods-and-actor-critic-methods)\n",
    "- The original DQN authors prefer Policy Gradients [source](http://karpathy.github.io/2016/05/31/rl/)\n",
    "\n",
    "For an extensive list of methods based on Policy Gradients, see this [blog post](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Gradients Quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment (Monte Carlo Policy Gradients)\n",
    "\n",
    "See Thomas Simonini’s example [here](https://github.com/simoninithomas/Deep_reinforcement_learning_Course/blob/master/Policy%20Gradients/Cartpole/Cartpole%20REINFORCE%20Monte%20Carlo%20Policy%20Gradients.ipynb): replicate it but for a different environment — Lunar Lander!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Artificial Curiosity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
