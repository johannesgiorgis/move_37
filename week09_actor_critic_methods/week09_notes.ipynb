{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 09 Notes - Actor Critic Methods <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Policy-Gradients-Math-Primer\" data-toc-modified-id=\"Policy-Gradients-Math-Primer-1\">Policy Gradients Math Primer</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence\" data-toc-modified-id=\"A-Short-Intro-to-Entropy,-Cross-Entropy-and-KL-Divergence-1.0.1\">A Short Intro to Entropy, Cross-Entropy and KL-Divergence</a></span></li><li><span><a href=\"#Softmax-Output-Function\" data-toc-modified-id=\"Softmax-Output-Function-1.0.2\">Softmax Output Function</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Math-Quiz\" data-toc-modified-id=\"Policy-Gradients-Math-Quiz-2\">Policy Gradients Math Quiz</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-2.0.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-2.0.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-2.0.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-2.0.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-2.0.5\">Question 5</a></span></li><li><span><a href=\"#Question-6\" data-toc-modified-id=\"Question-6-2.0.6\">Question 6</a></span></li><li><span><a href=\"#Question-7\" data-toc-modified-id=\"Question-7-2.0.7\">Question 7</a></span></li><li><span><a href=\"#Question-8\" data-toc-modified-id=\"Question-8-2.0.8\">Question 8</a></span></li><li><span><a href=\"#Question-9\" data-toc-modified-id=\"Question-9-2.0.9\">Question 9</a></span></li></ul></li></ul></li><li><span><a href=\"#Policy-Gradients-Methods-Tutorial\" data-toc-modified-id=\"Policy-Gradients-Methods-Tutorial-3\">Policy Gradients Methods Tutorial</a></span></li><li><span><a href=\"#Policy-Gradient-Methods-(REINFORCE)\" data-toc-modified-id=\"Policy-Gradient-Methods-(REINFORCE)-4\">Policy Gradient Methods (REINFORCE)</a></span></li><li><span><a href=\"#Evolved-Policy-Gradients\" data-toc-modified-id=\"Evolved-Policy-Gradients-5\">Evolved Policy Gradients</a></span></li><li><span><a href=\"#Policy-Gradients-Study-Guide\" data-toc-modified-id=\"Policy-Gradients-Study-Guide-6\">Policy Gradients Study Guide</a></span></li><li><span><a href=\"#Policy-Gradients-Quiz\" data-toc-modified-id=\"Policy-Gradients-Quiz-7\">Policy Gradients Quiz</a></span></li><li><span><a href=\"#Homework-Assignment-(Monte-Carlo-Policy-Gradients)\" data-toc-modified-id=\"Homework-Assignment-(Monte-Carlo-Policy-Gradients)-8\">Homework Assignment (Monte Carlo Policy Gradients)</a></span></li><li><span><a href=\"#Artificial-Curiosity\" data-toc-modified-id=\"Artificial-Curiosity-9\">Artificial Curiosity</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Drone Flight Controller\n",
    "\n",
    "\n",
    "**Video Description:**\n",
    "\n",
    "Drones are being used in real world applications around the world! They can help detect anomalies in crop yields, provide companies with physical assets a real-time continuous data stream, and help secure locations by giving teams an aerial view. We'll learn how a dron can learn to navigate a novel, complex environment using an advanced reinforcement learning technique called Deep Deterministic Policy Gradients. Its a mixture of policy gradients and actor critic, I'll go through the dependencies before explaining the real algorithm. Enjoy! \n",
    "\n",
    "\n",
    "**Take Aways**\n",
    "\n",
    "- The Robot Operating System allows for code reusability amongst Roboticist, and is currently the best free framework out there to architect a Robotic System\n",
    "- We can train a drone to navigate an environment using the Deep Deterministic Policy Gradients Algorithm or DDPG\n",
    "- DDPG is a mixture of Policy Gradients and Actor Critic, able to solve tasks in Continuous Action Spaces\n",
    "\n",
    "\n",
    "**Learning Resources**\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=PngA5YLFuvU)\n",
    "- [Code Link: Drone Flight Controller](https://github.com/llSourcell/Drone_Flight_Controller)\n",
    "- [ROS Robot Ignite Academy Account Sign Up](https://rds.theconstructsim.com/signup/)\n",
    "- [Blog: Deep Deterministic Policy Gradients in Tensorflow](https://pemami4911.github.io/blog/2016/08/21/ddpg-rl.html)\n",
    "- [Toward Data Science: Introduction to Various Reinforcement Learning Algorithms. Part I (Q-Learning, SARSA, DQN, DDPG)](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-i-q-learning-sarsa-dqn-ddpg-72a5e0cb6287)\n",
    "- [Reinforcement Learning Coach Documentation: Deep Deterministic Policy Gradient](https://coach.nervanasys.com/algorithms/policy_optimization/ddpg/index.html)\n",
    "- [Towards Data Science: Policy Gradients in a Nutshell](https://towardsdatascience.com/policy-gradients-in-a-nutshell-8b72f9743c5d)\n",
    "- [DeepMind: Deterministic Policy Gradient Algorithms](https://deepmind.com/research/publications/deterministic-policy-gradient-algorithms/)\n",
    "- [Medium: Solving Continuous Control environment using Deep Deterministic Policy Gradient (DDPG) agent](https://medium.com/@kinwo/solving-continuous-control-environment-using-deep-deterministic-policy-gradient-ddpg-agent-5e94f82f366d)\n",
    "- [FreeCodeCamp: An introduction to Policy Gradients with Cartpole and Doom](https://medium.freecodecamp.org/an-introduction-to-policy-gradients-with-cartpole-and-doom-495b5ef2207f)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Asynchronous Advantage Actor Critic (A3C) Tutorial\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment (Actor Critic Algorithms)\n",
    "\n",
    "Read this [comprehensive paper](http://busoniu.net/files/papers/ivo_smcc12_survey.pdf) that focuses on different actor critic techniques:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework Assignment (A2C)\n",
    "\n",
    "Re-implement A2C but in Tensorflow to fully grasp how it works. Here is a [PyTorch example](https://github.com/ikostrikov/pytorch-a3c)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Continuous Action Space Actor Critic Tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Master Roboschool with PPO (Coding Tutorial)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PPO (Proximal Policy Optimization)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Actor Critic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods Study Guide\n",
    "\n",
    "- [Study Guide](docs/Actor_Critic_Methods_Study_Guide.pdf)\n",
    "\n",
    "\n",
    "**Basics:**\n",
    "\n",
    "- Alternates between taking an actino and criticising the policy\n",
    "- **Actor**: decides the policy (which action to take) based on direct rewards\n",
    "- **Critic**: tells us how good our policy is relative to the environment state value estimates\n",
    "- Works with continuous action spaces\n",
    "\n",
    "\n",
    "**Relation to to other methods:**\n",
    "\n",
    "- Estimates both policy and value function (combines plicy and value based methods)\n",
    "- While policy gradients require waiting till theend of an episode, actor-critic updates at each step, greatly improving efficiency [source](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d)\n",
    "- Gradient version of Policy Iteration [source](https://www.youtube.com/watch?v=5Ke-d1Itk3k)\n",
    "- Similarities between Actor-Critic and GANs present an opportunity for cross-pollination of methods [source](https://www.youtube.com/watch?v=BkDqCV8hieE)\n",
    "\n",
    "\n",
    "**Main Actor-Critic Methods:**\n",
    "\n",
    "- **A2C, Advantage Actor-Critic** uses the equation $A(S, A) = Q(S, A) - V(S)$ to get the advantage (extra reward) of an action over the estimated value of a state. ([source](https://medium.freecodecamp.org/an-intro-to-advantage-actor-critic-methods-lets-play-sonic-the-hedgehog-86d6240171d))\n",
    "- **DDPG, Deep Deterministic Policy Gradient** based on A2C (Maxim Lapan: Deep learning hands on); combines DPG and DQN (2015) ([source](https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html)) ([paper](https://arxiv.org/abs/1509.02971))\n",
    "- **TRPO, Trust Region Policy Optimization** alters the parameter update for actors ([source](https://towardsdatascience.com/introduction-to-various-reinforcement-learning-algorithms-part-ii-trpo-ppo-87f2c5919bb9)) (2015) ([paper](https://arxiv.org/abs/1502.05477))\n",
    "- **A3C, Asynchronous Advantage Actor-Critic** similar to A2C; uses many actors trained asynchronously (Deepmind, 2016) ([paper](https://arxiv.org/abs/1602.01783))\n",
    "- **PPO, Proximal Policy Optimization** is the OpenAI algorithm of choice for RL as of 2017 ([source](https://blog.openai.com/openai-baselines-ppo/)) ([paper](https://arxiv.org/abs/1707.06347))\n",
    "    - OpenAI Five won a best of 3 against a team of top players at Dota using a scaled up version of PPO this August (2018) ([source](https://blog.openai.com/openai-five-benchmark-results/))\n",
    "    \n",
    "\n",
    "**A2C vs A3C:**\n",
    "- Maxim Lapan describes A3C as prefered over A2C (see chapter 11 of Deep Reinforcement Learning, Hands On)\n",
    "- OpenAI lists A2C as prefered to A3C ([source](https://blog.openai.com/baselines-acktr-a2c/))\n",
    "- A2C is simpler\n",
    "\n",
    "\n",
    "For a great explanation of A2C [see this comic](https://hackernoon.com/intuitive-rl-intro-to-advantage-actor-critic-a2c-4ff545978752)\n",
    "\n",
    "\n",
    "**Recent Advances in Actor-Critic:**\n",
    "\n",
    "- **BAC, Bayesian Actor-Critic** models the policy gradient as a gaussian process (2016)([paper](http://jmlr.org/papers/v17/10-245.html))\n",
    "- **ACER, Actor-Critic with Experience Replay** improves TRPO (2016) ([paper](https://arxiv.org/abs/1611.01224))\n",
    "- **ACKTR, Actor-Critic using Kronecker-factored Trust Region**, developed by OpenAI (2017) ([source](https://blog.openai.com/baselines-acktr-a2c/)) ([paper](https://arxiv.org/abs/1708.05144))\n",
    "- **GAC, Guide Actor-Critic** (2017) ([paper](https://arxiv.org/abs/1705.07606))\n",
    "- **SAC, Soft Actor-Critic** (2018) ([paper](https://arxiv.org/abs/1801.01290))\n",
    "- **TD3, Twin Delayed Deep Deterministic** works to reduce variance; improves DDPG (2018) ([paper](https://arxiv.org/abs/1802.09477))\n",
    "- **D4PG, Deep Distributional DDPG** improves DDPG by allowing critic to use probability distributions (2018) ([paper](https://arxiv.org/abs/1804.08617))\n",
    "- **SPU, Supervised Policy Update** improves both PPO and TRPO (2018) ([paper](https://arxiv.org/abs/1805.11706))\n",
    "- **POP3D, Policy Optimization with Penalized Point Probability Distance** improves TRPO; competitive with PPO (2018) ([paper](https://arxiv.org/abs/1807.00442v2))\n",
    "- **SIL, Self Imitation Learning** (2018) ([paper](https://arxiv.org/abs/1806.05635v1))\n",
    "\n",
    "\n",
    "**Variations on PPO:**\n",
    "\n",
    "- **AMBER, Adaptive Multi-Batch Experience Replay for Continuous Action Control** (2017) ([paper](https://arxiv.org/abs/1710.04423v2))\n",
    "- **PPO-CMA, Proximal Policy Optimization with Covariance Matrix Adaptation** (2018) ([paper](https://arxiv.org/abs/1810.02541v2))\n",
    "- **MPPO, Memory Proximal Policy Optimization** (2018) ([paper](https://arxiv.org/abs/1802.04063v2))\n",
    "- **PPO-Î»** (2018) ([paper](https://arxiv.org/abs/1804.06461v1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Actor Critic Methods Quiz\n",
    "\n",
    "Week 9 Quiz: Actor-Critic Methods\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Which of the following statements are true for actor-critic methods?\n",
    "\n",
    "    - [x] They are effective in continuous action-space problems\n",
    "    - [ ] They update at the end of the episode\n",
    "    - [x] They have similarities to GANs\n",
    "    - [ ] The actor estimates the value of the state\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Which of the following statements about A2C and A3C are true?\n",
    "\n",
    "    - [ ] A2C is prefered to A3C\n",
    "    - [ ] A3C is prefered to A2C\n",
    "    - [x] Sources disagree on which method gives better results\n",
    "    - [x] It would be a good idea perform your own tests\n",
    "\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "If you compare Maxim Lapan's explanation in Ch 11 of Deep RL Hands on, his results are contrary to those provided by [OpenAI](https://blog.openai.com/baselines-acktr-a2c/)\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "Which of the following are based on TRPO?\n",
    "\n",
    "    - [x] ACER\n",
    "    - [x] POP3D\n",
    "    - [ ] TD3\n",
    "    - [ ] AMBER\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Which of the following are based on DDPG?\n",
    "\n",
    "    - [ ] SPU\n",
    "    - [x] D4PG\n",
    "    - [ ] MPPO\n",
    "    - [x] TD3\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "Which of the following are Actor-Critic Methods?\n",
    "\n",
    "    - [x] GAC\n",
    "    - [x] SAC\n",
    "    - [x] SIL\n",
    "    - [ ] SOY"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
