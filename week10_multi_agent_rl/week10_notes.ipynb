{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 10 Notes - Multi Agent RL <a class=\"tocSkip\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#Move-37\" data-toc-modified-id=\"Move-37-1\">Move 37</a></span></li><li><span><a href=\"#Reading-Assignment-(Cooperative-Agents)\" data-toc-modified-id=\"Reading-Assignment-(Cooperative-Agents)-2\">Reading Assignment (Cooperative Agents)</a></span></li><li><span><a href=\"#Inverse-Reinforcement-Learning\" data-toc-modified-id=\"Inverse-Reinforcement-Learning-3\">Inverse Reinforcement Learning</a></span></li><li><span><a href=\"#MARL---Multi-Agent-Reinforcement-Learning\" data-toc-modified-id=\"MARL---Multi-Agent-Reinforcement-Learning-4\">MARL - Multi Agent Reinforcement Learning</a></span></li><li><span><a href=\"#Multi-Agent-and-Inverse-RL-Study-Guide\" data-toc-modified-id=\"Multi-Agent-and-Inverse-RL-Study-Guide-5\">Multi Agent and Inverse RL Study Guide</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Multi-Agent-Reinforcement-Learning\" data-toc-modified-id=\"Multi-Agent-Reinforcement-Learning-5.0.1\">Multi-Agent Reinforcement Learning</a></span></li><li><span><a href=\"#Inverse-Reinforcement-Learning\" data-toc-modified-id=\"Inverse-Reinforcement-Learning-5.0.2\">Inverse Reinforcement Learning</a></span></li></ul></li></ul></li><li><span><a href=\"#Multi-Agent-and-Inverse-RL-Quiz\" data-toc-modified-id=\"Multi-Agent-and-Inverse-RL-Quiz-6\">Multi Agent and Inverse RL Quiz</a></span><ul class=\"toc-item\"><li><ul class=\"toc-item\"><li><span><a href=\"#Question-1\" data-toc-modified-id=\"Question-1-6.0.1\">Question 1</a></span></li><li><span><a href=\"#Question-2\" data-toc-modified-id=\"Question-2-6.0.2\">Question 2</a></span></li><li><span><a href=\"#Question-3\" data-toc-modified-id=\"Question-3-6.0.3\">Question 3</a></span></li><li><span><a href=\"#Question-4\" data-toc-modified-id=\"Question-4-6.0.4\">Question 4</a></span></li><li><span><a href=\"#Question-5\" data-toc-modified-id=\"Question-5-6.0.5\">Question 5</a></span></li></ul></li></ul></li><li><span><a href=\"#AlphaGo-Zero-Tutorial-Part-1---Overview\" data-toc-modified-id=\"AlphaGo-Zero-Tutorial-Part-1---Overview-7\">AlphaGo Zero Tutorial Part 1 - Overview</a></span></li><li><span><a href=\"#AlphaGo-Zero-Tutorial-Part-2---Monte-Carlo-Tree-Search\" data-toc-modified-id=\"AlphaGo-Zero-Tutorial-Part-2---Monte-Carlo-Tree-Search-8\">AlphaGo Zero Tutorial Part 2 - Monte Carlo Tree Search</a></span></li><li><span><a href=\"#AlphaGo-Zero-Tutorial-Part-3---Neural-Network-Architecture\" data-toc-modified-id=\"AlphaGo-Zero-Tutorial-Part-3---Neural-Network-Architecture-9\">AlphaGo Zero Tutorial Part 3 - Neural Network Architecture</a></span></li><li><span><a href=\"#Final-Project-(Multi-Agent-Research-Project)\" data-toc-modified-id=\"Final-Project-(Multi-Agent-Research-Project)-10\">Final Project (Multi Agent Research Project)</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Move 37\n",
    "\n",
    "\n",
    "**Video Description:**\n",
    "\n",
    "Why was AlphaGo's Move 37 against Lee Sedol so significant? Why was it so important that I named my 10 week course on deep reinforcement learning on it? In this final video of my course, I'll explain what move 37 symbolized for humanity and detail 3 examples of how it will affect healthcare, design, and decision-making. We'll go through a code example of a Generative Adversarial Network and even discuss China ambitious 2030 AI initiative.  Theres a lot that I cover in this video, I hope that it helps connect the dots. Enjoy!\n",
    "\n",
    "**Learning Resources**\n",
    "\n",
    "- [Youtube Video](https://www.youtube.com/watch?v=vI9BllT7ovg)\n",
    "- [Code Link: Minigo: A minimalist Go engine modeled after AlphaGo Zero, built on MuGo](https://github.com/tensorflow/minigo)\n",
    "- [Blog: Was AlphaGo's Move 37 Inevitable?](https://katbailey.github.io/post/was-alphagos-move-37-inevitable/)\n",
    "- [Medium: Move 37](https://medium.com/@cristobal_esteban/move-37-a3b500aa75c2)\n",
    "- [Blog: Disrupted Humanity - Move 37](https://disruptionhub.com/disrupted-humanity-move-37/)\n",
    "- [Youtube: DeepMind AlphaGo Zero Explained](https://www.youtube.com/watch?v=UzYeqAJ2bA8)\n",
    "- [Youtube: AI in Medicine | Drug Discovery with GANs (Tensorflow Tutorial)](https://www.youtube.com/watch?v=hY9Bc3mtphs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reading Assignment (Cooperative Agents)\n",
    "\n",
    "Read these 3 articles on cooperative learning:\n",
    "\n",
    "1. [MIT Paper: Multi-Agent Reinforcement Learning: Independent vs. Cooperative Agents](http://web.media.mit.edu/~cynthiab/Readings/tan-MAS-reinfLearn.pdf)\n",
    "2. [DeepMind Blog: Capture the Flag](https://deepmind.com/blog/capture-the-flag/)\n",
    "3. [DeepMind Blog: Understanding Agent Cooperation](https://deepmind.com/blog/understanding-agent-cooperation/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inverse Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MARL - Multi Agent Reinforcement Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent and Inverse RL Study Guide\n",
    "\n",
    "[Study Guide](https://www.theschool.ai/wp-content/uploads/2018/11/Week-10-Study-Guide.pdf)\n",
    "\n",
    "\n",
    "### Multi-Agent Reinforcement Learning\n",
    "\n",
    "**Basics** ([source](https://towardsdatascience.com/modern-game-theory-and-multi-agent-reinforcement-learning-systems-e8c936d6de42))\n",
    "\n",
    "- Merges game theory, machine learning and cognitive science\n",
    "- The state of the environment depends on the actions of every agent\n",
    "- As the number of agents increases, teh complexity greatly rises\n",
    "- Multi-Agent RL suffers from the 'curse of dimensionality', a term Richard Bellman used to describe the problems relating to organizing data in high dimensional spaces\n",
    "\n",
    "\n",
    "**Advantages** ([source](https://www.youtube.com/watch?v=yE62Zwhmzi8))\n",
    "\n",
    "- _Robustness_: failure of a single agent can be compensated by other members\n",
    "- _Scalability_: more agents can be added over time\n",
    "- _Re-usability_: constituents can be re-tasked, while maintaining the system\n",
    "\n",
    "\n",
    "**Challenges**\n",
    "\n",
    "- Accomplishing global goals from local actions, each agent having limited observation\n",
    "- Credit assignment: how do we know which agents contributed to successful trials?\n",
    "- Incentives: how do we best reward our agents to accomplish our goal?\n",
    "- Learning while others are learning: leads to a very dynamic (hard to predict) environment\n",
    "\n",
    "\n",
    "**Aspects**\n",
    "\n",
    "- _Competition_: increasing challenges between rivals lead to an automatic curriculum. This principle underlies the success of AlphaGo\n",
    "- _Cooperation_: includes learning from others\n",
    "- _Communication_: includes negotiation\n",
    "- _Social Dilemmas_: can be useful for studying emergent cooperation and competition (these are often borrowed from game theory)\n",
    "\n",
    "\n",
    "**Reality-Import**\n",
    "\n",
    "- We live in a multi-agent world\n",
    "- An organization can be thought of as a multi-agent architecture\n",
    "- Traffic, the economy, and the environment are all multi-agent systems\n",
    "- AI assistant: an ideal digital assistant would make accurate predictions about a human's mental state and intentions (it would have a good theory of mind).\n",
    "- AI safety and ethics usually involves the interplay of humans and intelligent systems\n",
    "\n",
    "Please watch [this video](https://www.youtube.com/watch?v=yE62Zwhmzi8) when you have a chance. The speaker is a great authority on AlphaGo and move 37.\n",
    "\n",
    "\n",
    "### Inverse Reinforcement Learning\n",
    "\n",
    "**Basics** ([source](https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html))\n",
    "\n",
    "- Instead of using the rewards to find the policy, as we do in normal RL, Inverse RL seeks to find a reward function based on a given policy (behavior observed).\n",
    "- We assume that the expert demonstrator is acting optimally (vanilla RL)\n",
    "- Inverse RL is sometimes called Inverse Optimal Control (IOC)\n",
    "- 'Algorithms for Inverse Reinforcement Learning' (2000) is the original paper that lays the foundation for IRL ([source](https://thinkingwires.com/posts/2018-02-13-irl-tutorial-1.html))\n",
    "\n",
    "\n",
    "**Use cases** ([source](https://people.eecs.berkeley.edu/~pabbeel/cs287-fa12/slides/inverseRL.pdf))\n",
    "\n",
    "- Modeling animal (including human) behavior\n",
    "- Apprenticeship Learning/Learning from Demonstration\n",
    "- Modeling of agents for cooperation/competition ('Theory of Mind' for multi-agent RL)\n",
    "- This area of research has potential ethical consequence for how we interact with our technology (related to theory of mind) ([source](https://www.theguardian.com/sustainable-business/2015/jun/23/the-ethics-of-ai-how-to-stop-your-robot-cooking-your-cat))\n",
    "\n",
    "\n",
    "**Inverse Reinforcement Learning (IRL) vs Imitation Learning(IL)** ([source](http://www.lifl.fr/~pietquin/pdf/TNNLS_2016_BPMGOP.pdf))\n",
    "\n",
    "- Both IRL and IL are forms of Learning from Demonstration (LfD)\n",
    "- In IRL we find a reward function based on expert demonstration\n",
    "- In IL we try to generalize an expert strategy to unvisited states (similar to classification)\n",
    "- It's possible to represent both using a unified framework, which achieves better results than either alone\n",
    "- This term 'behavior cloning' is sometimes used interchangeably with imitation learning\n",
    "\n",
    "\n",
    "**Maximum Likelihood Inverse Reinforcement Learning** ([source](https://www.youtube.com/watch?v=h7uGyBcIeII))\n",
    "\n",
    "1. Guess a reward function\n",
    "2. Find a policy that's optimal for this reward function\n",
    "3. Get the probability of the demonstration data, given this policy\n",
    "4. Adjust the reward function in the direction of a gradient so as to increase the likelihood of the demonstration data\n",
    "5. Repeat steps 2 - 4 until we reach a local maximum\n",
    "\n",
    "\n",
    "Check out the following papers on theory of mind (not on the test):\n",
    "\n",
    "- [Machine Theory of Mind](https://arxiv.org/abs/1802.07740) (2018)\n",
    "- [Modelling User's Theory of AI's Mind in Interactive Intelligent Systems](https://arxiv.org/abs/1809.02869) (2018)\n",
    "- [M^3RL: Mind-aware Multi-agent Management Reinforcement Learning](https://arxiv.org/abs/1810.00147) (2018)\n",
    "- [Intrinsic Social Motivation via Causal Influence in Multi-Agent RL](https://arxiv.org/abs/1810.08647) (2018)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multi Agent and Inverse RL Quiz\n",
    "\n",
    "We are reaching the end of our journey together. I hope that you will continue to quiz yourself in the future so as to widen the boundary of your knowledge to new and exotic horizons. Good luck fellow wizard, and may the data be with you.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Which of the following statements about Multi-Agent RL are true?\n",
    "\n",
    "    - [x] Game theory is useful for describing multi-agent systems\n",
    "    - [x] The state of the environment depends on the actions of all the agents\n",
    "    - [ ] Multi-agent RL isn’t very scalable\n",
    "    - [ ] As the number of agents rises, the complexity decreases\n",
    "\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Which of the following are advantages of Multi-Agent RL?\n",
    "\n",
    "    - [ ] Having many agents makes our algorithm resilient to the curse of dimensionality\n",
    "    - [x] A single agent’s failure can be compensated by the collective system\n",
    "    - [x] New agents can be added over time, allowing for scalability\n",
    "    - [x] By retasking members, the system can be reconfigured\n",
    "\n",
    "\n",
    "### Question 3\n",
    "\n",
    "How does Multi-Agent RL apply to reality?\n",
    "\n",
    "    - [x] Traffic, the economy, and the environment are all examples of multi-agent systems\n",
    "    - [ ] A slot machine is usually modeled as a multi-agent environment\n",
    "    - [x] AI ethics involves the interplay of different agents, human and machine\n",
    "    - [x] An organization (business or other) can be thought of as a Multi-Agent system\n",
    "\n",
    "\n",
    "### Question 4\n",
    "\n",
    "Which of the following statements are true about Inverse Reinforcement Learning?\n",
    "\n",
    "    - [ ] Inverse RL is sometimes called behavior cloning\n",
    "    - [x] We assume that the demonstrator is behaving optimally\n",
    "    - [x] Agent-to-agent interactions (such as cooperation and competition) can be improved with IRL by allowing agents to form a theory of mind\n",
    "    - [x] There is a unified framework for Inverse RL and Imitation Learning\n",
    "\n",
    "\n",
    "### Question 5\n",
    "\n",
    "What are the use cases for Inverse Reinforcement Learning?\n",
    "\n",
    "    - [x] Modeling behavior of humans and other animals\n",
    "    - [x] Teaching a machine agent from an expert demonstrator\n",
    "    - [x] Developing agents that act ethically\n",
    "    - [x] Developing agents that work well together as teams\n",
    "\n",
    "**Explanation**:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero Tutorial Part 1 - Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero Tutorial Part 2 - Monte Carlo Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AlphaGo Zero Tutorial Part 3 - Neural Network Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Final Project (Multi Agent Research Project)\n",
    "\n",
    "Final project time! Reproduce the Deep Deterministic Policy Gradients algorithm for a multi-agent particle environment. The algorithm should learn how to get both agents to ‘tag’ each other. [Here](https://github.com/rohan-sawhney/multi-agent-rl) is the example repository, reproduce the code in pure tensorflow or pure pytorch. Due date is November 26, 2018. Submit to schoolofaigrading@gmail.com . Good luck!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
